[ { "title": "操作系统", "url": "/posts/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/", "categories": "八股, 操作系统", "tags": "操作系统", "date": "2022-07-19 02:00:00 +0000", "snippet": "1. 操作系统概述1.1 操作系统的概念、功能和目标联机命令接口：交互式命令接口，如time脱机命令接口：批处理命令接口程序接口：.dll文件1.2 操作系统的特征并发、共享、虚拟、异步。并发和共享是两个最基本的特征，二者互为存在条件。并发和并行不同：并发宏观上是同时发生的微观上是交替发生，并行是多个事件同时发生。共享：资源共享，指系统中的资源可供内存中多个并发执行的进程共同使用。 互斥共享：一个时间段内只允许一个进程访问该资源。 同时共享：允许一个时间段内由多个进程同时对它们进行访问，（同时指的是宏观）虚拟：指把一个物理上的实体变为若干个逻辑上的对应物。物理实体是实际存在的，而逻辑上对应物是用户感受到的。没有并发性，就谈不上虚拟性。 虚拟技术：空分复用技术（如虚拟存储器技术）、时分复用技术（如虚拟处理器）异步：在多到程序环境下，允许多个程序并发执行，但由于资源有限，进程的执行不是一贯到底的，而是走走停停，以不可预知的速度向前推进。（可能会阻塞）只有系统拥有并发性，才有可能导致异步性。1.3 操作系统的发展与分类手工操作阶段：打孔机（二进制），缺点：用户独占全机、人机速度矛盾导致资源利用率极低。批处理阶段： 单通道批处理系统：引入脱机输入/输出技术（用磁带完成），并监督程序负责控制作业的输入输出。监督程序是操作系统的雏形。 优点：缓解了一定程度的人机速度矛盾，资源利用率有所提升。 缺点：内存中仅能有一道程序运行，只有该程序运行结束之后才能调入下一道程序。CPU有大量的时间是在空闲等待I/O完成。资源利用率依然很低。 多道批处理系统：每次往内存中输入多道程序，操作系统正式诞生，并引入中断技术，由操作系统负责管理这些程序的运行。各个程序并发执行。 优点：多道程序并发执行，共享计算机资源。资源利用率大幅提升，CPU和其他资源保持忙碌状态，系统吞吐量增大。 缺点：用户响应时间长，没有人机交互功能，（用户提交自己的作业之后就只能等待计算机处理完成，中间不能控制自己的作业执行） 分时操作系统：计算机以时间片为单位轮流为各个用户/作业服务，各个用户可通过终端与计算机进行交互。 优点：解决了人机交互问题。 缺点：不能优先处理一些紧急任务。实时操作系统：能优先响应一些紧急任务。主要特点是及时性和可靠性。 硬实时系统：必须在绝对严格的规定时间内完成处理（导弹控制系统、自动驾驶系统） 软实时系统：能接受偶尔违反时间规定（12306火车订票系统）网络操作系统分布式操作系统个人计算机操作系统1.4 操作系统的运行机制和体系结构指令： 特权指令：如内存清零指令，不允许用户程序使用 非特权指令：如普通的运算指令CPU如何判断当前是否可以执行特权指令？ 处于用户态（目态）时，只能执行非特权指令；处于核心态（管态）时，都可以执行。程序： 内核程序：是系统的管理者，既可以执行特权指令，也可以执行非特权指令，运行在核心态； 应用程序：只能执行非特权指令，运行在用户态。1.5 中断和异常引入 中断机制，实现了多道程序并发执行。本质：发生中断意味着需要操作系统介入，开展管理工作。操作系统的管理工作需要使用特权指令，因此CPU要从用户态转为核心态。中断可以使CPU从用户态切换为核心态，使操作系统获得计算机的控制权。用户态和核心态是如何切换的？ 用户态-&gt;核心态的切换是通过中断实现的，并且中断是唯一途径。核心态-&gt;用户态的切换是通过执行一个特权指令，将程序状态字（PSW）的标志位设置为“用户态”。中断分类：1.6 系统调用系统调用：操作系统提供给应用程序（程序员/编程人员）使用的接口，应用程序可以发出系统调用请求来获得操作系统的服务，系统调用会使处理器从用户态进入核心态。凡是与资源有关的操作、会直接影响到其他进程的操作，一定需要操作系统介入，即需要通过系统调用来实现。为什么要提供系统调用功能？ 系统的共享资源由操作系统统一掌管，在用户程序中，凡是与资源有关的操作，都必须通过系统调用的方式向操作系统提出服务请求，由操作系统代为完成。可以保证系统的稳定性和安全性，防止用户进行非法操作。系统调用与库函数的区别：系统调用是操作系统向上层提供的接口，有的库函数是对系统调用的进一步封装，当今编写的应用程序大多是通过高级语言提供的库函数间接地进行系统调用。系统调用的过程：传递系统调用参数-&gt;执行陷入指令（用户态）-&gt;执行系统调用相应服务程序（核心态）-&gt;返回用户程序系统调用发生在用户态，对系统调用的处理发生在核心态。执行陷入指令会产生中断，使处理器从用户态进入核心态。2. 进程与线程2.1 进程进程：是进程实体的运动过程，是系统进行资源分配和调度的一个独立单位。（进程实体是静态的，进程是动态的）系统为每个运行的程序配置一个数据结构，称为进程控制块（PCB），用来描述进程的各种信息（如程序代码存放位置）。程序段、数据段、PCB三部分组成了进程实体（进程）。PCB是进程存在的唯一标志。进程的组织：多个进程之间的组织方式。进程的特征：进程的三种基本状态：进程状态间的切换：进程控制：实现进程状态转换。用原语实现进程控制。原语的特点是执行期间不允许中断，只能一气呵成。原语采用“关中断指令“和”开中断指令“实现。（核心态）原语所作的内容： 更新PCB中的信息（修改进程状态、将运行环境保存到PCB、从PCB恢复运行环境）； 将PCB插入合适的队列； 分配/回收资源相关原语：进程的创建、进程的终止、进程的阻塞、进程的唤醒、进程的切换。其中，阻塞和唤醒要成对出现。进程通信：进程之间的信息交换。进程是分配系统资源的单位，因此各进程拥有的内存地址空间相互独立。 共享存储：两个进程对共享空间的访问必须是互斥的。 基于数据结构的共享：比如共享空间里只能放一个长度为10的数组，速度慢、限制多，低级通信。 基于存储区的共享：在内存中画出一块共享存储区，数据的形式、存放位置都由进程控制，而不是操作系统。速度快，高级通信。 消息传递：进程间的数据交换以格式化的消息为单位。进程通过操作系统提供的”发送消息/接收消息“两个原语进行数据交换。 直接通信方式：消息直接挂到接收方的消息队列里； 间接（信箱）通信方式：消息先发到中间体（信箱）。 管道通信： 管道：用于连接读写进程的一个共享文件。即在内存中开辟一个大小固定的缓冲区。 只能采用半双工通信，某一时间段只能实现单向传输。如果要实现双向同时通信，需要设置两个管道。 各进程互斥地访问管道。 数据以字符流的形式写入管道，当管道写满时，写进程的write()系统调用将被阻塞，等待读进程将数据取走。当读进程将数据全部取走后，管道变空，此时读进程的read()系统调用将被阻塞。 如果没写满，就不允许读。如果没读空，就不允许写。 数据一旦被读出，就从管道中抛弃。读进程最多只能有一个，否则会有读错数据的情况。 2.2 线程什么是线程？为什么要引入线程？ 线程是一个基本的CPU执行单元，也是程序执行流的最小单位。 有的进程可能需要”同时“做很多事，而传统的进程只能串行地执行一系列程序。为此，引入了”线程“，来增加并发度。引入线程机制后，有什么变化？线程的属性：线程的实现方式： 用户级线程：由应用程序通过线程库实现。所有的线程管理工作都由应用程序负责（包括线程切换）；用户级线程中，线程切换可以在用户态下即可完成，无需操作系统干预；用户级线程对用户不透明，对操作系统透明。 内核级线程：线程的管理工作由操作系统内核完成。线程调度、切换等工作都由内核负责，因此内核级线程的切换必然需要在核心态下才能完成。内核级线程是处理机分配的单位。多线程模型： 多对一模型：多个用户及线程映射到一个内核级线程。每个用户进程只对应一个内核级线程。 优点：用户级线程的切换在用户空间即可完成，不需要切换到核心态，线程管理的系统开销小，效率高； 缺点：当一个用户级线程被阻塞后，整个进程都会被阻塞，并发度不高。多个进程不可在多核处理机上并行运行。 一对一模型：一个用户及线程映射到一个内核级线程。每个用户进程有与用户级线程同数量的内核级线程。 优点：当一个线程被阻塞后，别的线程还可以继续执行，并发能力强。多线程可在多核处理机上并行执行。 缺点：一个用户进程会占用多个内核级线程，线程切换由操作系统内核完成，需要切换到核心态，因此线程管理的成本高，开销大。 多对多模型：n用户及线程映射到m个内核级线程(n&gt;=m)。每个用户进程对应m个内核级线程。2.3 处理机调度调度：当有一堆任务要处理，但由于资源有限，这些事情没法同时处理。需要确定某种规则来决定处理这些任务的顺序。处理机调度：在多道程序系统中，进程的数量往往是多于处理机的个数的，这样不可能同时并行地处理各个进程。处理机调度就是从就绪队列中按照一定的算法选择一个进程并将处理机分配给它运行，以实现进程的并发执行。 高级调度（作业调度）：调入。 中级调度（内存调度）：暂时调到外存等待的进程状态为挂起状态。中级调度就是要决定将哪个处于挂起状态的进程重新调入内存。 低级调度（进程调度）：按照某种方法和策略从就绪队列中选取一个进程，将处理机分配给它。最基本的一种调度。频率很高。三种调度的联系、对比：2.3.1 进程调度（低级调度）（不是重点）进程调度的时机：进程调度的方式：进程切换：指一个进程让出处理机，由另一个进程占用处理机的过程。进程切换的过程完成了：对原来运行进程各种数据的保存；对新的进程各种数据的恢复。 注意：进程切换是有代价的，因此如果过于频繁的进行进程调度、切换，必然会使整个系统的效率降低，使系统大部分时间都花在了进程切换上，而真正用于执行进程的时间减少。2.3.2 调度算法的评价指标CPU利用率：指CPU”忙率“的时间占总时间的比例。系统吞吐量：单位时间内完成作业的数量。周转时间：指从作业被提交给系统开始，到作业完成作业为止的时间。包括：作业在外存后备队列上等待作业调度（高级调度）的时间、进程在就绪队列上等待进程调度（低级调度）的时间、进程在CPU上执行的时间、进程等待I/O操作完成的时间。带权周转时间：作业周转时间/作业实际运行的时间。等待时间：指进程/作业处于等待处理机状态时间之和，等待时间越长，用户满意度越低。响应时间：指从用户提交请求到首次产生响应所用的时间。2.3.3 调度算法先来先服务（FCFS）：按照到达的先后顺序调度。短作业优先（SJF）：每次调度时选择当前已到达且运行时间最短的作业/进程。（抢占式和非抢占式）高响应比优先（HRRN）：在每次调度时先计算各个作业/进程的响应比，选择响应比最高难顶作业/进程为其服务。（响应比=等待时间+要求服务时间/要求服务时间）。非抢占式，只有当前运行的进程主动放弃CPU（正常/异常完成，或主动阻塞），才需要进行调度，调度时计算所有就绪进程的响应比，选响应比最高的进程上处理机。时间片轮转（RR）：按照各进程到达就绪队列的顺序，轮流让各个进程执行一个时间片（如100 ms）。若进程未在一个时间片内执行完，则剥夺处理机，将进程重新放到就绪队列队尾重新排队。常用于分时操作系统，更注重”响应时间”。（抢占式） 一般，设计时间片时要让切换进程的开销占比不超过1%。 如果时间片太大，使得每个进程都可以在一个时间片内就完成，则时间片轮转调度算法退化为先来先服务调度算法，并且会增大进程响应时间。因此时间片不能太大。 如果时间片太小，会导致进程切换过于频繁，系统会花大量的时间来处理进程切换，从而导致实际用于进程执行的时间比例减少。优先级调度算法：每个作业/进程有各自的优先级，调度时选择优先级最高的作业/进程。（抢占式和非抢占式） 静态优先级：创建进程时确定，之后一直不变。 动态优先级：创建进程时有一个初始值，之后会根据情况动态地调整优先级。 如何合理设置各类进程的优先级？ 通常，系统进程优先级高于用户进程；前台进程优先级高于后台进程；操作系统更偏好I/O型进程（I/O繁忙型进程）。 如果采用动态优先级，什么时候应该调整？ 可以从追求公平、提升资源利用率等角度考虑。如果某进程在就绪队列中等待了很长时间，则可以适当提升其优先级；如果某进程占用处理机运行了很长时间，则可适当降低其优先级。 多级反馈队列调度算法：（对其他调度算法的折中权衡，抢占式）2.4 进程同步、进程互斥同步：直接制约关系，指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而产生的制约关系。临界资源：一个时间段内只允许一个进程使用的资源。对临界资源的访问，必须互斥地进行。互斥：间接制约关系，当一个进程访问某临界资源时，另一个想要访问该临界资源的进程必须等待。进程互斥的原则：2.4.1 进程互斥的软件实现方法单标志法： 两个进程在访问完临界区后会把使用临界区的权限转交给另一个进程。也就是说每个进程进入临界区的权限只能被另一个进程赋予。双标志先检查法：设置一个布尔数组flag[]，数组中各个元素用来标记各进程想进入临界区的意愿，比如”flag[0]=true“意味着0号进程P0现在想要进入临界区。每个进程在进入临界区之前先检查当前有没有别的进程想进入临界区，如果没有，则把自身对应的标志flag[i]设为true，之后开始访问临界区。双标志后检查法：先上锁后检查。Peterson算法：如果两个进程都想争着进入临界区，那可以让进程尝试”孔融让梨“，主动让对方先使用临界区。解决了进程互斥问题，遵循了空闲让进、忙则等待、有限等待三个原则，但是未遵循让权等待原则。2.4.2 进程互斥的硬件实现方法中断屏蔽方法：利用”开/关中断指令“实现。关中断即不允许当前进程被中断，也必然不会发生进程切换；直到当前进程访问完临界区，再执行开中断指令，才有可能有别的进程上处理机并访问临界区。 优点：简单、高效 缺点：不适用于多处理机；只适用于操作系统内核进程，不适用于用户进程。TestAndSet（TS）指令：用硬件实现，执行的过程不允许被中断，只能一气呵成。Swap指令：用硬件实现，执行的过程不允许被中断，只能一气呵成。逻辑上与TSL一致。2.5 信号量机制信号量：一个变量，表示系统中某种资源的数量。如：系统中只有一台打印机，就可以设置为一个初值为1的信号量。用户进程可以通过使用操作系统提供的一对原语来对信号量进行操作。一对原语：wait(S)原语和signal(S)原语，简称P，V操作，括号里的信号量S就是函数调用时传入的参数。整型信号量：用一个整数型的变量作为信号量。记录型信号量：用记录型数据结构表示的信号量。信号量机制实现进程互斥：信号量机制实现进程同步： 分析什么地方需要实现”同步关系“，即必须保证”一前一后“执行的两个操作； 设置同步信号量S，初始为0； 在”前操作“之后执行V(S)； 在”后操作“之前执行P(S)。信号量机制实现前驱关系：2.6 经典的进程同步、进程互斥问题2.6.1 生产者-消费者问题问题分析： 系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品放入缓冲区，消费者进程每次从缓冲区取出一个产品并使用。生产者、消费者共享一个初始为空、大小为n地缓冲区。只有缓冲区没满时，生产者才能把产品放入缓冲区，否则必须等待。只有缓冲区不空时，消费者才能从中取出产品，否则必须等待。缓冲区是临界资源，各进程必须互斥地访问。PV操作题目分析步骤： 关系分析。找出题目中描述的各个进程，分析它们之间的同步、互斥关系。 整理思路。根据各进程的操作流程确定P、V操作的大致顺序。 设置信号量。设置需要的信号量，并根据题目条件确定信号量初值。（互斥信号量初值一般为1，同步信号量的初始值要看对于资源的初始值是多少）如何用信号量机制实现生产者、消费者进程地这些功能？能否改变相邻P、V操作的顺序？2.6.2 多生产者-多消费者问题如果缓冲区大小为1，那么有可能不需要设置互斥信号量就可以实现互斥访问缓冲区的功能。2.6.3 吸烟者问题2.6.4 读者-写者问题核心思想：设置一个计数器count来记录当前正在访问共享文件的读进程数。2.6.5 哲学家进餐问题关键在于：解决进程死锁。2.7 管程信号量机制存在问题：编写程序困难、易出错。引入管程就是更方便地实现进程互斥和同步。管程：是一种特殊的软件模块（类似C++类）。有以下部分组成： 局部于管程的共享数据结构说明； 对该数据结构进行操作的一组过程（过程就是函数）； 对局部于管程的共享数据设置初始值的语句； 管程有一个名字。管程的基本特征：（常考） 局部于管程的数据只能被局部于管程的过程所访问； 一个进程只有通过调用管程内的过程才能进入管程访问共享数据； 每次仅允许一个进程在管程内执行某个内部过程。用管程解决生产者消费者问题：2.8 死锁死锁：在并发环境下，各进程因竞争资源而造成的一种互相等待对方手里的资源，导致各进程都阻塞，都无法向前推进的现象。如五个科学家拿筷子吃饭，同时拿起了左手边的筷子，都拿不到右手边的筷子。饥饿：由于长期得不到想要的资源，某进程无法向前推进的现象。比如：在短进程优先算法中，若有源源不断的短进程到来，则长进程将一直得不到处理机，从而发生长进程“饥饿”。死循环：某进程执行过程中一直跳不出某个循环的现象。有时是因为程序逻辑bug导致，有时是程序员故意设计的。死锁产生的必要条件： 互斥条件：只有对必须互斥使用的资源的争抢才会导致死锁。 不剥夺条件：进程所获得的资源在未使用完之前，不能由其他进程强行夺走，只能主动释放。 请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源又被其他进程占有，此时请求进程被阻塞，但又对自己已有的资源保持不放。 循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求。什么时候会发生死锁？ 对系统资源的竞争。各进程对不可剥夺的资源（如打印机）的竞争可能引起死锁，对可剥夺的资源（CPU）的竞争是不会引起死锁的。 进程推进顺序非法。请求和释放资源的顺序不当，也同样会导致死锁。 信号量的使用不当也会造成死锁。死锁的处理策略： 预防死锁。破坏死锁产生的四个必要条件中的一个或几个。 避免死锁。用某种方法防止系统进入不安全状态，从而避免死锁（银行家算法） 死锁的检测和解除。允许死锁的发生，不过操作系统会负责检测出死锁的发生，然后采取某种措施解除死锁。2.8.1 死锁的处理策略——预防死锁互斥条件：只有对必须互斥使用的资源的争抢才会导致死锁。 把只能互斥使用的资源改造为允许共享使用，如SPOOLing技术。 缺点：并不是所有的资源都可以改造成可共享使用的资源。并且为了系统安全，很多地方还必须保护这种互斥性。因此，很多时候都无法破坏互斥条件。不剥夺条件：进程所获得的资源在未使用完之前，不能由其他进程强行夺走，只能主动释放。 方案一：当某个进程请求新的资源得不到满足时，它必须立即释放保持的所有资源，待以后需要时再重新申请。 方案二：当某个进程需要的资源被其他进程所占有的时候，可以由操作系统协助，将想要的资源强行剥夺。这种方式一般需要考虑各进程的优先级。 缺点： 实现起来复杂； 释放已获得的资源可能造成前一阶段工作的失效。只适用于易保存和恢复状态的资源，如CPU。 反复地申请和释放资源会增加系统开销，降低系统吞吐量。 若采用方案一，意味着只要暂时得不到某个资源，之前获得的那些资源都需要放弃，以后再重新申请。如果一直发生这样的情况，就会导致进程饥饿。 请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源又被其他进程占有，此时请求进程被阻塞，但又对自己已有的资源保持不放。 采用静态分配方法，即进程在运行前一次申请完它所需要的全部资源，在它的资源未满足前，不让它投入运行。一旦投入运行，这些资源就一直归它所有，该进程就不会再请求别的任何资源了。 缺点：有些资源可能只需要用很短的时间，因此如果进程的整个运行期间都一直保持着所有资源，就会造成严重的资源浪费，资源利用率极低。另外，该资源也有可能导致某些进程饥饿。循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求。 采用顺序资源分配法。首先给系统中的资源编号，规定每个进程必须按编号递增的顺序请求资源，同类资源（即编号相同的资源）一次申请完。 原理分析：一个进程只有已占有小编号的资源时，才有资格申请更大编号的资源。按此规则，已持有大编号资源的进程不可能逆向地回来申请小编号的资源，从而就不会产生循环等待的现象。 缺点：不方便增加新的设备，因为可能需要重新分配所有的编号；进程实际使用资源的顺序可能和编号递增顺序不一致，会导致资源浪费；必须按规定次序申请资源，用户编程麻烦。2.8.2 死锁的处理策略——避免死锁安全序列： 如果系统按照这种序列分配资源，则每个进程都能顺序完成。只要能找出一个安全序列，系统就是安全状态。安全序列可能有多个。如果系统处于安全状态，就一定不会发生死锁。如果系统进入不安全状态，就有可能发生死锁。银行家算法的核心思想：在资源分配之前预先判断这次分配是否会导致系统进入不安全状态，以此决定是否答应资源分配请求。银行家算法步骤： 检查此次申请是否超过了之前声明的最大需求数； 检查此时系统剩余的可用资源是否还能满足这次请求； 试探着分配，更改各数据结构； 用安全性算法检查此次分配是否会导致系统进入不安全状态。2.8.3 死锁的处理策略——检测和解除死锁的检测： 用某种数据结构来保存资源的请求和分配信息； 提供一种算法，利用上述信息来检测系统是否已进入死锁状态。如果能消除所有的边，就称这个图是可完全简化的，此时一定没有发生死锁。如果最终不能消除所有边，那么此时就是发生了死锁。检测死锁的算法：依次消除与不阻塞进程相连的边，直到无边可消。死锁定理：如果某时刻系统的资源分配图是不可完成简化的，那么此时系统死锁。死锁的解除：如何决定“对谁动手”？ 进程优先级 已执行多长时间； 还要多久能完成； 进程已经使用了多少资源； 进程是交互式的还是批处理式的3. 内存管理内存：用于存放数据的硬件。程序执行前需要先放到内存中才能被CPU处理。如何区分各个程序的数据是放在什么地方的？ 给内存的存储单元编地址，内存地址从0开始，每个地址对应一个存储单元。2^10=1K(千)2^20=1M(兆，百万)2^30=1G(十亿，千兆)编译生成的指令一般使用逻辑地址（相对地址）。从写程序到程序运行： 编辑：*.c 编译：*.o，由编译程序将用户源代码编译成若干个目标模块（编译就是把高级语言翻译为机器语言） 链接：*.exe，由链接程序将编译后形成的一组目标模块，以及所需库函数链接在一起，形成一个完整的装入模块； 装入（装载）：由装入程序将装入模块装入内存运行。链接的三种方式： 静态链接：在程序运行之前，先将各目标模块及他们所需的库函数连接成一个完成的可执行文件（装入模块），之后不再拆开； 装入时动态链接：将各目标模块装入内存时，边装入边链接； 运行时动态链接：在程序执行中需要该目标模块时，才对它进行链接。优点是便于修改和更新，便于实现对目标模块的共享。装入的三种方式（用三种不同的方法完成逻辑地址到物理地址的转换）： 绝对装入：在编译时，如果知道程序将放到内存中的哪个位置，编译程序将产生绝对地址的目标代码。装入程序按照装入模块中的地址，将程序和数据装入内存。只适用于单道程序环境。一般情况下都是编译或汇编时再转换为绝对地址。 静态重定位（可重定位装入）：编译、链接后的装入模块的地址都是从0开始的，指令中使用的地址、数据存放的地址都是相对于起始地址而言的逻辑地址。可根据内存的当前情况，将装入模块装入到内存的适当位置。装入时对地址进行“重定位”，将逻辑地址变换为物理地址（地址变换是在装入时一次完成的）。（早期多道批处理阶段） 特点：在一个作业装入内存时，必须分配其要求的全部内存空间，如果没有足够的内存，就不能装入该作业。作业一旦进入内存后，在运行期间就不能再移动，也不能再申请内存空间。 动态重定位（动态运行时装入）：编译、链接后的装入模块的地址都是从0开始的。装入程序把装入模块装入内存后，并不会立即把逻辑地址转换为物理地址，而是把地址转换推迟到程序真正要执行时才进行。因此装入内存后所有的地址依然是逻辑地址。这种方式需要一个重定位寄存器的支持。（现代操作系统） 重定位寄存器：存放装入模块存放的起始位置。 特点：允许程序在内存中发生移动。可将程序分配到不连续的存储区中；在程序运行期只需装入它的部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内存；便于程序段的共享，可以向用户提供一个比存储空间大得多的地址空间。 3.1 内存管理的概念内存管理负责的内容： 内存空间的分配与回收； 提供某种技术从逻辑上对内存空间进行扩充； 提供地址转换功能，负责程序的逻辑地址与物理地址的转换； 提供内存保护功能。保证各进程在各自存储空间内运行，互不干扰。 方法一：在CPU中设置一对上、下限寄存器，存放进程的上、下限地址。进程的指令要访问某个地址时，CPU检测是否越界； 方法二：采用重定位寄存器和界地址寄存器进行越界检查。重定位寄存器中存放的是进程的起始物理地址。界地址寄存器中存放的是进程的最大逻辑地址。 3.1.1 内存空间的扩充——覆盖与交换（不重要）覆盖技术：用来解决”程序大小超过物理内存总和“的问题。 覆盖技术的思想：将程序分为多个段（多个模块）。常用的段常驻内存，不常用的段在需要时调入内存。内存中分为一个“固定区”和若干个“覆盖区”。需要常驻内存的段放在“固定区”中，调入后不再调出（除非运行结束）；不常用的段放在“覆盖区”，需要用到时调入内存，用不到时调出内存。 特点：必须由程序员声明覆盖结构，操作系统完成自动覆盖。缺点：对用户不透明，增加了用户编程负担。覆盖技术只用于早期的操作系统中。交换技术：内存空间紧张时，系统将内存中某些进程暂时换出外存，把外存中某些已具备运行条件的进程换入内存（进程在内存与磁盘间动态调度） 暂时换出外存等待的进程状态为挂起状态（挂起态，suspend），挂起态分为就绪挂起、阻塞挂起。覆盖与交换的区别：覆盖是在同一个程序或进程中的，交换是在不同进程（或作业）之间的。3.1.2 内存空间的分配与回收——连续分配管理方式连续分配：指为用户进程分配的必须是一个连续的内存空间。单一连续分配：固定分区分配：动态分区分配：3. 如何进行分区的分配与回收操作？ 更改空闲分区表的分区大小和起始地址即可。动态分区分配没有内部碎片，但是有外部碎片。可以通过紧凑技术来解决外部碎片。 内部碎片：分配给某进程的内存区域中，如果有些部分没有用上。 外部碎片：是指内存中的某些空闲分区由于太小而难以利用。动态分区分配算法动态分区分配算法：在动态分区分配方式中，当很多个空闲分区都能满足需求时，应该选择哪个分区进行分配？ 首次适应算法：每次都从低地址开始查找，找到第一个能满足大小的空闲分区。 如何实现：空闲分区以地址递增的次序排列。每次分配内存时顺序查找空闲分区链（或空闲分区表），找到大小能满足要求的第一个空闲分区。 最佳适应算法：由于动态分区分配是一种连续分配方式，为各进程分配的空间必须是连续的一整片区域。因此为了保证当”大进程“到来时能有连续的大片空间，就可以尽可能多地留下大片的空闲区，即，优先使用更小的空闲区。 如何实现：空闲分区按容量递增次序链接。每次分配内存时顺序查找空闲分区链（或空闲分区表），找到大小能满足要求的第一个空闲分区。 缺点：每次都选最小的分区进行分配，会留下越来越多的、很小的、难以利用的内存块。因此这种方法会产生很多的外部碎片。 最坏适应算法：在每次分配时优先使用最大的连续空闲区。 如何实现：空闲分区按容量递减次序链接。每次分配内存时顺序查找空闲分区链（或空闲分区表），找到大小能满足要求的第一个空闲分区。 缺点：每次都选最大的分区进行分配，虽然可以让分配后留下的空闲区更大，更可用，但是这种方式会导致较大的连续空闲区被迅速用完。如果之后有”大进程“到达，就没有内存分区可用了。 邻近适应算法：首次适应算法每次都从链头开始查找。这可能会导致低地址部分出现很多小的空闲分区，而每次分配查找时，都要经过这些分区，增加了查找的开销。如果每次都从上次查找结束的位置开始检索，就能解决上述问题。 如何实现：空闲分区以地址递增的顺序排列（可排列一个循环链表）。每次分配内存时从上次查找结束的位置开始查找空闲分区链（或空闲分区表），找到大小能满足要求的第一个空闲分区。 3.1.3 内存空间的分配与回收——非连续分配管理方式基本分页存储管理思想：把内存分为一个个相等的小分区，再按照分区大小把进程拆分成一个个小部分。将内存空间分为一个个大小相等的分区，每个分区就是一个”页框“（或称为页帧、内存块、物理块），每个页框有一个编号，即：页框号”，页框号从0开始。将用户进程的地址空间也分为与页框大小相等的一个个区域，称为“页”或“页面”。每个页面也有一个编号，即“页号”，页号也是从0开始。（进程的最后一个页面可能没有一个页框那么大。因此，页框不能太大，否则可能产生过大的内部碎片）操作系统以页框为单位为各个进程分配内存空间。进程的每个页面分别放入一个页框中。也就是说，进程的页面与内存的页框有一一对应的关系。各个页面不必连续存放，也不必按先后顺序来，可以放到不相邻的各个页框中。将进程地址空间分页之后，操作系统该如何实现逻辑地址到物理地址的转换？如果每个页面大小为$2^kB$，用二进制数表示逻辑地址，则末尾K位即为页内偏移量，其余部分就是页号。页表：记录进程页面和实际存放的内存块之间的对应关系。一个进程对应一张页表，进程的每一页对应一个页表项，每个页表项由页号和块号组成。每个页表项的长度是相同的，页号是“隐含”的。基本地址变换机构（重要）用于实现逻辑地址到物理地址转换的一组硬件机构。通常会在系统中设置一个页表寄存器（PTR），存放页表在内存中的起始地址F和页表长度M。进程未执行时，页表的始址和页表长度放在进程控制块（PCB）中，当进程被调度时，操作系统内核会把它们放到页表寄存器中。具有快表的地址变换机构是基本地址变换机构的改进版本。快表，又称联想寄存器（TLB），是一种访问速度比内存快很多的高速缓冲寄存器，用来存放当前访问的若干页表项，以加速地址变换的股票池。与此对应，内存中的页表常称为慢表。引入快表后，地址的变换过程：两级页表单级页表存在的问题： 页表必须连续存放，因此当页表很大时，需要占用很多个连续的页框。 如何实现地址变换？ 没有必要让整个页表常驻内存，因为进程在一段时间内可能只需要访问某几个特定的页面。 在需要访问页面时才把页面调入内存（虚拟存储技术）。可以在页表项中增加一个标志位，用于表示该页面是否已经调入内存。若想访问的页面不在内存中，则产生缺页中断（内中断），然后将目标页面从外存调入内存。 需要注意的几个细节： 若采用多级页表机制，则各级页表的大小不能超过一个页面。 两级页表的访存次数分析（假设没用快表机构） 第一次访存：访问内存中的页目录表； 第二次访存：访存内存中的二级页表； 第三次访存：访问目标内存单元 基本分段存储管理方式与“分页”最大的区别就是离散分配时所分配地址空间的基本单位不同。进程的地址空间：按照程序自身的逻辑关系划分为若干个段，每个段都有一个段名（在低级语言中，程序员使用段名来编程），每段从0开始编址。内存分配规则：以段为单位，每个段在内存中占据连续空间，但各段之间可以不相邻。分段系统的逻辑地址由段号（段名）和段内地址（段内偏移量）所组成。段表：为了保证程序能正常运行，必须能从物理内存中找到各个逻辑段的存放位置。因此，需要为每个进程建立一张段映射表。分段、分页管理的对比：为什么分段比分页更容易实现信息的共享和保护？ 因为页面不是按逻辑模块划分的。段页式管理方式分页、分段的优缺点分析：采用分段+分页的段页式管理：将进程按逻辑模块分段，再将各段分页（如每个页面4KB），再将内存空间分为大小相同的内存块/页框/页帧/物理块，进程前将各页面分别装入各内存块中。段页式系统的逻辑地址结构由段号、页号、页内地址（页内偏移量）组成。段号的位数决定了每个进程最多可以分几个段，页号位数决定了每个段最大有多少页，页内偏移量决定了页面大小、内存块大小是多少。“分段”对用户是可见的，程序员编程时需要显式地给出段号、段内地址。而将各段“分页”对用户是不可见的。系统会根据段内地址自动划分页号和页内偏移量。因此，段页式管理的地址结构是二维的。每个段对应一个段表项，每个段表项由段号、页表长度、页表存放块号（页表起始地址）组成。每个段表项长度相等，段号是隐含的。每个页面对应一个页表项，每个页表项由页号、页面存放的内存块号组成。每个页表项长度相等，页号是隐含的。地址变换：3.2 虚拟内存管理传统存储管理方式的特征、缺点：虚拟内存的定义和特征：虚拟内存的三个主要特征：如何实现虚拟内存技术？虚拟内存技术，允许一个作业分多次调入内存。如果采用连续分配方式，会不方便实现。因此，虚拟内存的实现需要建立在离散分配的内存管理方式基础上。3.2.1 请求分页存储管理页表机制：缺页中断机构：缺页中断是因为当前执行的指令想要访问的目标页面未调入内存而产生的，因此属于内中断。一条指令在执行期间，可能产生多次缺页中断。地址变换机构：与基本分页不同之处： 找到页表项是需要检查页面是否在内存中； 若页面不在内存中，需要请求调页； 若内存空间不够，还需换出页面； 页面调入内存后，需要修改相应页表项。页面置换算法页面的换入、换出需要磁盘I/O，会有较大的开销，因此好的页面置换算法应该追求更少的缺页率。最佳置换算法（OPT）：每次选择淘汰的页面将是以后永不使用，或者在最长时间内不再被访问的页面，这样可以保证最低的缺页率。缺页率=缺页中断的次数/访问次数。 最佳置换算法可以保证最低的缺页率，但实际上，只有在进程执行的过程中才能知道接下来会访问到的是哪个页面。操作系统无法提前预判页面访问序列。因此，最佳置换算法是无法实现的。先进先出置换算法（FIFO）：每次选择淘汰的页面是最早进入内存的页面。 实现方法：把调入内存的页面根据调入的先后顺序排成一个队列，需要换出页面时选择队头页面即可。队列的最大长度取决于系统为进程分配了多少个内存块。 Belady异常：当为进程分配的物理块数增大时，缺页次数不减反增的异常现象。 只有FIFO算法会产生Belady异常。另外，FIFO算法虽然实现简单，但是该算法与进程实际运行时的规律不适应，因为先进入的页面也有可能最经常被访问。因此，算法性能差。最近最久未使用置换算法（LRU）：每次淘汰的页面是最近最久未使用的页面。 实现方法：赋予每个页面对应的页表项中，用访问字段记录该页面自上次被访问以来所经历的时间t。当需要淘汰一个页面时，选择现有页面中t值最大的，即最近最久未使用的页面。 该算法的实现需要专门的硬件支持，虽然算法性能好，但是实现困难，开销大。时钟置换算法（CLOCK）：性能和开销较为均衡的算法，又称为最久未用算法（NRU）。改进型的时钟置换算法：页面分配策略驻留集：指请求分页存储管理中给进程分配的物理块的集合。在采用了虚拟存储技术的系统中，驻留集大小一般小于进程的总大小。若驻留集太小，会导致缺页频繁，系统要花大量的时间来处理缺页，实际用于进程推进的时间很少；驻留集太大，又会导致多道程序并发度下降，资源利用率降低。所以应该选择一个合适的驻留集大小。固定分配：操作系统为每个进程分配一组固定数目的物理块，在进程运行期间不再改变。即，驻留集大小不变。可变分配：先为每个进程分配一定数目的物理块，在进程运行期间，可根据情况做适当的增加或减少。即，驻留集大小可变。局部置换：发生缺页时只能选进程自己的物理块进行置换。全局置换：可以将操作系统保留的空闲物理块分配给缺页进程，也可以将别的进程持有的物理块置换到外存，再分配给缺页进程。页面分配、置换策略：何时调入页面：从何处调入页面？抖动（颠簸）现象：工作集：指在某段时间间隔里，进程实际访问页面的集合。" }, { "title": "我的第一篇博客！", "url": "/posts/MyFirstBlog/", "categories": "Test", "tags": "Test", "date": "2022-04-22 07:00:00 +0000", "snippet": "这是我的第一篇博客这里可以放代码片段噢～//代码片段int main(){ hello world;}" }, { "title": "计算机网络", "url": "/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/", "categories": "八股, 计算机网络", "tags": "计算机网络", "date": "2022-04-15 12:00:00 +0000", "snippet": "1. 计算机网络1.1 TCP/IP五层协议或OSI七层参考模型TCP/IP五层协议： 应用层：应用层协议（HTTP、SMTP、FTP）分布在多个端系统上，而一个端系统中的应用程序使用协议与另一个端系统中的应用程序交换信息分组，位于应用层的信息分组称为报文；提供用户接口，特指能够发起网络流量的程序 运输层：在应用程序端点之间传送应用层报文。在因特网中，有两种运输协议，即TCP和UDP，利用其中的任一个都能运输应用层报文。运输层的分组称为报文段；（数据成功传给对方电脑后，不知道传给哪个应用程序）提供的是进程间的通用数据传输服务。只有有了IP和端口，我们才能进行准确的通信。 网络层：将运输层的报文段和目的地址封装成数据报，用于下一层的传输，网络层协议有IP，网络层的分组称为数据报；（负责选择最佳路径 规划IP地址） IP协议： IP地址由 32 位的二进制数组成，一般把它分成4段的十进制表示，地址范围为0.0.0.0~255.255.255.255。 每一台想要联网的计算机都会有一个IP地址。这个IP地址被分为两部分，前面一部分代表网络部分，后面一部分代表主机部分。并且网络部分和主机部分所占用的二进制位数是不固定的。如果两台计算机的网络部分完全一致，那么说明这两台计算机处于同一个子网中。如192.168.43.1和192.168.43.2，网络部分为24位，主机部分为8位。 如何判断网络部分和主机部分所占位数？如何判断IP地址是否在一个子网？ 利用子网掩码，子网掩码和IP地址一样也是 32 位二进制数，不过它的网络部分规定全部为 1，主机部分规定全部为 0.这样就知道了所占位数。将IP地址与子网掩码进行与运算，如果结果相同，则在一个子网。 ARP协议：如何知道其他计算机的MAC地址？ 通过广播的形式给同一个子网中的每台电脑发送一个数据包，数据包包含接收方的IP地址，对方收到这个数据包之后，会取出IP地址与自身的对比，如果相同，则把自己的MAC地址回复给对方，否则就丢弃这个数据包。 发送数据通过广播，询问对方MAC地址也是广播，如何区分呢？ 在询问MAC地址的数据包中，在对方的MAC地址这一栏中，填的是一个特殊的MAC地址，其他计算机看到这个特殊的MAC地址之后，就能知道广播想干嘛了。 DNS服务器 如何知道对方的IP地址？ 输入网址域名www.baidu.com，DNS服务器解析域名，返回这个域名对应的IP地址。 链路层：不同的网络类型，发送数据的机制不同，数据链路层就是将数据包封装成能够在不同的网络传输的帧。能够进行差错检验，但不纠错，监测处错误丢掉该帧。制定一套规则来进行0，1的传送。例如多少个电信号为一组啊，每一组信号应该如何标识才能让计算机读懂啊等等。 以太网协议：一组电信号构成一个数据包，我们把这个数据包称之为帧。每一个桢由标头(Head)和数据(Data)两部分组成。Head存储发送者，接收者等信息。而数据部分则是这个数据包具体的，想给接收者的内容。Head长度固定为18个字节。 MAC地址：连入网络的每一个计算机都会有网卡接口，每一个网卡都会有一个唯一的地址，这个地址就叫做 MAC 地址。计算机之间的数据传送，就是通过 MAC 地址来唯一寻找、传送的。MAC地址 由 48 个二进制位所构成，在网卡生产时就被唯一标识。 广播与ARP协议： 在同一个子网中，计算机 A 要向计算机 B 发送一个数据包，这个数据包会包含接收者的 MAC 地址。当发送时，计算机 A 是通过广播的方式发送的，这时同一个子网中的计算机 C, D 也会收到这个数据包的，然后收到这个数据包的计算机，会把数据包的 MAC 地址取出来，与自身的 MAC 地址对比，如果两者相同，则接受这个数据包，否则就丢弃这个数据包。这种发送方式我们称之为广播。 ARP 协议，通过它我们可以知道子网中其他计算机的 MAC 地址。 物理层：将帧中的一个个比特从一个节点移动到下一个节点。负责把两台计算机连起来，然后在计算机之间通过高低电频来传送0,1这样的电信号。 1.2 通信双方如何保证消息不丢失？数据在传输的时候是分割成一小块一小块传输的，我们把这一小块的数据称之为一个分组。我们在传输这块分组的时候，主要面临两个问题： 分组在信道传输中，受到干扰，导致这个分组到达目的地之后出现了差错，例如分组里面的二进制位1变成了0，0变成了1； 给每个分组添加序号，分组中放入校验码，当接收方收到分组时，可根据校验码判断是否出现差错。如果没有差错，接收方就给发送方发送一个ACK分组，告诉对方，数据正确无误。如果出现差错的话，就给对方发送一个NAK分组，告诉对方，分组数据出现了差错。当计算机A收到接受方的反馈之后，如果收到的是ACK分组，那么就继续发送下一个分组数据。如果收到的是NAK分组，那么就重新传输这个分组。接收方根据序号判断传输的是新分组还是重传的。（GBN） 分组还没传输到目的地，就丢失了，我们也把这种情况称之为丢包。 设置一个超时定时器，如果发送方没有收到接收方的反馈，就超时重传。 发送方一次只发送一个分组，效率低。使用流水线协议。 回退N步协议（GBN），也称为滑动窗口协议：在回退N步法中允许发送多个分组而不需要等待确认，但它也受限于在流水线中未确认的分组数不能超过某个最大允许数N。 base为最早的未确认分组的序号，nextseqnum为最小的未使用序号。可以将序号分成4段。在[0, base-1]段内的序号对应已发送并且已经确认的分组序号，[base,nextseqnum-1]段内对应已经发送但未确认的分组序号，[nextseqnum, base+N-1]段内表示即将要被发送的分组序号，大于base+N的序号目前还不能使用，直到当前流水线中未被确认的分组得到确认，窗口整体向右移动之后，才能够被使用。 发送方需要响应以下两个事件： 收到一个ACK：在GBN协议中，对序号为n的分组的确认采取累计确认的方式。也就是说，当A收到序号为n的分组时，表明分组n以及n之前的分组已经被接收方正确接受了。 超时事件：当很久没有收到ACK时，发送方就认为它发送的分组已经丢失了，这时发送方会重传所有已发送但还未被确认的分组。并不是为每个分组设置一个定时器，而是在序号[base,nextseqnum-1]中，设置一个定时器，当base发送的那一刻，就开始计时，当收到一个ACK时，则刷新重新开始计时。 接收方需要响应： 如果一个序号为n的分组被正确收到，并且按序(所谓按序就是指n-1的分组也已经收到了)，则B为分组n发送一个ACK，否则，丢弃该分组，并且为最近按序接收的分组重新发送ACK。 选择重传协议：回退N步协议的缺点也是很明显的，单个分组的差错能够引起GBN重传大量的分组，而且许多分组根本就没有必要重传。在选择重传中，接收方收到失序的分组时，会把它缓存起来，直到拼凑到分组按序，才把分组传输给上一层。而发送方会为每个分组设置一个定时器，这样，只需要重传那些没有被接收方正确接收的分组就可以了。1.3 集线器、交换机与路由器的区别集线器：具备多个网口，专门实现多台计算机的互联作用，集线器是通过网线直接传送数据的，我们说他工作在物理层。存在问题：集线器无法分辨具体信息是发送给谁的，只能广播。小A说话时其他人不能发言，否则信息间会产生碰撞，引发错误，我们叫做各设备处于同一冲突域内。交换机：把用户的网口命名，根据网口名称自动寻址传输数据，解决了冲突的问题，实现了任意两台电脑间的互联，大大地提升了网络间的传输速度。由于交换机是根据网口地址传送信息，比网线直接传送多了一个步骤，我们也说交换机工作在数据链路层。路由器：操作系统不同时，信息的传送形式不匹配。把信息经协议加工成统一形式，再经由一个特殊的设备传送出去。这个设备就是路由器。路由器通过IP地址寻址，我们说它工作在计算机的网络层。1.4 TCP拥塞控制？区分拥塞控制和流量控制：拥塞控制与网络的拥堵情况相关联，而流量控制与接收方的缓存状态相关联。1.4.1 为什么要进行拥塞控制？两台主机在传输数据包的时候，如果发送方迟迟没有收到接收方反馈的 ACK，那么发送方就会认为它发送的数据包丢失了，进而会重新传输这个丢失的数据包。实际情况有可能此时有太多主机正在使用信道资源，导致网络拥塞了，而 A 发送的数据包被堵在了半路，迟迟没有到达 B。这个时候 A 就会误认为是发生了丢包情况，进而重新传输这个数据包。这样会浪费信道资源，使网络更加拥塞，因此需要进行拥塞控制。1.4.2 如何知道网络的拥塞情况？拥塞窗口：发送方一次性连续发送数据包的个数N。两种策略： 先发送一个数据包试探下，如果该数据包没有发生超时事件(也就是没有丢包)。那么下次发送时就发送2个，如果还是没有发生超时事件，下次就发送3个，以此类推，即N = 1, 2, 3, 4, 5….. 刚开始发送1个，如果没有发生超时时间，就发送2个，如果还是没有发送超时事件就发送4个，接着8个…，用翻倍的速度类推,即 N = 1, 2, 4, 8, 16…方法一增长过慢，方法二增长过快，把二者结合起来。设置一个阈值，当增长到阈值时，我们就不在以指数增长了，而是一个一个线性增长。把指数增长阶段称之为慢启动，线性增长阶段称之为拥塞避免。1.4.3 到了瓶颈值怎么办？回到最初的状态，也就是说从1，2，4，8…..开始，但是把阈值调整为瓶颈值的一半。1.4.4 超时事件一定是网络拥塞吗？不一定，有可能是某个数据包出现了丢失或者损害，导致这个数据包超时事件发送。为了防止这种情况，通过冗余 ACK来处理。数据包是有序号的，如果A给B发送M1, M2, M3, M4, M5…N个数据包，如果B收到了M1, M2, M4….却始终没有收到M3，这个时候就会重复确认M2，意在告诉A,M3还没收到，可能是丢失。当A连续收到了三个确认M2的ACK，且M3超时事件还没发生。A就知道M3可能丢失了，这个时候A就不必等待M3设置的计时器到期了，而是快速重传M3。并且把阈值设置为MAX的一半，但是这个时候并非把控制窗口N设置为1，而是让N = 阈值，N再一个一个增长。这种情况称为快速恢复，这种具有快速恢复的TCP版本称之为TCP Reno。还有另外一种TCP版本，无论是收到三个相同的ACK还是发生超时事件，都把拥塞窗口的大小设为1，从最初状态开始，这种版本的TCP我们称之为TCP Tahoe。1.5 TCP流量控制流量控制：对发送方速率的控制。1.5.1 为什么要进行流量控制？双方在通信的时候，发送方的速率与接收方的速率是不一定相等，如果发送方的发送速率太快，会导致接收方处理不过来，这时候接收方只能把处理不过来的数据存在缓存区里（失序的数据包也会被存放在缓存区里）。如果缓存区满了发送方还在疯狂着发送数据，接收方只能把收到的数据包丢掉，大量的丢包会极大着浪费网络资源，因此，我们需要控制发送方的发送速率，让接收方与发送方处于一种动态平衡才好。1.5.2 如何进行流量控制？接收方每次收到数据包，可以在发送确定报文的时候，同时告诉发送方自己的缓存区还剩余多少是空闲的，我们也把缓存区的剩余大小称之为接收窗口大小，用变量 win来表示接收窗口的大小。发送方收到之后，便会调整自己的发送速率，也就是调整自己发送窗口的大小，当发送方收到接收窗口的大小为0时，发送方就会停止发送数据，防止出现大量丢包情况的发生。1.5.3 发送方何时继续发送数据？当发送方收到接受窗口 win = 0 时，这时发送方停止发送报文，并且同时开启一个定时器，每隔一段时间就发个测试报文去询问接收方，打听是否可以继续发送数据了，如果可以，接收方就告诉他此时接受窗口的大小；如果接受窗口大小还是为0，则发送方再次刷新启动定时器。1.5.4 接收窗口大小固定吗？早期固定，随着网络发展，固定大小的窗口不灵活，成为TCP性能瓶颈之一。现在的不固定，接收窗口的大小是根据某种算法动态调整的。1.5.5 接收窗口越大越好吗？接收窗口太小：浪费链路利用率，增加丢包率。当接收窗口达到某个值的时候，再增大的话也不怎么会减少丢包率的了，而且还会更加消耗内存。所以接收窗口的大小必须根据网络环境以及发送方的拥塞窗口来动态调整。1.5.6 发送窗口和接收窗口相等吗？接收方在发送确认报文的时候，会告诉发送发自己的接收窗口大小，而发送方的发送窗口会据此来设置自己的发送窗口，但这并不意味着他们就会相等。首先接收方把确认报文发出去的那一刻，就已经在一边处理堆在自己缓存区的数据了，所以一般情况下接收窗口 &gt;= 发送窗口。1.6 TCP三次握手和四次挥手1.6.1 三次握手为什么需要三次握手？1、第一次握手：客户端给服务端发一个 SYN 报文，并指明客户端的初始化序列号 ISN(c)。此时客户端处于 SYN_Send 状态。2、第二次握手：服务器收到客户端的 SYN 报文之后，会以自己的 SYN 报文作为应答，并且也是指定了自己的初始化序列号 ISN(s)，同时会把客户端的 ISN + 1 作为 ACK 的值，表示自己已经收到了客户端的 SYN，此时服务器处于 SYN_REVD 的状态。3、第三次握手：客户端收到 SYN 报文之后，会发送一个 ACK 报文，当然，也是一样把服务器的 ISN + 1 作为 ACK 的值，表示已经收到了服务端的 SYN 报文，此时客户端处于 establised 状态。4、服务器收到 ACK 报文之后，也处于 establised 状态，此时，双方以建立起了链接。三次握手的作用？1、确认双方的接受能力、发送能力是否正常。2、指定自己的初始化序列号，为后面的可靠传送做准备。为什么只有三次握手才能确认双方的接受与发送能力是否正常，而两次却不可以？第一次握手：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。第二次握手：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。第三次握手：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。因此，需要三次握手才能确认双方的接收与发送能力是否正常。（ISN）是固定的吗？三次握手的一个重要功能是客户端和服务端交换ISN(Initial Sequence Number), 以便让对方知道接下来接收数据的时候如何按序列号组装数据。如果ISN是固定的，攻击者很容易猜出后续的确认号，因此 ISN 是动态生成的。什么是半连接队列？服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放在一个队列里，我们把这种队列称之为半连接队列。当然还有一个全连接队列，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。三次握手过程中可以携带数据吗？第三次握手时可以。假如第一次握手可以携带数据的话，如果有人要恶意攻击服务器，那他每次都在第一次握手中的 SYN 报文中放入大量的数据，因为攻击者根本就不理服务器的接收、发送能力是否正常，然后疯狂着重复发 SYN 报文的话，这会让服务器花费很多时间、内存空间来接收这些报文。对于第三次的话，此时客户端已经处于 established 状态，也就是说，对于客户端来说，他已经建立起连接了，并且也已经知道服务器的接收、发送能力是正常的了。1.6.2 四次挥手刚开始双方都处于 establised 状态，假如是客户端先发起关闭请求，则：1、第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于FIN_WAIT1状态。2、第二次挥手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 + 1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 CLOSE_WAIT状态。3、第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 LAST_ACK 的状态。4、第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 + 1 作为自己 ACK 报文的序列号值，此时客户端处于 TIME_WAIT 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态。5、服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。为什么客户端发送 ACK 之后不直接关闭，而是要等一阵子才关闭？要确保服务器是否已经收到了我们的 ACK 报文，如果没有收到的话，服务器会重新发 FIN 报文给客户端，客户端再次收到 ACK 报文之后，就知道之前的 ACK 报文丢失了，然后再次发送 ACK 报文。 TIME_WAIT 持续的时间至少是一个报文的来回时间。一般会设置一个计时，如果过了这个计时没有再次收到 FIN 报文，则代表对方成功就是 ACK 报文，此时处于 CLOSED 状态。 LISTEN – 侦听来自远方TCP端口的连接请求； SYN-SENT -在发送连接请求后等待匹配的连接请求； SYN-RECEIVED – 在收到和发送一个连接请求后等待对连接请求的确认； ESTABLISHED- 代表一个打开的连接，数据可以传送给用户； FIN-WAIT-1 – 等待远程TCP的连接中断请求，或先前的连接中断请求的确认； FIN-WAIT-2 – 从远程TCP等待连接中断请求； CLOSE-WAIT – 等待从本地用户发来的连接中断请求； CLOSING -等待远程TCP对连接中断的确认； LAST-ACK – 等待原来发向远程TCP的连接中断请求的确认； TIME-WAIT -等待足够的时间以确保远程TCP接收到连接中断请求的确认； CLOSED – 没有任何连接状态；1.7 什么是HTTP？1.7.1 HTTP/0.9HTTP 是基于 TCP/IP 协议的应用层协议。它不涉及数据包（packet）传输，主要规定了客户端和服务器之间的通信格式，默认使用80端口。0.9版本只有get命令。协议规定，服务器只能回应HTML格式的字符串，不能回应别的格式。服务器发送完毕，就关闭TCP连接。1.7.2 HTTP/1.0任何格式的内容都可以发送，除了GET命令，还引入了POST命令和HEAD命令，丰富了浏览器与服务器的互动手段。HTTP请求和回应的格式也变了。除了数据部分，每次通信都必须包括头信息（HTTP header），用来描述一些元数据。其他的新增功能还包括状态码（status code）、多字符集支持、多部分发送（multi-part type）、权限（authorization）、缓存（cache）、内容编码（content encoding）等。回应的格式是”头信息 + 一个空行（\\r\\n） + 数据”。其中，第一行是”协议版本 + 状态码（status code） + 状态描述”。头信息必须是 ASCII 码，后面的数据可以是任何格式。服务器回应的时候，必须告诉客户端，数据是什么格式，这就是Content-Type字段的作用。缺点：每个TCP连接只能发送一个请求。发送数据完毕，连接就关闭，如果还要请求其他资源，就必须再新建一个连接。使用keep-alive来保持TCP链接。1.7.3 HTTP/1.1持久连接：TCP连接默认不关闭，可以被多个请求复用，不用声明Connection: keep-alive。客户端和服务器发现对方一段时间没有活动，就可以主动关闭连接。管道机制：在同一个TCP连接里面，客户端可以同时发送多个请求。这样就进一步改进了HTTP协议的效率。如何区分数据包属于哪个回应？ Content-length字段，声明本次回应的数据长度。 使用Content-Length字段的前提条件是，服务器发送回应之前，必须知道回应的数据长度。对于一些很耗时的动态操作来说，这意味着，服务器要等到所有操作完成，才能发送数据，显然这样的效率不高。更好的处理方法是，产生一块数据，就发送一块，采用”流模式”（stream）取代”缓存模式”（buffer）。分块传输编码：每个非空的数据块之前，会有一个16进制的数值，表示这个块的长度。最后是一个大小为0的块，就表示本次回应的数据发送完了。缺点：同一个TCP连接里面，所有的数据通信是按次序进行的。服务器只有处理完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为”队头堵塞”（Head-of-line blocking）。1.7.4 HTTP/2HTTP/1.1 版的头信息肯定是文本（ASCII编码），数据体可以是文本，也可以是二进制。HTTP/2 则是一个彻底的二进制协议，头信息和数据体都是二进制，并且统称为”帧”（frame）：头信息帧和数据帧。二进制的好处：可以定义额外的帧如果使用文本实现这种功能，解析数据将会变得非常麻烦。多工：HTTP/2 复用TCP连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应，这样就避免了”队头堵塞”。这样双向的、实时的通信，就叫做多工（Multiplexing）。数据流 因为 HTTP/2 的数据包是不按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。 HTTP/2 将每个请求或回应的所有数据包，称为一个数据流（stream）。每个数据流都有一个独一无二的编号。数据包发送的时候，都必须标记数据流ID，用来区分它属于哪个数据流。另外还规定，客户端发出的数据流，ID一律为奇数，服务器发出的，ID为偶数。 数据流发送到一半的时候，客户端和服务器都可以发送信号（RST_STREAM帧），取消这个数据流。1.1版取消数据流的唯一方法，就是关闭TCP连接。这就是说，HTTP/2 可以取消某一次请求，同时保证TCP连接还打开着，可以被其他请求使用。 客户端还可以指定数据流的优先级。优先级越高，服务器就会越早回应。头信息压缩：HTTP 协议不带有状态，每次请求都必须附上所有信息。所以，请求的很多字段都是重复的，浪费带宽。通过头信息压缩机制，一方面，头信息使用gzip或compress压缩后再发送；另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。服务器推送：允许服务器未经请求，主动向客户端发送资源。服务器可以预期到客户端请求网页后，很可能会再请求静态资源，所以就主动把这些静态资源随着网页一起发给客户端了。1.8 什么是HTTPS？http协议没有对数据加密，都是明文传输的。如何对数据加密？ 对称加密：在每次发送真实数据之前，服务器先生成一把密钥，然后先把密钥传输给客户端。之后服务器给客户端发送真实数据的时候，会用这把密钥对数据进行加密，客户端收到加密数据之后，用刚才收到的密钥进行解密。 对称加密的问题：密钥是通过明文传输的，如果密钥被中间人给捕获了，那么在之后服务器和客户端的加密传输过程中，中间人也可以用他捕获的密钥进行解密。这样的话，加密的数据在中间人看来和明文没啥两样。 非对称加密：让客户端和服务器都拥有两把钥匙，一把钥匙是公开的(全世界知道都没关系)，我们称之为公钥；另一把钥匙则是保密的(只有自己本人才知道)，我们称之为私钥。这样，用公钥加密的数据，只有对应的私钥才能解密；用私钥加密的数据，只有对应的公钥才能解密。 非对称加密的问题：加密速度慢，比对称加密慢了上百倍。 对称和非对称加密结合：用非对称加密的方式来传输对称加密过程中的密钥，之后我们就可以采取对称加密的方式来传输数据。服务器用明文的方式给客户端发送自己的公钥，客户端收到公钥之后，会生成一把密钥(对称加密用的)，然后用服务器的公钥对这把密钥进行加密，之后再把密钥传输给服务器，服务器收到之后进行解密，最后服务器就可以安全着得到这把密钥了，而客户端也有同样一把密钥，他们就可以进行对称加密了。 非对称加密也不一定安全，中间人截取服务器发送给客户端的明文公钥，中间人用自己的公钥冒充服务器的传输给客户端。客户端用中间人的公钥生成自己的密钥，被加密的密钥传输给服务器。中间人截取密钥，中间人用自己的私钥对密钥解密，解密后中间人就可以获得这把密钥了。中间人再对这把密钥用刚才服务器的公钥进行加密，再发给服务器。 非对称加密不安全是因为客户端不知道公钥是不是属于服务器的。 数字证书：证明公钥是属于服务器的，需要找到一个拥有公信力、大家都认可的认证中心(CA)。服务器在给客户端传输公钥的过程中，会把公钥以及服务器的个人信息通过Hash算法生成信息摘要。为了防止信息摘要被人调换，服务器还会用CA提供的私钥对信息摘要进行加密来形成数字签名。并且，最后还会把原来没Hash算法之前的个人信息以及公钥 和 数字签名合并在一起，形成数字证书。当客户端拿到这份数字证书之后，就会用CA提供的公钥来对数字证书里面的数字签名进行解密来得到信息摘要，然后对数字证书里服务器的公钥以及个人信息进行Hash得到另外一份信息摘要。最后把两份信息摘要进行对比，如果一样，则证明这个人是服务器，否则就不是。 CA的公钥是怎么拿给客户端的？服务器怎么有CA的私钥？ 服务器一开始就向认证中心申请了这些证书，而客户端是，也会内置这些证书。当客户端收到服务器传输过来的数据数字证书时，就会在内置的证书列表里，查看是否有解开该数字证书的公钥。 1.9 SSL/TLS协议HTTPS是其实就是在HTTP协议上套上一层SSL加密。作用： 窃听风险：第三方可以获知通信内容 篡改风险：第三方可以修改通信内容 冒充风险：第三方可以冒充他人身份参与通信SSL/TLS协议可以实现： 所有信息都是加密传播，第三方无法窃听 具有校验机制，一旦被篡改，通信双方会立刻发现 配备身份证书，防止身份被冒充基本运行过程：采用公钥加密法，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。前两步称为握手阶段。（1） 客户端向服务器端索要并验证公钥。（2） 双方协商生成”对话密钥”。（3） 双方采用”对话密钥”进行加密通信。握手阶段的过程： 客户端发出请求(ClientHello)：客户端向服务器发出加密通信的请求，客户端提供信息有：支持的协议版本，比如TLS 1.0版；一个客户端生成的随机数，稍后用于生成”对话密钥”；支持的加密方法，比如RSA公钥加密；支持的压缩方法。 服务器回应(SeverHello)：确认使用的加密通信协议版本；一个服务器生成的随机数，稍后用于生成”对话密钥”；确认使用的加密方法，比如RSA公钥加密；服务器证书。 客户端回应：客户端收到服务器回应以后，首先验证服务器证书。如果证书不是可信机构颁布、或者证书中的域名与实际域名不一致、或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。如果证书没有问题，客户端就会从证书中取出服务器的公钥。然后，向服务器发送下面三项信息。 一个随机数。该随机数用服务器公钥加密，防止被窃听。 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。 客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供服务器校验。 三个随机数，生成对话密钥。为什么是三个随机数？ SSL协议不信任每个主机都能产生完全随机的随机数，如果随机数不随机，那么pre master secret就有可能被猜出来，因此必须引入新的随机因素。 服务器的最后回应：服务器收到客户端的第三个随机数pre-master key之后，计算生成本次会话所用的”会话密钥”。然后，向客户端最后发送下面信息。 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。 服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供客户端校验。 如何保证公钥不被篡改？将公钥放在数字证书中，只要证书是可信的，公钥就是可信的。公钥计算量太大，如何减少消耗的时间？每一次对话（session），客户端和服务器端都生成一个”对话密钥”（session key），用它来加密信息。由于”对话密钥”是对称加密，所以运算速度非常快，而服务器公钥只用于加密”对话密钥”本身，这样就减少了加密运算的消耗时间。1.10 DNSDNS：Domain Name System，根据域名查出IP地址。查询过程： 查询参数和统计； 查询内容； DNS服务器的答复； 显示stackexchange.com的NS记录（Name Server的缩写），即哪些服务器负责管理stackexchange.com的DNS记录； 四个域名服务器的IP地址； DNS服务器的一些传输信息。DNS服务器：本机一定知道DNS服务器的IP地址，否则上不了网。DNS服务器的IP地址，有可能是动态的，每次上网时由网关分配，这叫做DHCP机制；也有可能是事先指定的固定地址。域名的层级：DNS通过分级查询每个域名的IP地址，www.example.com真正的域名是www.example.com.root，简写为www.example.com.。因为，根域名.root对于所有域名都是一样的，所以平时是省略的。根域名的下一级是顶级域名，如.com、.net；次级域名，比如www.example.com里面的.example，这一级域名是用户可以注册的；主机名(host)，比如www.example.com里面的www，又称为”三级域名”，这是用户在自己的域里面为服务器分配的名称，是用户可以任意分配的。分级查询：从根域名开始，依次查询每一级域名的NS记录，直到查到最终的IP地址。根域名服务器的IP地址的NS记录和IP地址一般不变，内置在DNS服务器中。域名与IP之前的对应关系称为记录，记录分为不同的类型：（1） A：地址记录（Address），返回域名指向的IP地址。（2） NS：域名服务器记录（Name Server），返回保存下一级域名信息的服务器地址。该记录只能设置为域名，不能设置为IP地址。（3）MX：邮件记录（Mail eXchange），返回接收电子邮件的服务器地址。（4）CNAME：规范名称记录（Canonical Name），返回另一个域名，即当前查询的域名是另一个域名的跳转。（5）PTR：逆向查询记录（Pointer Record），只用于从IP地址查询域名。1.11 DHCPip地址是如何自动获取的？客户端发送广播，广播的目的 ip 是 255.255.255.255，目的端口是 68，为了让别人知道它是来请求一个 ip 的，我们的客户端会把 0.0.0.0 作为自己的源 ip，源端口是 67。意在告诉别人：我现在啥也没有，急需一个 ip，哪位老铁能给我提供一个 ip。我们把这个请求 ip 的报文称之为 discover 报文。DHCP响应：当 DHCP 服务器收到这个报文之后，一看源地址是 0.0.0.0，知道这是一个请求 ip 的报文，DHCP 服务器就会给它提供一个 ip，包括 ip 地址，子码掩码，网关，ip 的有效期等信息。如何把ip交给客户端？在 discover 报文中，就会包含客户端的 MAC 地址。DHCP 服务器只需要发一个广播报文就可以了，广播报文的源ip是 DHCP 服务器自己的 ip，源端口是 67，目的地址是 255.255.255.255，目的端口是 68。我们把 DHCP 提供 ip 地址的报文称之为offer报文。客户端挑选ip地址：有可能不止一台服务器收到了discover请求报文，客户端可能收到多个offer报文，客户端一般会选择最先收到的offer报文，选择好之后，会给对应的 DHCP 服务器次发送一个 request 报文，意在告诉它，我看中了你的报文。DHCP 收到 request 报文之后，会给它回复一个 ACK 报文，并且把这个分配出去的 ip 进行登记（例如把这个 ip 标记为已使用状态）。主机收到ACK报文后，就可以上网冲浪了。如果DHCP服务器不在局域网，discover 报文 就会通过我们的网关来进行传递，并且会把源 ip 替换成网络的 ip，源端口是 68。DHCP 服务器收到报文之后，就可以根据源端口 68 来判断这是一个 discover 请求报文了。就会把 offer 发给网关，网关再发给我们的主机。租期：在DHCP客户端的租约时间到达 1/2 时，客户端会向为它分配 IP 地址的DHCP服务器发送 request 单播报文，以进行 IP 租约的更新。如果服务器判断客户端可以继续使用这个 IP 地址，就回复 ACK 报文，通知客户端更新租约成功。如果此IP地址不能再分配给客户端，则回复 NAK 报文，通知客户端续约失败。如果客户端在租约到达 1/2 时续约失败，客户端会在租约到 7/8 时间时，广播发送 request 报文进行续约。DHCP服务器处理同首次分配 IP 地址的流程。1.12 广播路由算法广播风暴：当节点形成圈时，节点之间不停着发送广播分组，这时网络上充斥着大量重复的广播分组，这将会严重影响资源的利用。控制广播风暴：给广播分组做标记。例如，源节点(发起广播的节点)可以将其地址以及广播序号放入这个广播分组中，然后发送给他的所有邻居节点，每个节点会维护它已经收到的、转发的源地址和广播分组的序号列表。当节点收到一个广播分组时，会检查这个广播分组是否之前接收过(可以通过源地址、报文序号来检查)，如果接收过，那么就把该广播分组丢弃，否则，把该广播分组接收，且向所有邻居节点转发。生成树广播：上面的方法会存在很多的冗余分组(那些被丢弃的广播分组就是冗余的广播分组)，最理想的情况是找到最小生成树，让广播报文在最小生成树的路径中传送。先选出一个中心节点，然后其他节点向这个中心节点发送加入树报文，加入树报文经过的路径，都会被嫁接到生成树上。1.13 SQL注入攻击SQL注入：通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，服务器拿到这个字符串之后，会把这个字符串作为 sql 的执行参数去数据库查询，然而这个参数是恶意的，以至于服务器执行这条 sql 命令之后，出现了问题。应对方法： 参数绑定：使用预编译手段，绑定参数是最好的防SQL注入的方法。目前许多的ORM框架及JDBC等都实现了SQL预编译和参数绑定功能，攻击者的恶意SQL会被当做SQL的参数而不是SQL命令被执行。在mybatis的mapper文件中，对于传递的参数我们一般是使用 # 和$来获取参数值。 使用正则表达式过滤传入的参数，例如把出现双-的过滤掉等等。1.14 XSS攻击？XSS攻击是指恶意攻击者利用网站没有对用户提交数据进行转义处理或者过滤不足的缺点，进而添加一些脚本代码嵌入到web页面中去，使别的用户访问都会执行相应的嵌入代码，从而盗取用户资料、利用用户身份进行某种动作或者对访问者进行病毒侵害的一种攻击方式。危害：盗取各类用户帐号；控制企业数据；非法转账；强制发送电子邮件原因：过分信任客户端提交的数据，对用户所提交的数据过滤不足。解决方法：将重要的cookie标记为http only, 这样的话Javascript 中的document.cookie语句就不能获取到cookie了；表单数据规定值的类型；对数据进行Html Encode 处理；过滤或移除特殊的Html标签；过滤JavaScript 事件的标签1.15 NAT网络地址转换协议IPV4的IP地址数量有限，内网中不要把每台电脑都分配ip地址。在我们这个内网里，我们可以指定自己的规则，例如，我们可以给这三台电脑随便分配三个IP(请注意，这三个IP不是去申请的，而且我自己随意给它分配的)。分别分配电脑A = 192.168.1.2 电脑B = 192.168.1.3 电脑C = 192.168.1.4。为了让 A 可以访问百度，那么我们可以采取这样的方法：让网关去帮助 A 访问，然后百度把结果传递给网关，而网关再把结果传递给 A。在访问时，把 A的IP + 端口 映射成 网关的IP+端口就可以区分是A、B、C哪个访问。1.16 区分LAN、WAN、WLAN、VLAN、VPN1、局域网LAN和广域网WAN公司办公室内部的网络。短距离称为局域网LAN，长距离称为广域网WAN。2、无线局域网WLAN与广域网WAN的区别是，WAN采用网线连接，WLAN采用无线。3、虚拟局域网VLAN在没有VLAN技术之前，所有连接在同一台交换机的主机工作在一个广播域。但是往往这些主机并不在一个网段，而不同网段的主机不需要广播通信。为了防止收到不需要的广播信息，发明了VLAN技术，在交换机上将相同的网段的主机放在一个VLAN，一个VLAN对应一个广播域。4、虚拟私有网络VPN互联网是开放的、共享的，但是可以借助加密/解密技术，让互联网成为公司的私有网络，仿佛互联网是公司独有的。也可以理解为一种私有专线，但相比物理专线，价格要便宜一个数量级。1.17 TCP和UDP传输控制协议 TCP（Transmission Control Protocol）：面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。首部字段：20个字节，序号seq、确认号ack、数据偏移(首部长度)、确认ack、同步SYN、终止FIN、窗口。用户数据报协议 UDP（User Datagram Protocol）：无连接的，尽最大可能交付，没有拥塞控制，面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。例如：视频传输、实时通信。 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。区别：(1)TCP是可靠传输,UDP是不可靠传输;(2)TCP面向连接,UDP无连接;(3)TCP传输数据有序,UDP不保证数据的有序性;(4)TCP不保存数据边界,UDP保留数据边界;(5)TCP传输速度相对UDP较慢;(6)TCP有流量控制和拥塞控制,UDP没有;(７)TCP是重量级协议,UDP是轻量级协议;(８)TCP首部20字节,UDP首部8字节;基于TCP、UDP的常用协议：TCP：TTP、HTTPS、FTP、TELNET、SMTP(简单邮件传输协议)协议；UDP：DNS、DHCP、TFTP、SNMP(简单网络管理协议)、RIP。1.18 GET和POST的区别？各自使用场景？ GET 被强制服务器支持 浏览器对URL的长度有限制，所以GET请求不能代替POST请求发送大量数据 GET请求发送数据更小 GET请求是不安全的 GET请求是幂等的 幂等的意味着对同一URL的多个请求应该返回同样的结果 POST请求不能被缓存 POST请求相对GET请求是「安全」的 这里安全的含义仅仅是指是非修改信息 GET用于信息获取，而且是安全的和幂等的 所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET 请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。 POST是用于修改服务器上的资源的请求 发送包含未知字符的用户输入时，POST 比 GET 更稳定也更可靠1.19 ABC类地址和私有地址A类地址(1~126)：网络号占前8位，以0开头，主机号占后24位。B类地址(128~191)：网络号占前16位，以10开头，主机号占后16位。C类地址(192~223)：网络号占前24位，以110开头，主机号占后8位。D类地址(224~239)：以1110开头，保留位多播地址。E类地址(240~255)：以1111开头，保留位今后使用1.20 在浏览器中输入 URL 地址到显示主页的过程？ DNS 解析：浏览器查询 DNS，获取域名对应的 IP 地址：具体过程包括浏览器搜索自身的 DNS 缓存、搜索操作系统的 DNS 缓存、读取本地的 Host 文件和向本地 DNS 服务器进行查询等。对于向本地 DNS 服务器进行查询，如果要查询的域名包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析(此解析具有权威性)；如果要查询的域名不由本地 DNS 服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个 IP 地址映射，完成域名解析（此解析不具有权威性）。如果本地域名服务器并未缓存该网址映射关系，那么将根据其设置发起递归查询或者迭代查询； TCP 连接：浏览器获得域名对应的 IP 地址以后，浏览器向服务器请求建立链接，发起三次握手； 发送 HTTP 请求：TCP 连接建立起来后，浏览器向服务器发送 HTTP 请求； 服务器处理请求并返回 HTTP 报文：服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器； 浏览器解析渲染页面：浏览器解析并渲染视图，若遇到对 js 文件、css 文件及图片等静态资源的引用，则重复上述步骤并向服务器请求这些资源；浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面。 连接结束。1.3 子网划分1.4 IP数据报分片1.5 TCP/UDP的特点和区别1.6 TCP滑动窗口1.7 拥塞控制以及快速重传1.8 TCP的三次握手和四次挥手1.9 路由选择" }, { "title": "C++难点", "url": "/posts/C++%E9%9A%BE%E7%82%B9/", "categories": "八股, C++", "tags": "C++", "date": "2022-04-15 12:00:00 +0000", "snippet": "1、C++ 11有哪些新特性？ nullptr替代 NULL 引入了 auto 和 decltype 这两个关键字实现了类型推导 基于范围的 for 循环for(auto&amp; i : res){} 类和结构体的中初始化列表 Lambda 表达式（匿名函数） std::forward_list（单向链表） 右值引用和move语义 …2、auto、decltype和decltype(auto)的用法（1）autoC++11新标准引入了auto类型说明符，用它就能让编译器替我们去分析表达式所属的类型。和原来那些只对应某种特定的类型说明符(例如 int)不同，auto 让编译器通过初始值来进行类型推演。从而获得定义变量的类型，所以说 auto 定义的变量必须有初始值。举个例子：//普通；类型int a = 1, b = 3;auto c = a + b;// c为int型//const类型const int i = 5;auto j = i; // 变量i是顶层const, 会被忽略, 所以j的类型是intauto k = &amp;i; // 变量i是一个常量, 对常量取地址是一种底层const, 所以b的类型是const int*const auto l = i; //如果希望推断出的类型是顶层const的, 那么就需要在auto前面加上cosnt//引用和指针类型int x = 2;int&amp; y = x;auto z = y; //z是int型不是int&amp; 型auto&amp; p1 = y; //p1是int&amp;型auto p2 = &amp;x; //p2是指针类型int*Copy to clipboardErrorCopied（2）decltype有的时候我们还会遇到这种情况，我们希望从表达式中推断出要定义变量的类型，但却不想用表达式的值去初始化变量。还有可能是函数的返回类型为某表达式的值类型。在这些时候auto显得就无力了，所以C++11又引入了第二种类型说明符decltype，它的作用是选择并返回操作数的数据类型。在此过程中，编译器只是分析表达式并得到它的类型，却不进行实际的计算表达式的值。int func() {return 0};//普通类型decltype(func()) sum = 5; // sum的类型是函数func()的返回值的类型int, 但是这时不会实际调用函数func()int a = 0;decltype(a) b = 4; // a的类型是int, 所以b的类型也是int//不论是顶层const还是底层const, decltype都会保留 const int c = 3;decltype(c) d = c; // d的类型和c是一样的, 都是顶层constint e = 4;const int* f = &amp;e; // f是底层constdecltype(f) g = f; // g也是底层const//引用与指针类型//1. 如果表达式是引用类型, 那么decltype的类型也是引用const int i = 3, &amp;j = i;decltype(j) k = 5; // k的类型是 const int&amp;//2. 如果表达式是引用类型, 但是想要得到这个引用所指向的类型, 需要修改表达式:int i = 3, &amp;r = i;decltype(r + 0) t = 5; // 此时是int类型//3. 对指针的解引用操作返回的是引用类型int i = 3, j = 6, *p = &amp;i;decltype(*p) c = j; // c是int&amp;类型, c和j绑定在一起//4. 如果一个表达式的类型不是引用, 但是我们需要推断出引用, 那么可以加上一对括号, 就变成了引用类型了int i = 3;decltype((i)) j = i; // 此时j的类型是int&amp;类型, j和i绑定在了一起Copy to clipboardErrorCopied（3）decltype(auto)decltype(auto)是C++14新增的类型指示符，可以用来声明变量以及指示函数返回类型。在使用时，会将“=”号左边的表达式替换掉auto，再根据decltype的语法规则来确定类型。举个例子：int e = 4;const int* f = &amp;e; // f是底层constdecltype(auto) j = f;//j的类型是const int* 并且指向的是eCopy to clipboardErrorCopied3、C++中NULL和nullptr区别算是为了与C语言进行兼容而定义的一个问题吧NULL来自C语言，一般由宏定义实现，而 nullptr 则是C++11的新增关键字。在C语言中，NULL被定义为(void*)0,而在C++语言中，NULL则被定义为整数0。编译器一般对其实际定义如下：#ifdef __cplusplus#define NULL 0#else#define NULL ((void *)0)#endifCopy to clipboardErrorCopied在C++中指针必须有明确的类型定义。但是将NULL定义为0带来的另一个问题是无法与整数的0区分。因为C++中允许有函数重载，所以可以试想如下函数定义情况：#include &lt;iostream&gt;using namespace std;void fun(char* p) { cout &lt;&lt; \"char*\" &lt;&lt; endl;}void fun(int p) { cout &lt;&lt; \"int\" &lt;&lt; endl;}int main(){ fun(NULL); return 0;}//输出结果：intCopy to clipboardErrorCopied那么在传入NULL参数时，会把NULL当做整数0来看，如果我们想调用参数是指针的函数，该怎么办呢?。nullptr在C++11被引入用于解决这一问题，nullptr可以明确区分整型和指针类型，能够根据环境自动转换成相应的指针类型，但不会被转换为任何整型，所以不会造成参数传递错误。nullptr的一种实现方式如下：const class nullptr_t{public: template&lt;class T&gt; inline operator T*() const{ return 0; } template&lt;class C, class T&gt; inline operator T C::*() const { return 0; }private: void operator&amp;() const;} nullptr = {};Copy to clipboardErrorCopied以上通过模板类和运算符重载的方式来对不同类型的指针进行实例化从而解决了(void*)指针带来参数类型不明的问题，另外由于nullptr是明确的指针类型，所以不会与整形变量相混淆。但nullptr仍然存在一定问题，例如：#include &lt;iostream&gt;using namespace std;void fun(char* p){ cout&lt;&lt; \"char* p\" &lt;&lt;endl;}void fun(int* p){ cout&lt;&lt; \"int* p\" &lt;&lt;endl;}void fun(int p){ cout&lt;&lt; \"int p\" &lt;&lt;endl;}int main(){ fun((char*)nullptr);//语句1 fun(nullptr);//语句2 fun(NULL);//语句3 return 0;}//运行结果：//语句1：char* p//语句2:报错，有多个匹配//3：int pCopy to clipboardErrorCopied在这种情况下存在对不同指针类型的函数重载，此时如果传入nullptr指针则仍然存在无法区分应实际调用哪个函数，这种情况下必须显示的指明参数类型。4、智能指针的原理、常用的智能指针及实现智能指针是一个类，用来存储指向动态分配对象的指针，负责自动释放动态分配的对象，防止堆内存泄漏。动态分配的资源，交给一个类对象去管理，当类对象声明周期结束时，自动调用析构函数释放资源常用的智能指针(1) shared_ptr实现原理：采用引用计数器的方法，允许多个智能指针指向同一个对象，每当多一个指针指向该对象时，指向该对象的所有智能指针内部的引用计数加1，每当减少一个智能指针指向对象时，引用计数会减1，当计数为0的时候会自动的释放动态分配的资源。 智能指针将一个计数器与类指向的对象相关联，引用计数器跟踪共有多少个类对象共享同一指针 每次创建类的新对象时，初始化指针并将引用计数置为1 当对象作为另一对象的副本而创建时，拷贝构造函数拷贝指针并增加与之相应的引用计数 对一个对象进行赋值时，赋值操作符减少左操作数所指对象的引用计数（如果引用计数为减至0，则删除对象），并增加右操作数所指对象的引用计数 调用析构函数时，构造函数减少引用计数（如果引用计数减至0，则删除基础对象）(2) unique_ptrunique_ptr采用的是独享所有权语义，一个非空的unique_ptr总是拥有它所指向的资源。转移一个unique_ptr将会把所有权全部从源指针转移给目标指针，源指针被置空；所以unique_ptr不支持普通的拷贝和赋值操作，不能用在STL标准容器中；局部变量的返回值除外（因为编译器知道要返回的对象将要被销毁）；如果你拷贝一个unique_ptr，那么拷贝结束后，这两个unique_ptr都会指向相同的资源，造成在结束时对同一内存指针多次释放而导致程序崩溃。(3) weak_ptrweak_ptr：弱引用。 引用计数有一个问题就是互相引用形成环（环形引用），这样两个指针指向的内存都无法释放。需要使用weak_ptr打破环形引用。weak_ptr是一个弱引用，它是为了配合shared_ptr而引入的一种智能指针，它指向一个由shared_ptr管理的对象而不影响所指对象的生命周期，也就是说，它只引用，不计数。如果一块内存被shared_ptr和weak_ptr同时引用，当所有shared_ptr析构了之后，不管还有没有weak_ptr引用该内存，内存也会被释放。所以weak_ptr不保证它指向的内存一定是有效的，在使用之前使用函数lock()检查weak_ptr是否为空指针。(4) auto_ptr主要是为了解决“有异常抛出时发生内存泄漏”的问题 。因为发生异常而无法正常释放内存。auto_ptr有拷贝语义，拷贝后源对象变得无效，这可能引发很严重的问题；而unique_ptr则无拷贝语义，但提供了移动语义，这样的错误不再可能发生，因为很明显必须使用std::move()进行转移。auto_ptr不支持拷贝和赋值操作，不能用在STL标准容器中。STL容器中的元素经常要支持拷贝、赋值操作，在这过程中auto_ptr会传递所有权，所以不能在STL中使用。智能指针shared_ptr代码实现：template&lt;typename T&gt;class SharedPtr{public: SharedPtr(T* ptr = NULL):_ptr(ptr), _pcount(new int(1)) {} SharedPtr(const SharedPtr&amp; s):_ptr(s._ptr), _pcount(s._pcount){ (*_pcount)++; } SharedPtr&lt;T&gt;&amp; operator=(const SharedPtr&amp; s){ if (this != &amp;s) { if (--(*(this-&gt;_pcount)) == 0) { delete this-&gt;_ptr; delete this-&gt;_pcount; } _ptr = s._ptr; _pcount = s._pcount; *(_pcount)++; } return *this; } T&amp; operator*() { return *(this-&gt;_ptr); } T* operator-&gt;() { return this-&gt;_ptr; } ~SharedPtr() { --(*(this-&gt;_pcount)); if (*(this-&gt;_pcount) == 0) { delete _ptr; _ptr = NULL; delete _pcount; _pcount = NULL; } }private: T* _ptr; int* _pcount;//指向引用计数的指针};Copy to clipboardErrorCopied5、说一说你了解的关于lambda函数的全部知识1) 利用lambda表达式可以编写内嵌的匿名函数，用以替换独立函数或者函数对象；2) 每当你定义一个lambda表达式后，编译器会自动生成一个匿名类（这个类当然重载了()运算符），我们称为闭包类型（closure type）。那么在运行时，这个lambda表达式就会返回一个匿名的闭包实例，其实一个右值。所以，我们上面的lambda表达式的结果就是一个个闭包。闭包的一个强大之处是其可以通过传值或者引用的方式捕捉其封装作用域内的变量，前面的方括号就是用来定义捕捉模式以及变量，我们又将其称为lambda捕捉块。3) lambda表达式的语法定义如下：[capture] （parameters） mutable -&gt;return-type {statement};Copy to clipboardErrorCopied4) lambda必须使用尾置返回来指定返回类型，可以忽略参数列表和返回值，但必须永远包含捕获列表和函数体6、说说你了解的auto_ptr作用1) auto_ptr的出现，主要是为了解决“有异常抛出时发生内存泄漏”的问题；抛出异常，将导致指针p所指向的空间得不到释放而导致内存泄漏；2) auto_ptr构造时取得某个对象的控制权，在析构时释放该对象。我们实际上是创建一个auto_ptr类型的局部对象，该局部对象析构时，会将自身所拥有的指针空间释放，所以不会有内存泄漏；3) auto_ptr的构造函数是explicit，阻止了一般指针隐式转换为 auto_ptr的构造，所以不能直接将一般类型的指针赋值给auto_ptr类型的对象，必须用auto_ptr的构造函数创建对象；4) 由于auto_ptr对象析构时会删除它所拥有的指针，所以使用时避免多个auto_ptr对象管理同一个指针；5) Auto_ptr内部实现，析构函数中删除对象用的是delete而不是delete[]，所以auto_ptr不能管理数组；6) auto_ptr支持所拥有的指针类型之间的隐式类型转换。7) 可以通过*和-&gt;运算符对auto_ptr所有用的指针进行提领操作；8) T* get(),获得auto_ptr所拥有的指针；T* release()，释放auto_ptr的所有权，并将所有用的指针返回。7、智能指针的循环引用循环引用是指使用多个智能指针share_ptr时，出现了指针之间相互指向，从而形成环的情况，有点类似于死锁的情况，这种情况下，智能指针往往不能正常调用对象的析构函数，从而造成内存泄漏。举个例子：#include &lt;iostream&gt;using namespace std;template &lt;typename T&gt;class Node{public: Node(const T&amp; value) :_pPre(NULL) , _pNext(NULL) , _value(value) { cout &lt;&lt; \"Node()\" &lt;&lt; endl; } ~Node() { cout &lt;&lt; \"~Node()\" &lt;&lt; endl; cout &lt;&lt; \"this:\" &lt;&lt; this &lt;&lt; endl; } shared_ptr&lt;Node&lt;T&gt;&gt; _pPre; shared_ptr&lt;Node&lt;T&gt;&gt; _pNext; T _value;};void Funtest(){ shared_ptr&lt;Node&lt;int&gt;&gt; sp1(new Node&lt;int&gt;(1)); shared_ptr&lt;Node&lt;int&gt;&gt; sp2(new Node&lt;int&gt;(2)); cout &lt;&lt; \"sp1.use_count:\" &lt;&lt; sp1.use_count() &lt;&lt; endl; cout &lt;&lt; \"sp2.use_count:\" &lt;&lt; sp2.use_count() &lt;&lt; endl; sp1-&gt;_pNext = sp2; //sp2的引用+1 sp2-&gt;_pPre = sp1; //sp1的引用+1 cout &lt;&lt; \"sp1.use_count:\" &lt;&lt; sp1.use_count() &lt;&lt; endl; cout &lt;&lt; \"sp2.use_count:\" &lt;&lt; sp2.use_count() &lt;&lt; endl;}int main(){ Funtest(); system(\"pause\"); return 0;}//输出结果//Node()//Node()//sp1.use_count:1//sp2.use_count:1//sp1.use_count:2//sp2.use_count:2Copy to clipboardErrorCopied从上面shared_ptr的实现中我们知道了只有当引用计数减减之后等于0，析构时才会释放对象，而上述情况造成了一个僵局，那就是析构对象时先析构sp2,可是由于sp2的空间sp1还在使用中，所以sp2.use_count减减之后为1，不释放，sp1也是相同的道理，由于sp1的空间sp2还在使用中，所以sp1.use_count减减之后为1，也不释放。sp1等着sp2先释放，sp2等着sp1先释放,二者互不相让，导致最终都没能释放，内存泄漏。在实际编程过程中，应该尽量避免出现智能指针之前相互指向的情况，如果不可避免，可以使用使用弱指针——weak_ptr，它不增加引用计数，只要出了作用域就会自动析构。8、手写实现智能指针类需要实现哪些函数？1) 智能指针是一个数据类型，一般用模板实现，模拟指针行为的同时还提供自动垃圾回收机制。它会自动记录SmartPointer&lt;T*&gt;对象的引用计数，一旦T类型对象的引用计数为0，就释放该对象。除了指针对象外，我们还需要一个引用计数的指针设定对象的值，并将引用计数计为1，需要一个构造函数。新增对象还需要一个构造函数，析构函数负责引用计数减少和释放内存。通过覆写赋值运算符，才能将一个旧的智能指针赋值给另一个指针，同时旧的引用计数减1，新的引用计数加12) 一个构造函数、拷贝构造函数、复制构造函数、析构函数、移动函数；9、智能指针出现循环引用怎么解决？弱指针用于专门解决shared_ptr循环引用的问题，weak_ptr不会修改引用计数，即其存在与否并不影响对象的引用计数器。循环引用就是：两个对象互相使用一个shared_ptr成员变量指向对方。弱引用并不对对象的内存进行管理，在功能上类似于普通指针，然而一个比较大的区别是，弱引用能检测到所管理的对象是否已经被释放，从而避免访问非法内存。10. 右值引用C++11正是通过引入右值引用来优化性能，具体来说是通过移动语义来避免无谓拷贝的问题，通过move语义来将临时生成的左值中的资源无代价的转移到另外一个对象中去，通过完美转发来解决不能按照参数实际类型来转发的问题（同时，完美转发获得的一个好处是可以实现移动语义）。\\1) 在C++11中所有的值必属于左值、右值两者之一，右值又可以细分为纯右值、将亡值。在C++11中可以取地址的、有名字的就是左值，反之，不能取地址的、没有名字的就是右值（将亡值或纯右值）。举个例子，int a = b+c, a 就是左值，其有变量名为a，通过&amp;a可以获取该变量的地址；表达式b+c、函数int func()的返回值是右值，在其被赋值给某一变量前，我们不能通过变量名找到它，＆(b+c)这样的操作则不会通过编译。\\2) C++11对C++98中的右值进行了扩充。在C++11中右值又分为纯右值（prvalue，Pure Rvalue）和将亡值（xvalue，eXpiring Value）。其中纯右值的概念等同于我们在C++98标准中右值的概念，指的是临时变量和不跟对象关联的字面量值；将亡值则是C++11新增的跟右值引用相关的表达式，这样表达式通常是将要被移动的对象（移为他用），比如返回右值引用T&amp;&amp;的函数返回值、std::move的返回值，或者转换为T&amp;&amp;的类型转换函数的返回值。将亡值可以理解为通过“盗取”其他变量内存空间的方式获取到的值。在确保其他变量不再被使用、或即将被销毁时，通过“盗取”的方式可以避免内存空间的释放和分配，能够延长变量值的生命期。\\3) 左值引用就是对一个左值进行引用的类型。右值引用就是对一个右值进行引用的类型，事实上，由于右值通常不具有名字，我们也只能通过引用的方式找到它的存在。右值引用和左值引用都是属于引用类型。无论是声明一个左值引用还是右值引用，都必须立即进行初始化。而其原因可以理解为是引用类型本身自己并不拥有所绑定对象的内存，只是该对象的一个别名。左值引用是具名变量值的别名，而右值引用则是不具名（匿名）变量的别名。左值引用通常也不能绑定到右值，但常量左值引用是个“万能”的引用类型。它可以接受非常量左值、常量左值、右值对其进行初始化。不过常量左值所引用的右值在它的“余生”中只能是只读的。相对地，非常量左值只能接受非常量左值对其进行初始化。\\4) 右值值引用通常不能绑定到任何的左值，要想绑定一个左值到右值引用，通常需要std::move()将左值强制转换为右值。左值和右值左值：表示的是可以获取地址的表达式，它能出现在赋值语句的左边，对该表达式进行赋值。但是修饰符const的出现使得可以声明如下的标识符，它可以取得地址，但是没办法对其进行赋值const int&amp; a = 10;Copy to clipboardErrorCopied右值：表示无法获取地址的对象，有常量值、函数返回值、lambda表达式等。无法获取地址，但不表示其不可改变，当定义了右值的右值引用时就可以更改右值。左值引用和右值引用左值引用：传统的C++中引用被称为左值引用右值引用：C++11中增加了右值引用，右值引用关联到右值时，右值被存储到特定位置，右值引用指向该特定位置，也就是说，右值虽然无法获取地址，但是右值引用是可以获取地址的，该地址表示临时对象的存储位置这里主要说一下右值引用的特点： 特点1：通过右值引用的声明，右值又“重获新生”，其生命周期与右值引用类型变量的生命周期一样长，只要该变量还活着，该右值临时量将会一直存活下去 特点2：右值引用独立于左值和右值。意思是右值引用类型的变量可能是左值也可能是右值 特点3：T&amp;&amp; t在发生自动类型推断的时候，它是左值还是右值取决于它的初始化。举个例子：#include &lt;bits/stdc++.h&gt;using namespace std;template&lt;typename T&gt;void fun(T&amp;&amp; t){ cout &lt;&lt; t &lt;&lt; endl;}int getInt(){ return 5;}int main() { int a = 10; int&amp; b = a; //b是左值引用 int&amp; c = 10; //错误，c是左值不能使用右值初始化 int&amp;&amp; d = 10; //正确，右值引用用右值初始化 int&amp;&amp; e = a; //错误，e是右值引用不能使用左值初始化 const int&amp; f = a; //正确，左值常引用相当于是万能型，可以用左值或者右值初始化 const int&amp; g = 10;//正确，左值常引用相当于是万能型，可以用左值或者右值初始化 const int&amp;&amp; h = 10; //正确，右值常引用 const int&amp; aa = h;//正确 int&amp; i = getInt(); //错误，i是左值引用不能使用临时变量（右值）初始化 int&amp;&amp; j = getInt(); //正确，函数返回值是右值 fun(10); //此时fun函数的参数t是右值 fun(a); //此时fun函数的参数t是左值 return 0;}12. C++的顶层const和底层const概念区分 顶层const：指的是const修饰的变量本身是一个常量，无法修改，指的是指针，就是 * 号的右边 底层const：指的是const修饰的变量所指向的对象是一个常量，指的是所指变量，就是 * 号的左边举个例子int a = 10;int* const b1 = &amp;a; //顶层const，b1本身是一个常量const int* b2 = &amp;a; //底层const，b2本身可变，所指的对象是常量const int b3 = 20; //顶层const，b3是常量不可变const int* const b4 = &amp;a; //前一个const为底层，后一个为顶层，b4不可变const int&amp; b5 = a; //用于声明引用变量，都是底层constCopy to clipboardErrorCopied区分作用 执行对象拷贝时有限制，常量的底层const不能赋值给非常量的底层const 使用命名的强制类型转换函数const_cast时，只能改变运算对象的底层constconst int a;int const a;const int *a;int *const a;Copy to clipboardErrorCopied int const a和const int a均表示定义常量类型a。 const int a，其中a为指向int型变量的指针，const在 * 左侧，表示a指向不可变常量。(看成const (a)，对引用加const) int *const a，依旧是指针类型，表示a为指向整型数据的常指针。(看成const(a)，对指针const)13. 强制类型转换运算符reinterpret_castreinterpret_cast (expression)type-id 必须是一个指针、引用、算术类型、函数指针或者成员指针。它可以用于类型之间进行强制转换。const_castconst_cast (expression)该运算符用来修改类型的const或volatile属性。除了const 或volatile修饰之外， type_id和expression的类型是一样的。用法如下： 常量指针被转化成非常量的指针，并且仍然指向原来的对象 常量引用被转换成非常量的引用，并且仍然指向原来的对象 const_cast一般用于修改底指针。如const char *p形式static_caststatic_cast &lt; type-id &gt; (expression)该运算符把expression转换为type-id类型，但没有运行时类型检查来保证转换的安全性。它主要有如下几种用法： 用于类层次结构中基类（父类）和派生类（子类）之间指针或引用引用的转换 进行上行转换（把派生类的指针或引用转换成基类表示）是安全的 进行下行转换（把基类指针或引用转换成派生类表示）时，由于没有动态类型检查，所以是不安全的 用于基本数据类型之间的转换，如把int转换成char，把int转换成enum。这种转换的安全性也要开发人员来保证。 把空指针转换成目标类型的空指针 把任何类型的表达式转换成void类型注意：static_cast不能转换掉expression的const、volatile、或者__unaligned属性。dynamic_cast有类型检查，基类向派生类转换比较安全，但是派生类向基类转换则不太安全dynamic_cast (expression)该运算符把expression转换成type-id类型的对象。type-id 必须是类的指针、类的引用或者void*如果 type-id 是类指针类型，那么expression也必须是一个指针，如果 type-id 是一个引用，那么 expression 也必须是一个引用dynamic_cast运算符可以在执行期决定真正的类型，也就是说expression必须是多态类型。如果下行转换是安全的（也就说，如果基类指针或者引用确实指向一个派生类对象）这个运算符会传回适当转型过的指针。如果 如果下行转换不安全，这个运算符会传回空指针（也就是说，基类指针或者引用没有指向一个派生类对象）dynamic_cast主要用于类层次间的上行转换和下行转换，还可以用于类之间的交叉转换在类层次间进行上行转换时，dynamic_cast和static_cast的效果是一样的在进行下行转换时，dynamic_cast具有类型检查的功能，比static_cast更安全举个例子：#include &lt;bits/stdc++.h&gt;using namespace std;class Base{public: Base() :b(1) {} virtual void fun() {}; int b;};class Son : public Base{public: Son() :d(2) {} int d;};int main(){ int n = 97; //reinterpret_cast int *p = &amp;n; //以下两者效果相同 char *c = reinterpret_cast&lt;char*&gt; (p); char *c2 = (char*)(p); cout &lt;&lt; \"reinterpret_cast输出：\"&lt;&lt; *c2 &lt;&lt; endl; //const_cast const int *p2 = &amp;n; int *p3 = const_cast&lt;int*&gt;(p2); *p3 = 100; cout &lt;&lt; \"const_cast输出：\" &lt;&lt; *p3 &lt;&lt; endl; Base* b1 = new Son; Base* b2 = new Base; //static_cast Son* s1 = static_cast&lt;Son*&gt;(b1); //同类型转换 Son* s2 = static_cast&lt;Son*&gt;(b2); //下行转换，不安全 cout &lt;&lt; \"static_cast输出：\"&lt;&lt; endl; cout &lt;&lt; s1-&gt;d &lt;&lt; endl; cout &lt;&lt; s2-&gt;d &lt;&lt; endl; //下行转换，原先父对象没有d成员，输出垃圾值 //dynamic_cast Son* s3 = dynamic_cast&lt;Son*&gt;(b1); //同类型转换 Son* s4 = dynamic_cast&lt;Son*&gt;(b2); //下行转换，安全 cout &lt;&lt; \"dynamic_cast输出：\" &lt;&lt; endl; cout &lt;&lt; s3-&gt;d &lt;&lt; endl; if(s4 == nullptr) cout &lt;&lt; \"s4指针为nullptr\" &lt;&lt; endl; else cout &lt;&lt; s4-&gt;d &lt;&lt; endl; return 0;}//输出结果//reinterpret_cast输出：a//const_cast输出：100//static_cast输出：//2//-33686019//dynamic_cast输出：//2//s4指针为nullptrCopy to clipboardErrorCopied从输出结果可以看出，在进行下行转换时，dynamic_cast安全的，如果下行转换不安全的话其会返回空指针，这样在进行操作的时候可以预先判断。而使用static_cast下行转换存在不安全的情况也可以转换成功，但是直接使用转换后的对象进行操作容易造成错误。13、 vector与list的区别与应用？怎么找某vector或者list的倒数第二个元素\\1) vector数据结构 vector和数组类似，拥有一段连续的内存空间，并且起始地址不变。因此能高效的进行随机存取，时间复杂度为o(1);但因为内存空间是连续的，所以在进行插入和删除操作时，会造成内存块的拷贝，时间复杂度为o(n)。另外，当数组中内存空间不够时，会重新申请一块内存空间并进行内存拷贝。连续存储结构：vector是可以实现动态增长的对象数组，支持对数组高效率的访问和在数组尾端的删除和插入操作，在中间和头部删除和插入相对不易，需要挪动大量的数据。它与数组最大的区别就是vector不需程序员自己去考虑容量问题，库里面本身已经实现了容量的动态增长，而数组需要程序员手动写入扩容函数进形扩容。\\2) list数据结构 list是由双向链表实现的，因此内存空间是不连续的。只能通过指针访问数据，所以list的随机存取非常没有效率，时间复杂度为o(n);但由于链表的特点，能高效地进行插入和删除。非连续存储结构：list是一个双链表结构，支持对链表的双向遍历。每个节点包括三个信息：元素本身，指向前一个元素的节点（prev）和指向下一个元素的节点（next）。因此list可以高效率的对数据元素任意位置进行访问和插入删除等操作。由于涉及对额外指针的维护，所以开销比较大。区别： vector的随机访问效率高，但在插入和删除时（不包括尾部）需要挪动数据，不易操作。 list的访问要遍历整个链表，它的随机访问效率低。但对数据的插入和删除操作等都比较方便，改变指针的指向即可。 从遍历上来说，list是单向的，vector是双向的。 vector中的迭代器在使用后就失效了，而list的迭代器在使用之后还可以继续使用。3)int mySize = vec.size();vec.at(mySize -2);list不提供随机访问，所以不能用下标直接访问到某个位置的元素，要访问list里的元素只能遍历，不过你要是只需要访问list的最后N个元素的话，可以用反向迭代器来遍历：14. Python 和 C++ 的区别区别： 语言自身：Python 为脚本语言，解释执行，不需要经过编译；C++ 是一种需要编译后才能运行的语言，在特定的机器上编译后运行。 运行效率：C++ 运行效率高，安全稳定。原因：Python 代码和 C++ 最终都会变成 CPU指令来跑，但一般情况下，比如反转和合并两个字符串，Python 最终转换出来的 CPU 指令会比 C++ 多很多。首先，Python中涉及的内容比 C++ 多，经过了更多层，Python 中甚至连数字都是 object ；其次，Python 是解释执行的，和物理机CPU 之间多了解释器这层，而 C++ 是编译执行的，直接就是机器码，编译的时候编译器又可以进行一些优化。 开发效率：Python 开发效率高。原因：Python 一两句代码就能实现的功能，C++ 往往需要更多的代码才能实现。 书写格式和语法不同：Python 的语法格式不同于其 C++ 定义声明才能使用，而且极其灵活，完全面向更上层的开发者。" }, { "title": "机试题", "url": "/posts/%E6%9C%BA%E8%AF%95%E9%A2%98/", "categories": "刷题", "tags": "机试", "date": "2022-04-11 12:00:00 +0000", "snippet": "荣耀通用软件实习生——机试（150分左右）/*最少严格递减子序列 AC题目要求给一个由n个正整数组成的数组，将它拆分为多个严格递减的子序列，输出通过拆分可以获得的最少的子序列的个数,并输出这些子序列。说明:1.某个序列的子序列是从最初序列通过去除某些元素但不破坏余下元素的相对位置(在前或在后)而形成的新序列，且该子序列是指从原序列第一个元素遍历到最后一个元素而形成的序列。2.严格递减，指的是数组元素n[i]&lt;n[i-1]的话，即为递减，不能跳跃算递减元素。*/#include&lt;bits/stdc++.h&gt;using namespace std;int main(){ string s; cin&gt;&gt;s; vector&lt;int&gt;nums; if(s.size()==0) cout&lt;&lt;\"1\"&lt;&lt;endl; for(auto &amp;c:s){ if(c!=','){ int temp = c-'0'; nums.push_back(temp); } } vector&lt;int&gt;dp(nums.size(),0); int res = 0; for(int i=0;i&lt;nums.size();i++){ dp[i] = 1; for(int j=0;j&lt;i;j++){ if(nums[j]&lt;=nums[i]){ dp[i] = max(dp[i],dp[j]+1); res = max(res,dp[i]); } } } if(res==0) cout&lt;&lt;\"1\"&lt;&lt;endl; else cout&lt;&lt;res&lt;&lt;endl; string result; vector&lt;string&gt;vec; for(int i=1;i&lt;=res;i++){ string a; for(int j=0;j&lt;nums.size();j++){ if(dp[j]==i){// result.append(to_string(nums[j]));// result.push_back(','); a.append(to_string(nums[j])); } }// int size = result.size();// result.resize(size-1);// cout&lt;&lt;result&lt;&lt;endl; vec.push_back(a); } for(int i=0;i&lt;vec.size();i++){ string temp = vec[i]; for(int j=0;j&lt;temp.size()-1;j++){ cout&lt;&lt;temp[j]&lt;&lt;','; } cout&lt;&lt;temp[temp.size()-1]&lt;&lt;endl; } return 0;}" }, { "title": "机器学习面试八股", "url": "/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1/", "categories": "八股, 算法", "tags": "算法", "date": "2022-03-25 12:00:00 +0000", "snippet": "1. 线性回归模型就是拟合方程假设特征和结果满足线性关系；经过最⼤似然估计推导出来的待优化的⽬标函数与平⽅损失函数是等价的。 岭回归加入L2正则项，等价于对参数w引入协方差为a的零均值高斯先验，不能做variable selection。 LASSO回归加入L1正则项，等价于对参数w引入拉普拉斯先验，可以做variable selection。2. 逻辑回归LR原理LR是一中常见的用于分类的模型，本质上还是一个线性回归，先把特征线性组合，然后使用sigmoid函数（单调可微）将结果约束到0~1之间，结果用于二分类或者回归预测。 用来估计某种事物的可能性。把线性回归的结果通过sigmoid映射到0-1之间。 模型参数估计最大似然估计法估计模型参数，使用梯度下降或者拟牛顿法进行学习。 损失函数最小化交叉熵误差，等价于最大似然估计 解决非线性分类问题加核函数或特征变换，显式地把特征映射到高维空间。2.1 线性回归和逻辑回归的关系线性回归解决连续值预测问题，逻辑回归解决分类或问题。3. SVM原理（二分类）一个样本集线性可分的时候，存在无穷个可以将两类数据正确分开，而SVM是通过最大化间隔来选择分类超平面，最大化这个间隔可以构造成一个约束最优化问题，这是一个凸二次规划问题，然后使用拉格朗日乘子法把约束优化问题转为无约束优化问题，令各个变量偏导为0代入拉格朗日函数得到它的对偶问题，最后用SMO算法来求解这个对偶问题。 解对偶问题的好处一是对偶问题更好解，因为原问题是凸二次规划问题，对偶问题那里只需要求解alpha系数，而alpha系数只有支持向量才非0（KKT条件）。二是自然引入核函数，进而推广到非线性问题。 软间隔当数据近似线性可分时，可以引入松弛变量，使间隔加上松弛变量大于等于1，针对每一个松弛变量都有一个惩罚。 核技巧用一个变换把数据从原空间映射到新空间，数据在新空间是线性可分的。由于新空间有可能非常高维甚至无限维，难以计算且难以表示，所以不显式的定义该映射函数，而是定义一个函数把两个样本直接映射到他们在新空间的内积，因为求解对偶问题时只需要用到内积。常用核函数有高斯核、多项式核、线性核、拉普拉斯核、sigmoid核。 核函数的选择根据专家先验知识，或采用交叉验证，试用不同的核函数（线性核，多项式核和径向基核函数）。问题：什么样的决策边界是最好的？找到一条线(w,b)，使离该线最近的点能够最远。支持向量指的是真正能发挥作用的数据点。软间隔：有时候数据中有一些噪音点，如果考虑它们，决策边界就不好了。为了解决，引入松弛因子，（不一定要把所有点分开），\\(y_i(w*x_i+b)\\ge 1-\\delta_i\\)SVM优点：解决低维不可分问题（二维不可分，三维可分），找到一种变换方法$\\phi(x)$，把结果映射到高维空间，高斯核函数，$K(x,y)=exp(- \\frac {||x-y||^2}{2\\theta^2})$4. LR与SVM的异同 相同之处都可以处理分类问题，都可以添加正则项。 不同之处 损失函数不同，LR使用logistical loss（交叉熵），SVM使用hingeloss，SVM的损失函数自带正则项，而LR则需要自己添加正则项。 解决非线性问题时，SVM采用核函数机制，而LR一般不用，因为复杂核函数需要很大计算量，SVM中只有支持向量参与计算，而LR是全部样本都需要参与计算，若使用核函数计算量太大。 对异常值的敏感度不一样。LR中所有样本都对分类平面有影响，所以异常点的影响会被掩盖。但SVM的分类平面取决于支持向量，如果支持向量受到异常值影响，则结果难以预测。 在高维空间LR的表现比SVM更稳定，因为SVM是要最大化间隔，这个间隔依赖与距离测度，在高维空间时这个距离测度往往不太好。（所以需要做归一化） 5. 分类任务常用的目标函数交叉熵误差，hinge loss等。 相对熵也称KL散度，相对熵可以用来衡量两个概率分布之间的差异。 交叉熵可以用来计算学习模型分布与训练分布之间的差异，交叉熵损失通常适用于Softmax 分类器。最小化相对熵（KL散度）等价于最小化交叉熵，也等价于最大化似然估计。 Hinge loss（折页损失函数）在二分类情况下，公式如下：L(y) =max(0 , 1–t*y)，其中，y是预测值(-1到1之间)，t为目标值(1或 -1)。其含义为，y的值在 -1到1之间即可，并不鼓励|y|&gt;1，即让某个样本能够正确分类就可以了，不鼓励分类器过度自信，当样本与分割线的距离超过1时并不会有任何奖励。目的在于使分类器更专注于整体的分类误差。6. LR逻辑回归为什么对特征进行离散化 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。7. 正则项 结构风险最小化即正则化。在经验风险最小化基础上加上正则项，惩罚复杂的模型。结构风险小同时需要经验风险小和模型简单，如贝叶斯估计中的最大后验概率估计。 经验风险最小化即误差函数最小，样本数量大时，经验风险最小化学习效果好，如极大似然估计。样本少时会出现过拟合。 范数 其非负性使得它天然适合作为机器学习的正则项。 L1范数：向量中各个元素绝对值之和。 L2范数：向量中各个元素平方和的开二次方根。 Lp范数：向量中各个元素绝对值的p次方和的开p次方根。 L1正则项目标函数中增加所有权重w参数的绝对值之和, 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0, 奔向零的速度不如L1给力了)，倾向于使参数稀疏化； L2正则项倾向于使参数稠密地接近于0，目标函数中增加所有权重w参数的平方之和, 逼迫所有w尽可能趋向零但不为零。 8. 决策树概念：决策树一种基本的分类和回归方法，呈树形结构，表示基于特征对实例进行分类的过程。决策树的学习过程包括三个步骤：特征选择、决策树的生成以及决策树的修剪（预剪枝和后剪枝）。特征选择的准则是信息增益或者信息增益比（基尼系数）。 ID3生成算法（使用信息增益进行特征选择）从根节点开始，对结点结算所有可能的特征的信息增益，选择信息增益最大的特征作为结点特征，然后对子节点递归的调用这个方法，构建决策树。信息增益偏向于选择取值较多的特征，容易过拟合。 C4.5生成算法（使用信息增益比进行特征选择）与ID3算法相似，用信息增益率（偏好可取值数目较少）来选择特征。小技巧：先找出信息增益高于平均水平的属性，再从中选择增益率最高的。 CART生成算法（使用平方误差最小化来选择特征）全称分类树与回归树，可用于分类也可用于回归。分类树用基尼系数（越小越好）进行特征选择，用最小二乘回归树生成算法生成回归树（结点下所有点的均值作为该结点的预测值）。 基尼指数反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。因此基尼指数越小，则数据集的纯度越高。 决策树的剪枝（防止过拟合）通过极小化决策树整体的损失函数来进行剪枝。从树的叶节点开始向上回缩，假设回缩前树为Ta，回缩到父节点后树为Tb，计算两棵树的损失函数，如果回缩后损失函数变小，则剪枝，把父节点设为叶节点，这样迭代。8.1 如何避免过拟合 设置每个叶子节点的最小样本数； 设置树的最大深度； 设置叶子节点的总数量； 剪枝：对每个结点或子树进行裁剪，评估剪枝前后模型的预测能力。9. 随机森林（样本随机性和特征随机性） 原理由很多棵决策树组成，每棵决策树之间没有关联。每棵树的生成方法是，随机而且有放回地（如果没有放回那就每棵树都训练集完全不相交，太片面）从训练集选取N个（训练集大小为N）训练样本作为训练集，所以每棵树训练集都不一样，但包含重复样本，生成决策树选择特征时，从特征子集里面选择最优的特征用于分裂。得到森林后，输入样本让每棵决策树进行判断，输出结果为被最多决策树选择的分类。 与Bagging区别与Bagging不一样的是会抽取与样本数目同样大小的样本来训练（Bagging一般少于n），且只用部分特征得到分类器（Bagging用全部特征）。 森林中任意两棵树的相关性相关性越大，错误率越大。 森林中每棵树的分类能力每棵树的分类能力越强，整个森林的错误率越低。 随机森林唯一的一个参数特征个数m，减小m会降低树的相关性和分类能力，增加m两者也会随之增大。10. 集成学习结合多个学习器组合成一个性能更好的学习器。 Bagging对原数据有放回抽取，构建出多个样本数据集，训练出多个分类器。 并行训练K个模型，采取投票方式得到分类结果。 代表：随机森林，数据采样随机、特征选择随机、很多个决策树并行放在一起。 理论上数越多效果越好，但超过一定数量就不变了。 Boosting（AdaBoost，Xgboost) AdaBoost：根据前一次的分类结果调整数据权重。 使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权)。通过改变样本分布，对容易错分的数据加强学习，增加错分样本数据的权重。 每轮训练改变样本权重。 两者区别： 样本选择不同Bagging是有放回地抽取数据集，每轮训练集是独立的。而Boosting每一轮的训练集不变，但是训练集中每个样本的权重会发生变化，根据上一轮分类结果调整。 样本权重不同Bagging每个样本权重相等，Boosting根据错误率不断调整样本的权重，被错分的样本权重会增加。 生成方式不同Bagging各个分类器可以并行生成，而Boosting只能顺序生成。 优化目标有差异Bagging减小的是方差，Boosting减小的是偏差。Bagging减小方差是因为对多个用不同训练集得到的分类器取了平均（取平均比单一的方差要小）。Boosting减小偏差是因为每次迭代都在之前误差的基础上进一步降低错误率，所以就是减小偏差，对于训练集的拟合程度好。 Boosting不会显著降低方差是因为其训练过程中各基学习器是强相关的，缺少独立性。 Stacking模型堆叠各种各样的分类器（KNN，SVM，RF）11. Boosting方法 AdaBoost初始化样本权重，每个样本最开始的权重一样。训练弱分类器，若样本在这次训练中被正确分类，则降低它的权重，反之增加。这样多次训练得到多个弱分类器组成一个强分类器，误差率小的弱分类器拥有更大的权重。 提升树 提升树是迭代多棵回归树来共同决策， 当采用平方误差函数，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，最后累加所有树的结果作为最终结果。 GBDT梯度提升树。利用损失函数的负梯度在当前模型的值作为提升树算法的残差的近似值。因为当损失函数不是平方误差时，残差不是简单的真实值减去预测值，把目标函数对当前模型求梯度，就可以得到模型改进的方向，负梯度就可以近似成残差。 XGBoost基于C++通过多线程实现了回归树的并行构建，并在原有GBDT基础上加以改进，在目标函数增加了正则化项，正则项里包括了树的叶子结点个数、每个叶子结点上输出分数的L2模的平方和，且对目标函数做了二阶泰勒展开，从而极大提升了模型的训练速度和预测精度。 12. XGBoost为什么要泰勒展开二阶导数有利于梯度下降更快更准；统一损失函数求导的形式以支持自定义损失函数。12.1 为什么不三阶展开要求三阶可导，对精度提升不大。13. GBDT和XGBoost的区别 基分类器的选择：GBDT以CART做基分类器，XGBoost还支持线性分类器。 GBDT在优化时只用到了一阶导数信息，XGBoost对目标函数进行了二阶泰勒展开，同时用到一阶和二阶导数。 XGBoost在目标函数里加入了正则项，控制模型的复杂度。 XGBoost可以为缺失值或者指定的值指定分支的默认方向，对特征值有缺失的样本可以自动学习出他的分裂方向。 XGBoost支持并行，可以在特征粒度上并行，在训练前对特征进行分块排序，在寻找最佳分裂点的时候可以并行化计算，大大提升速度。14. RF和GBDT的区别 相同点都是由多棵树组成，最终的结果都是由多棵树一起决定。 不同点 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成； 组成随机森林的树可以并行生成，而GBDT是串行生成； 随机森林的结果是多棵树投票表决的，而GBDT则是多棵树累加之和； 随机森林对异常值不敏感，而GBDT对异常值比较敏感； 随机森林是减少模型的方差，而GBDT是减少模型的偏差； 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化。 14.1 GBDT为什么需要进行特征归一化因为GBDT的树是在上一颗树的基础上通过梯度下降求解最优解，归一化能收敛的更快。15. XGBoost和LightGBM的区别 XGBoost采用的是level-wise的分裂策略，而lightGBM采用了leaf-wise的策略，区别是XGBoost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是XGBoost也进行了分裂，带来了务必要的开销。 leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。 LightGBM使用了基于histogram的决策树算法，这一点不同与XGBoost中的exact算法，histogram算法在内存和计算代价上都有不小优势。16. EM算法 定义：在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。 方法：最大期望算法经过两个步骤交替进行计算：第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值； 第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。17. 归一化处理 标准化：把特征值变为均值0，方差1 归一化：把每个特征向量的值放缩到相同数值范围，如[0,1] 归一化的作用 加快梯度下降求最优解的速度。因为若两个特征的值区间相差大，数据会呈扁长形，梯度下降很可能会走之字形路线（垂直等高线走）。 可能提高精度。有些分类器依赖于计算样本之间的距离，而区间大特征值对距离影响会更大，但若区间小的特征值才是真正重要的特征，则容易被忽略。 哪些机器学习算法需要归一化利用梯度下降法求解的模型一般需要归一化，如线性回归、LR、SVM、KNN、神经网络等。树形模型一般不需要归一化，因为他们不关心变量的值（数值缩放不影响分裂位置），而是关心变量的分布，如决策树、随机森林。18. 如何进行特征选择 特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解. 常见的特征选择方式 去除方差较小的特征； 正则化：L1正则化能够生成稀疏的模型；L2正则化的表现更加稳定，由于有用的特征往往对应系数非零; 稳定性选择：在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果。选择算法可以是回归、SVM或其他类似的方法。 19. 样本不均匀 加权：不同类别分错的代价设为不同。 采样：上采样和下采样，上采样是把小众类复制多份，下采样是从大众类中选取部分样本。 上采样上采样会把小众样本复制多份，一个点会在高维空间中反复出现，这会导致一个问题，那就是运气好就能分对很多点，否则分错很多点。为了解决这一问题，可以在每次生成新数据点时加入轻微的随机扰动。 下采样因为下采样会丢失信息，为了减少信息的损失，第一种方法可以利用模型融合的方法（bagging）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。第二种方法利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。 20. 模型评价指标 TP：正类预测为正类 FN：正类预测为负类 FP：负类预测为正类 TN：负类预测为负类 精确率：预测为正的样本当中有多少预测正确了，P = TP/(TP+FP) 召回率：真正为正的样本当中有多少被预测，R = TP/(TP+FN) F1值: 综合考虑了精确率和召回率，2/F1 = 1/R + 1/P ROC：横轴是假阳性，FPR=FP/(FP+TN)，真实的正例中，被预测正确的比例；纵轴是真阳性，TPR=TP/(TP+FN)，真实的反例中，被预测正确的比例；绘制方法：假设已有一系列样本被分为正类的概率，按概率值从大到小排序，依次将该概率值作为阈值来预测分类，每个阈值都对应一对FPR、TPR，这样一来就能绘制出曲线。 \\[FDP=\\frac {FP}{FP+TN}\\]\\[TPR=\\frac {TP}{TP+FN}\\] AUC：ROC曲线的面积，AUC大的分类器效果更好。 AUC&lt;=121. 常用计算距离的方法 欧氏距离：将样本的不同属性之间的差别等同对待，但实际上可能有些属性更重要。适用于各分量的度量标准统一的情况。 曼哈顿距离：两个点在标准坐标系上的绝对轴距之和。计算速度快 切比雪夫距离：国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子(x1,y1)走到格子(x2,y2)最少需要的步数叫切比雪夫距离。22. 聚类常用方法有：层次的方法（hierarchical method）、划分方法（partitioning method）、基于密度的方法（density-based method）、基于网格的方法（grid-based method）、基于模型的方法（model-based method）等。 经典K-means（划分）算法流程 随机地选择k个对象，每个对象初始地代表了一个簇的中心； 对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇； 重新计算每个簇的平均值，更新为新的簇中心； 不断重复ii、iii，直到准则函数收敛。 K个初始类簇点的选取还有两种方法 选择彼此距离尽可能远的K个点； 先对数据用层次聚类算法进行聚类，得到K个簇之后，从每个类簇中选择一个点，该点可以是该类簇的中心点，或者是距离类簇中心点最近的那个点。 K值的选择轮廓系数，求出所有样本的轮廓系数后再求平均值就得到了平均轮廓系数。平均轮廓系数的取值范围为[-1,1]，且簇内样本的距离越近，簇间样本距离越远，平均轮廓系数越大，聚类效果越好。那么，很自然地，平均轮廓系数最大的k便是最佳聚类数。 经典DBSCAN算法流程 DBSCAN通过检查数据集中每点的Eps邻域来搜索簇，如果点p的Eps邻域包含的点多于MinPts个，则创建一个以p为核心对象的簇； 然后，DBSCAN迭代地聚集从这些核心对象直接密度可达的对象，这个过程可能涉及一些密度可达簇的合并； 当没有新的点添加到任何簇时，该过程结束。 K-means与DBSCAN对比和传统的K-Means算法相比，DBSCAN最大的不同就是不需要输入类别数k，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点 DBSCAN的主要优点有 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集； 可以在聚类的同时发现异常点，对数据集中的异常点不敏感； 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。 DBSCAN的主要缺点有 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合； 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进； 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。 23.监督学习和无监督学习监督学习：决策树，RF，KNN，SVM，贝叶斯无监督学习：K-means，PCA24.为什么分类问题不用MSE损失函数？ MSE：预测值与目标值的欧氏距离； 交叉熵：真实概率分布与预测概率分布的差异。 MSE对二分类问题是非凸的，不能保证损失函数最小化25. 过拟合泛化性不好。variance会变大。减轻过拟合的方法：增加数据量，L1、L2正则化（使损失函数的权重减小），dropout（删除部分的隐藏单元，一般用于全连接层，池化层，LSTM层之后，）26.bias和variancebias：反映样本上的输出与真实值的误差（精确度）；偏差描述的是算法的预测的平均值和真实值的关系（可以想象成算法的拟合能力如何），而方差描述的是同一个算法在不同数据集上的预测值和所有数据集上的平均预测值之间的关系（可以想象成算法的稳定性如何）。27. 梯度下降梯度下降就是用来求某个函数最小值时自变量对应取值其中这句话中的某个函数是指：损失函数（cost/loss function），直接点就是误差函数。一个算法不同参数会产生不同拟合曲线，也意味着有不同的误差。损失函数就是一个自变量为算法的参数，函数值为误差值的函数。所以梯度下降就是找让误差值最小时候算法取的参数。28. 决策树决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。优点：可以做分析、可视化、速度快、集成。衡量标准：信息熵（表示随机变量不确定性的度量。使熵值最小）决策树的剪枝：（原因）过拟合风险很大；设置参数不要让树模型过于复杂。29. K-MEANS算法（聚类）无监督（无标签），无法证明算法的有效性。需要指定K值（类别），质心：均值。半监督：样本中有的有标签，有的无标签。然后用聚类，将无标签的打上标签。30. 贝叶斯先验概率。朴素贝叶斯：假设特征之间相互独立，为了计算方便。31. 线性判别LDA用于数据预处理中的降维，分类任务。将特征空间投影到一个维度更小的K维子空间中，同时保持区分类别的信息。相同类别的点离得更近，不同的更远。有监督学习。32. 主成分分析PCA无监督学习。用于降维，每个维度独立无关。提取最有价值的信息(基于方差)。方差越大，效果越好，数据分得越开。33. LSTM和RNN的区别 RNN没有细胞状态，LSTM通过细胞状态记忆信息； RNN的激活函数只有tanh，LSTM通过输入门、遗忘门、输出门引入sigmoid函数，并结合tanh函数，添加求和操作； RNN有梯度爆炸问题； RNN只能处理短期依赖问题，LSTM既可以长期，也可以短期。33.1 RNN梯度爆炸和消失的原因梯度消失：一句话，RNN梯度消失是因为激活函数tanh函数的倒数在0到1之间，反向传播时更新前面时刻的参数时，当参数W初始化为小于1的数，则多个(tanh函数’ * W)相乘，将导致求得的偏导极小（小于1的数连乘），从而导致梯度消失。梯度爆炸：当参数初始化为足够大，使得tanh函数的导数乘以W大于1，则将导致偏导极大（大于1的数连乘），从而导致梯度爆炸。LSTM引入了细胞状态，在梯度前加了常量。34. 激活函数sigmoid：容易出现梯度消失，计算量大。\\(f(x)=\\frac 1{1+e^{-x}}\\)tanh：收敛速度比sigmoid快，产生梯度消失。\\(f(x)=\\frac {1-e^{-2x}}{1+e^{-2x}}\\)Relu：计算简单，避免梯度爆炸和梯度消失。\\(f(x)=\\begin{cases}x,&amp; {x &gt; 0}\\\\0,&amp; {x\\le0}\\end{cases}\\)全连接层：sigmoid，tanh池化层：Relu35. 池化层的作用（特征减少，参数减少）降低卷积层对位置的敏感性，降低对空间降采样表示的敏感性。maxpooling：减少卷积层参数误差造成估计均值的偏移，更多保留纹理信息；meanpooling：减少邻域大小受限造成的估计方差增大，更多保留图像的背景信息。" }, { "title": "机器学习算法面经", "url": "/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E9%9D%A2%E7%BB%8F/", "categories": "面经", "tags": "机器学习", "date": "2022-03-25 12:00:00 +0000", "snippet": "阿里健康1面（电话面）归一化的方法：答了标准化和[0,1]，问还有哪些方法？线性回归、SVM等需要归一化吗？SVM归一化是在哪里进行归一化的？有没有用过前向神经网络？欠拟合和过拟合的概念？解决过拟合的方式？（回答了增加数据量、L1、L2正则化、dropout）问神经网络中解决过拟合还有哪些技巧？CNN中dropout设置在哪一层？值一般是多少？dropout可以在卷积层之后吗？dropout减少神经元个数，在矩阵运算上是怎么做的？训练和测试的时候，dropout设置的参数一样吗？行为一样吗？K折交叉验证的过程介绍自己的项目自己的项目是如何改进的？介绍一下ResNet？了解过搜索推荐吗？假设用户在淘宝买商品，作为服务方，需要用哪些特征来进行推荐商品呢？学习过程中遇到的比较困难的事情反问环节华为通用软件实习生（技术面1+主管面1）自我介绍健患侧解释手撕代码：回文子串（简单题）" }, { "title": "Essential C++", "url": "/posts/Essential-C++/", "categories": "C++", "tags": "侯捷", "date": "2022-01-08 12:00:00 +0000", "snippet": "第1章 C++编程基础命名空间namespace初始化语法：构造函数语法int num_trier(0)模板类(class template)：必须在类名之后的尖括号内指定其元素类型*pi：指针的值；pi：指针的地址；&amp;ival：数值ival的地址读写文件：#include&lt;fstream&gt;//不希望丢弃原文件内容，用追加模式打开ofsream outfile(\"seq_data.txt\",ios__base::app);//读取文件ifstream infile(\"seq_data.txt\");//同时读写文件fstream iofile(\"seq_data.txt\",ios__base::in||ios__base::app);if(!iofile) //由于某种原因，文件无法打开else{ //开始读取之前，将文件重新定位至起始处 iofile.seekg(0);}#include：io代表输入输出，manip是manipulator（操纵器）的缩写。主要是对cin,cout之类的一些操纵运算子，比如setfill，setw，setbase，setprecision等等。它是I/O流控制头文件,就像C里面的格式化输出一样.以下是一些常见的控制函数的：cout &lt;&lt; setw( 3 ) &lt;&lt; 1 &lt;&lt; setw( 3 ) &lt;&lt; 10 &lt;&lt; setw( 3 ) &lt;&lt; 100 &lt;&lt; endl; #输出结果为_ _1_10100 //（默认是右对齐）当输出长度大于3时(&lt;&lt;1000)，setw(3)不起作用。▲setw(n)用法： 通俗地讲就是预设宽度第2章 面向对象的编程风格传值和传址局部函数调用时，传值的话不会修改原参数的值，只会重新复制一个对象，称为”程序堆栈“。要想修改原参数，就需要用&amp;取址符号，船址。将参数声明为reference的理由&amp;希望得以直接对所传入的对象进行修改；降低复制大型对象的额外负担。reference&amp;、指针*pointer参数和reference参数的差异：pointer可能(也可能不)指向某个实际对象。当我们提领pointer时，一定要先确定其值为非0。而reference必定代表某个对象，所以不需要作此检查。int ival = 1024;int *pi = &amp;ival; //表示pi为ival的地址，*pi为ival的值1024int &amp;rval = ival; //表示rval和ival为一个对象，两者地址相同int *pi;pi = new int(1024); //pi = 1024的地址，*pi为1024//将vector的参数声明为reference#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;fstream&gt;using namespace std;void display(vector&lt;int&gt;&amp;vec){\tfor(int i=0;i&lt;vec.size();i++)\t{\t\tcout&lt;&lt;vec[i]&lt;&lt;' ';\t}\tcout&lt;&lt;endl;}int main(){\tint arr[8] = {8,34,3,13,1,21,5,2};\tvector&lt;int&gt;vec(arr,arr+8);\tcout&lt;&lt;\"vector before sort: \";\tdisplay(vec);\treturn 0;}//将vector以指针pointer传递#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;fstream&gt;using namespace std;void display(vector&lt;int&gt;*vec){ //在提领pointer时，要先确定其值非0\tif(!vec)\t{\t\tcout&lt;&lt;\"display():the vector pointer is 0\\n\";\t\treturn;\t}\tfor(int i=0;i&lt;vec-&gt;size();i++)\t{\t\tcout&lt;&lt;(*vec)[i]&lt;&lt;' ';\t}\tcout&lt;&lt;endl;}int main(){\tint arr[8] = {8,34,3,13,1,21,5,2};\tvector&lt;int&gt;vec(arr,arr+8);\tcout&lt;&lt;\"vector before sort: \";\tdisplay(vec);\treturn 0;}作用域及范围对象如果在函数外部声明，则该对象的内存在main()开始执行之前就已经分配好，可以一直存在至程序结束。内置类型的对象如果定义在file scope内，将被初始化为0；如果被定义于local scope之内，除非指定其初值，否则不会被初始化。动态内存管理int *pi;pi = new int(1024);int *pia = new int[24]//从heap中分配数组delete pi; //释放pi所指的对象delete []pia;提供默认参数值默认参数值规则：(1) 解析操作由最右边开始进行，如果为某个参数提供了默认值，那么这一参数右侧的所有参数都必须也具有默认参数值。//错误：没有为vec提供默认值void display(ostream &amp;os=cout,const vector&lt;int&gt;&amp;vec);(2) 默认值只能指定一次声明inline函数将经常被调用、体积小的函数声明为inline，改善性能。bool is_size_ok(int size){\tconst int max_size = 1024;\tif(size&lt;=0||size&gt;max_size)\t{\t\tcerr&lt;&lt;\"invalid size\\n\";\t\treturn false;\t}\treturn true;}const vector&lt;int&gt;*fibon_seq(int size){\tconst int max_size = 1024;\tstatic vector&lt;int&gt;elems;\t\tif(!is_size_ok(size))\t\treturn 0;\t\tfor(int i=elems.size();i&lt;size;i++)\t{\t\tif(i==0||i==1)\t\t{\t\t\telems.push_back(1);\t\t}\t\telse\t\t{\t\t\telems.push_back(elems[i-1]+elems[i-2]);\t\t}\t}\treturn &amp;elems;}//返回Fibonacci数列中位置为pos的元素inline bool fibon_elem(int pos,int &amp;elem){\tconst vector&lt;int&gt;*pseq = fibon_seq(pos);\tif(!pseq)\t{\t\telem=0;\t\treturn false;\t}\telem = (*pseq)[pos-1];\treturn true;}函数重载函数自动根据参数类型来调用函数。#include&lt;iostream&gt;using namespace std;void add(int a,int b){ cout&lt;&lt;\"print a integer :\"&lt;&lt;a+b&lt;&lt;endl;}void add(float a,float b){ cout&lt;&lt;\"print a float :\"&lt;&lt;a+b&lt;&lt;endl;}int main(){ add(1,2); add(1.1,2.1); return 0;}定义并使用模板函数在函数重载时，定义函数时，每个函数非常相像，只有变量类型不同，使用模板函数解决。只需要定义一份函数内容。#include&lt;iostream&gt;using namespace std;template&lt;typename elemType&gt;void add(elemType a,elemType b){ cout&lt;&lt;a+b&lt;&lt;endl;}int main(){\t\tint a=1,b=2;\t\tfloat c=1.1,d=2.1; add(a,b); add(c,d); return 0;}函数指针const vector&lt;int&gt; *fibon_seq(int size);const vector&lt;int&gt; *lucas_seq(int size);const vector&lt;int&gt; *pell_seq(int size);const vector&lt;int&gt; *triang_seq(int size);const vector&lt;int&gt; *square_seq(int size);const vector&lt;int&gt; *pent_seq(int size);为了获得该数组的第pos个数，我们采用函数指针，这样不需要提供6个不同的函数，只需定义一个。//返回Fibonacci数列中位置为pos的元素inline bool fibon_elem(int pos,int &amp;elem){\tconst vector&lt;int&gt;*pseq = fibon_seq(pos); //(A)\tif(!pseq)\t{\t\telem=0;\t\treturn false;\t}\telem = (*pseq)[pos-1];\treturn true;}只有A处不同，用函数指针代替。函数指针：必须指明其所指函数的返回类型及参数列表。本例中函数的返回类型是const vector&lt;int&gt; *，参数列表是int，函数指针的定义必须将*放在某个位置，表示这份定义所表现的是一个指针。const vector&lt;int&gt;* (*seq_ptr)(int)// 函数指针inline bool fibon_elem(int pos,int &amp;elem,const vector&lt;int&gt;* (*seq_ptr)(int)){ //检验指针是否指向某个函数 if(!seq_ptr) { cout&lt;&lt;\"Internal Error:seq_ptr is set to null!\"; }\tconst vector&lt;int&gt;*pseq = seq_ptr(pos);\tif(!pseq)\t{\t\telem=0;\t\treturn false;\t}\telem = (*pseq)[pos-1];\treturn true;}// 函数指针数组const vector&lt;int&gt;* (*seq_array[])(int) = { fibon_seq,ns_lucas,ns_pent, ns_triang,ns_square,ns_pent};//循环调用int seq_index = 0;while(next_seq==true){ seq_ptr = seq_array[++seq_index];}设定头文件函数定义只能有一份，inline函数例外，在每个调用点上，编译器都得取得其定义。因此，必须将inline函数定义在头文件中。//将seq_array定义于file scope，需要在头文件声明const int seq_cnt = 6; //const object喝inline一样，一出文件之外便不可见，因此不需要加externconst vector&lt;int&gt;* (*seq_array[seq_cnt])(int);//错误写法，会被解读为定义extern const vector&lt;int&gt;* (*seq_array[seq_cnt])(int); //正确，声明包含头文件时，如果头文件喝包含此文件的程序代码文件位于同一个磁盘目录下，便使用双引号，否则使用尖括号。#include\"NumSeq.h\"#include&lt;NumSeq.h&gt;第3章 泛型编程风格STL由两种组件组成：（1）容器：vector、list、set、map；（2）泛型算法：find()、sort()、replace()、merge()vector和list是顺序性容器，顺序性容器会依次维护第一个、第二个……直到最后一个元素。map和set是关联性容器，关联性容器让我们快速查找容器中的元素值。map是key/value组合，key用于查找，value用来表示我们要储存或取出的数据。set仅含有key，可对其进行查询操作，来判断某值是否存在于其中。3.1 指针的算术运算array[2]和*(array+2)array以第一个元素的指针传入函数，可以通过下标访问array中的每一个元素。如同此array是个对象(而非指针形式)一般。下标操作就是将array的起始地址加上索引值来产生某个元素的地址。然后对该地址进行提领（dereference）操作。array[2];*(array+2);//结果一样//无论数组元素的类型是什么，都可以访问数组中的每一个元素template&lt;typename elemType&gt;elemType* find(const elemType *first,const elemType *last,const elemType &amp;value){ if(!first||!last) return 0; //当first!=last，就把value拿来和first所指元素比较 for(;first!=last;++first) { if(*first==value) return frist; } return 0;}3.2 了解Iterator(泛型指针)// iter被定义为一个iterator，指向一个vector，后者的元素类型为string。其初值指向svec的第一个元素。// 双冒号::表示此iterator是位于string vector定义内的嵌套(nested)类型vector&lt;string&gt;::iterator iter = svec.begin()//用iterator取代下标运算符template&lt;typename elemType&gt;void display(const vector&lt;elemType&gt;&amp;vec,ostream &amp;os){ vector&lt;elemType&gt;::const_iterator iter = vec.begin(); vector&lt;elemType&gt;::const_iterator end_iter = vec.end(); for(;iter!=end_iter;++iter) { os&lt;&lt;*iter&lt;&lt;' '; } os&lt;&lt;endl;}// 重新实现find()，使得其支持一对指针或者一对指向某种容器的iteratortemplate&lt;typename IteratorType,typename elemType&gt;IteratorType find(IteratorType first,IteratorType last,const elemType &amp;value){ for(;first!last;++first) { if(value==*first) return first; } return last;}//应用const int asize = 8;int ia[asize] = {1,1,2,3,5,8,13,21};vector&lt;int&gt;ivec(ia,ia+asize);list&lt;int&gt;ilist(ia,ia+asize);int *pia = find(ia,ia+asize,1024);if(pia!=ia+asize) //找到了......vector&lt;int&gt;::iterator it;it = find(ivec.begin(),ivec.end(),1024);if(it!=ivec.end()) //找到了...... list&lt;int&gt;::iterator iter;iter = find(ilist.begin(),ilist.end(),1024);if(iter!=ilist.end()) //找到了......3.3 使用顺序性容器vector、list、dequedeque：对于最前端元素的插入和删除，效率较高。容器的插入函数insert()iterator insert(iterator position,elemType value);//在position之前插入valuevoid insert(iterator position,int count,elemType value)；//在position之前插入count个valuevoid insert(iterator1 position,iterator2 first,iterator2 last);//在position之前插入[firsy,last)所标识的各个元素容器的删除操作(erase)iterator erase(iterator position);//删除position指定的元素iterator erase(iterator first,iterator last);//删除[first,last)范围内的元素3.5 使用泛型算法#include&lt;algorithm&gt;四种泛型搜索算法：（1）find()：用于搜索无序集合中是否存在某值；（2）binary_search()：用于有序集合的搜素。如果搜索到目标，就返回true。binary_search()比find()更有效率；（3）count()：返回数值相符的元素数目；（4）search()：比对某个容器内是否存在某个子序列，如果存在，就返回一个iterator指向子序列的起始处，否则指向容器末尾。3.6 如何设计一个泛型算法Function Objectsort(vec.begin(),vec.end(),greater&lt;int&gt;()); //按降序排序Function Object Adapterbinder adapter(绑定适配器)会将function object的参数绑定至某个特定值。bind1st将指定值绑定至第一操作数，bind2nd将指定值绑定至第二操作数。vector&lt;int&gt;filter_ver1(const vector&lt;int&gt;&amp;vec,int filter_val,less&lt;int&gt;&amp;lt){\tvector&lt;int&gt;nvec; vector&lt;int&gt;::const_iterator iter = vec.begin(); while((iter=find_if(iter,vec.end(),bind2nd(lt,val)))!=vec.end()) { nvec.push_back(*iter); iter++; }\treturn nvec;}// 消除filter()与vector元素类型，以及filter()与vector容器类型的依赖关系，使filter()更加泛型化#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;algorithm&gt;using namespace std;template&lt;typename InputIterator,typename OutputIterator,typename ElemType,typename Comp&gt;OutputIterator filter(InputIterator first,InputIterator last,OutputIterator at,const ElemType &amp;val,Comp pred){\twhile((first=find_if(first,last,bind2nd(pred,val)))!=last)\t{\t\tcout&lt;&lt;\"found value: \"&lt;&lt;*first&lt;&lt;endl;\t\t*at++ = *first++;\t}\treturn at;\t}int main(){\tconst int elem_size = 8;\tint ia[elem_size] = {12,8,43,0,6,21,3,7};\tvector&lt;int&gt;ivec(ia,ia+elem_size);\t\t//储存过滤结果\tint ia2[elem_size];\tvector&lt;int&gt;ivec2(elem_size);\t\tcout&lt;&lt;\"filtering integer array for values less than 8\\n\";\tfilter(ia,ia+elem_size,ia2,elem_size,less&lt;int&gt;());\t\tcout&lt;&lt;\"filtering integer vector for values greater than 8\\n\";\tfilter(ivec.begin(),ivec.end(),ivec2.begin(),elem_size,greater&lt;int&gt;());\t\treturn 0;}not1可对unary function object的真伪值取反，not2可对binary function object的真伪值取反。while((iter=find_if(iter,vec.end(),not1(bind2nd(lt,val)))!=vec.end())3.7 使用map#include&lt;map&gt;#include&lt;string&gt;map&lt;string,int&gt;words;//循环打印所有单词出现的次数map&lt;string,int&gt;::iterator it = words.begin();for(;it!=words.end();++it){ cout&lt;&lt;\"keys: \"&lt;&lt;it-&gt;first &lt;&lt;\"value: \"&lt;&lt;it-&gt;second&lt;&lt;endl;}//查询map内是否有某个key//利用find()函数,如果存在key，find()函数返回key/value形成的pair；不存在返回end()int count = 0;map&lt;string,int&gt;::iterator it;it = words.find(\"vermeer\");if(it!=words.end()) count = it-&gt;seond;//利用count()函数，返回某特定项在map内的个数int count = 0;string search_word(\"vermeer\");if(words.count(search_word)) count = word[search_word];3.8 使用SetSet由一群key组合而成，对于任何key值只能存储一份。#include&lt;set&gt;#include&lt;string&gt;//定义一个用来记录“排除字眼”的setset&lt;string&gt;word_exclusion;while(cin&gt;&gt;tword){ if(word_exclusion.count(tward)) continue;//不执行后续语句，进入下一轮while循环 words[tward]++;}//为set插入单一元素iset.insert(ival);//为set插入某个范围的元素iset.insert(vec.begin(),vec.end());3.9 如何使用Iterator Inserter将源端容器中每一个符合条件的元素一一幅值至目的端容器时，目的端容器必须足够大。以前的方法是将目的端容器大小设置为与源端容器大小相同，这样子会占用较大内存。C++标准库提供3个insertion adapter：（1）back_inserter()：以容器的push_back()取代赋值运算符。传入的参数就是容器本身。vector&lt;int&gt;result_vec;unique_copy(ivec.begin(),ivec.end(),back_inserter(result_vec));（2）inserter()：以容器的insert()函数取代assignment运算符。接收参数为容器和iterator，iterator指向插入操作的起点。vector&lt;string&gt;svec_res;unique_copy(svec.begin(),svec.end(),inserter(svec_res,svec_res.end()));（3）front_inserter()：以容器的push_front()函数取代assignment运算符。只适用于list和dequelist&lt;int&gt;ilist_clone;copy(ilist.begin(),ilist.end(),front_inserter(ilist_clone));需要包含头文件：#include&lt;iterator&gt;将3.6节的main函数改写为：int main(){\tconst int elem_size = 8;\tint ia[elem_size] = {12,8,43,0,6,21,3,7};\tvector&lt;int&gt;ivec(ia,ia+elem_size);\t\t//储存过滤结果\tint ia2[elem_size];\tvector&lt;int&gt;ivec2(elem_size);\t\tcout&lt;&lt;\"filtering integer array for values less than 8\\n\";\tfilter(ia,ia+elem_size,ia2,elem_size,less&lt;int&gt;());//以上inserter不能用于array\t\tcout&lt;&lt;\"filtering integer vector for values greater than 8\\n\";\tfilter(ivec.begin(),ivec.end(),back_inserter(ivec2),elem_size,greater&lt;int&gt;());\t\treturn 0;}3.10 使用iostream Iterator#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;#include&lt;iterator&gt;#include&lt;fstream&gt;using namespace std;int main(){\t/*\t//从文件读取\tifstream in_file(\"input_file.txt\");\tofstream out_file(\"out_file.txt\");\t\tif(!in_file||!out_file)\t{\t\tcerr&lt;&lt;\"!!unable to open the necessary files.\\n\";\t\treturn -1;\t}\tistream_iterator&lt;string&gt;is(infile);//first iterator\tistream_iterator&lt;string&gt;eof; //在定义istream_iterator时，不指定对象，就代表为last iterator\t\tvector&lt;string&gt;text;\tcopy(is,eof,back_inserter(text));//泛型算法copy()\t\tostream_iterator&lt;string&gt;os(out_file,\" \"); //第一个参数为字符串输出位置，第二个参数为各个元素被输出时的分隔符\tcopy(text.begin(),text.end(),os); //copy()函数将text每一个元素一一写到os所表示的ostream上\t*/\t//从标准输入输出设备读取\tistream_iterator&lt;string&gt;is(cin);//first iterator\tistream_iterator&lt;string&gt;eof; //在定义istream_iterator时，不指定对象，就代表为last iterator\t\tvector&lt;string&gt;text;\tcopy(is,eof,back_inserter(text));//泛型算法copy()\t\tsort(text.begin(),text.end());\t\tostream_iterator&lt;string&gt;os(cout,\" \"); //第一个参数为字符串输出位置，第二个参数为各个元素被输出时的分隔符\tcopy(text.begin(),text.end(),os); //copy()函数将text每一个元素一一写到os所表示的ostream上\t\t\treturn 0;}泛型算法partition()std::partition会将区间[first,last)中的元素重新排列，满足判断条件pred的元素会被放在区间的前段，不满足pred的元素会被放在区间的后段。可以用来区分奇偶。该算法不能保证元素的初始相对位置，如果需要保证初始相对位置，应该使用stable_partition.#include&lt;bits/stdc++.h&gt;using namespace std;bool cmp(string s){ return s.size()&lt;=5;}int main(){ vector&lt;string&gt; v1 = {\"fjskf\", \"fjslfjksl\", \"fjsklfk\", \"f\", \"fdds\", \"fjs\", \"fjslf\"}; vector&lt;string&gt; v2 = {\"fjskf\", \"fjslfjksl\", \"fjsklfk\", \"f\", \"fdds\", \"fjs\", \"fjslf\"}; auto x1=partition(v1.begin(),v1.end(),cmp());//将满足条件的放在前面，不满足条件的放在后面，不保证稳定。 auto x2=stable_partition(v2.begin(),v2.end(),cmp());//将满足条件的放在前面，不满足条件的放在后面，保证稳定。}第4章 基于对象的编程风格class由两部分组成：公开的(public)操作函数、私有的(private)实现细节。4.1 如何实现一个classClass声明：class Stack;//class 定义class Stack{public: bool push(const string&amp;); bool pop(string &amp;elem); bool peek(string &amp;elem); bool find(const string&amp;)const; bool empty(); bool full(); //size()定义与class本身中 int size(){return _stack.size();}private: vector&lt;string&gt;_stack;};//在class主体之外定义成员函数//::类作用域解析运算符inline bool Stack::empty(){ return _stack.empty();}bool Stack::pop(string &amp;elem){ if(empty()) { return false; } elem = _stack.back(); _stack.pop_back(); return true;}bool Stack::find(const string &amp;elem)const{\treturn ::find(_stack.begin(),_stack.end(),elem)!=_stack.end();//:;全局作用域，如果不加，会递归到自己的find()}4.2 什么是构造函数和析构函数构造函数(Constructor)用来初始化data member，函数名称必须和类名相同，可以被重载。class Triangular{public: //一组重载的constructor Triangular();//默认 Triangular(int len); Triangular(int len,int beg_pos); private: string name; int _length; int _beg_pos; int _next;};//构造函数初始化,法一Triangular::Triangular(int len,int bp){ _length = len&gt;0?len:1; _beg_pos = bp&gt;0?bp:1; _next = _bet_pos-1;}//法二：成员初始化列表,主要用于将参数传给member class objectTriangular::Triangular(int len,int bp) :_name(\"Triangular\") //将Triangular传给_name{ _length = len&gt;0?len:1; _beg_pos = bp&gt;0?bp:1; _next = _bet_pos-1;}析构函数(destructor)用于释放在constructor中或对象生命周期中分配的资源。名称为：类名加上’~’的前缀，不能被重载。class Matrix{ public: Matrix(int row,int col) :_row(row),_col(col) {\t\t_pmat = new double(row*col); } ~Matrix() { delete []_pmat; //释放内存 }private: int _row,_col; double* _pmat;};成员逐一初始化Triangular tri1(8);Triangular tri2 = tri1; //_length、_beg_pos、_next会依次从tri1复制到tri2.成员逐一初始化可能导致已经释放内存的数组重新进行释放内存操作（对指针操作时），会报错。为防止这个问题，通过提供一个copy constructor。Matrix:Matrix(const Matrix &amp;rhs) :_row(rhs._row),_col(rhs._col){ //对rhs._pmat所指的数组产生一份完全副本 int elem_cnt = _row*_col; _pmat = new double[elem_cnt]; for(int ix=0;ix&lt;elem_cnt;++ix) { _pmat[ix] = rhs._pmat[ix]; }}4.3 何谓mutable（可变）和const（不变）int sum(const Triangular &amp;train){ int beg_pos = train.beg_pos(); int length = train.length(); int sum = 0; for(int ix=0;ix&lt;length;++ix) sum+=train.elem(beg_pos+ix); return sum;}train为const reference参数，train不能被修改。需要在调用train的相关函数时，加上const。int length() const{return _length;}当想要修改与类状态无关的数据成员，加上关键字mutable。class Triangular{public: bool next(int &amp;val)const; void next_reset()const {_next = _beg_pos-1;} private: mutable int _next; //修改_next不会被视为改变了class object的状态 int _beg_pos; int _length;}4.4 什么是this指针this指针：在成员函数内指向其调用者(一个对象)。Triangular&amp; Triangular::copy(const Triangular &amp;rhs){ if(this!=rhs) { _length = rhs._length; _beg_pos = rhs._beg_pos; _next = rhs._beg_pos-1; } return *this;}4.5 静态类成员static data member表示唯一的、可共享的member，只有唯一的一份实体，类似全局对象。class Triangular{public: //...private: static vector&lt;int&gt;_elems;};//在程序代码文件中进行定义vector&lt;int&gt;Triangular::_elems;static member function一般情况下，成员函数必须通过某个类的对象进行调用（实例化对象，通过对象来调用成员函数）。bool Triangular::is_elem(int value){ if(!_elems.size()||_elems[_elems.size()-1]&lt;value) gen_elems_to_value(value); vector&lt;int&gt;::iterator found_it; vector&lt;int&gt;::iterator end_it = _elems.end(); found_it = find(_elems.begin(),end_it,value); return found_it!=end_it;}is_elem()并未访问任何non_static data member，就无需通过类的对象进行调用。if(Triangular::is_elem(8)) //这样调用即可4.6 打造一个Iterator Class为Iterator Class定义!-、*、++等运算符class Triangular_iterator{public: Triangular_iterator(int index):_index(index-1){}; bool operator==(const Triangular_iterator&amp;)const; bool operator!=(const Triangular_iterator&amp;)const; int operator*()const; Triangular_iterator&amp; operator++(); //前置++ Triangular_iterator&amp; operator++(int); //后置++private: void check_integrity()const; int _index;}inline bool Triangular_iterator::operator==(const Triangular_iterator &amp;ths)const{ return _index==rhs._index; //判断两个Triangular_iterator的_index是否相等}inline int Triangular_iterator::operator*()const{ check_integrity(); return Triangular::_elems[_index];}inline void Triangular_iterator check_integrity()const{ if(_index&gt;=Triangular::_max_elems) throw iterator_overflow(); if(_index&gt;=Triangular::_elems.size()) Triangular::gen_elements(_index+1);}//前置++inline Triangular_iterator&amp; Triangular_iterator::operator++(){ ++_index; check_integrity(); return *this;}//后置++ inline Triangular_iterator&amp; Triangular_iterator::operator++(int){ Triangular_iterator tmp = *this; ++_index; check_integrity(); return tmp;;}4.7 友元friend为了访问private member，可以在某个函数原型前加上关键字friend。//将重载函数声明为某个class的friendclass Triangular{ friend int Triangular_iterator::operator*(); friend void Triangular_iterator::check_integrity();}4.8 实现一个function objectfunction object是一种“提供有function call运算符”的class。#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;class LessThan{public:\t\tLessThan(int val):_val(val){}\t\tint comp_val()const {return _val;} //基值的读取\t\tvoid comp_val(int nval){_val=nval;} //基值的写入\t\tbool operator()(int _value)const;private:\t\t\tint _val;};inline bool LessThan::operator()(int value)const {return value&lt;_val;}int count_less_than(const vector&lt;int&gt;&amp;vec,int comp){\tLessThan It(comp);\t\tint count = 0;\tfor(int ix=0;ix&lt;vec.size();++ix)\t{\t\tif(It(vec[ix]))\t\t++count;\t}\treturn count;}void print_less_than(const vector&lt;int&gt;&amp;vec,int comp,ostream &amp;os=cout){\tLessThan It(comp);\tvector&lt;int&gt;::const_iterator iter = vec.begin();\tvector&lt;int&gt;::const_iterator it_end = vec.end();\t\tos&lt;&lt;\"elements less than \"&lt;&lt;It.comp_val()&lt;&lt;endl;\twhile((iter=find_if(iter,it_end,It))!=it_end)\t{\t\tos&lt;&lt;*iter&lt;&lt;' ';\t\t++iter;\t}}int main(){\tint ia[16] = {17,12,44,9,18,45,6,14,\t23,67,9,0,27,55,8,16};\tvector&lt;int&gt;vec(ia,ia+16);\t\tint comp_val=20;\tcout&lt;&lt;\"Number of elements less than \"&lt;&lt;comp_val&lt;&lt;\" are \"&lt;&lt;count_less_than(vec,comp_val)&lt;&lt;endl;\tprint_less_than(vec,comp_val);\t\t\treturn 0;}4.10 重载iostream运算符//重载output运算符ostream&amp; operator&lt;&lt;(ostream &amp;os,const Triangular &amp;ths){ os&lt;&lt;\"(\"&lt;&lt;rhs.beg_pos()&lt;&lt;\",\"&lt;&lt;rhs.length()&lt;&lt;\")\"; rhs.display(rhs.length(),rhs.beg_pos(),os); return os;}//重载input运算符istream&amp; operator&gt;&gt;(istream &amp;is,Triangular &amp;rhs){ char ch1,ch2; int bp,len; //假设输入为(3,6) 6 10 15 21 28 36 //那么ch1=='('，bp==3，ch2==','，len==6 is&gt;&gt;ch1&gt;&gt;bp&gt;&gt;ch2&gt;&gt;len; rhs.beg_pos(bp); rhs.length(len); rhs.next_reset(); return is;}//调用output运算符Triangular tri(6,3);cout&lt;&lt;tri&lt;&lt;\"\\n\";//输出结果(3,6) 6 10 15 21 28 364.11 指针，指向Class Member Function将PtrType声明为一个指针，指向num_sequence的member functiontypedef void(num_sequence::*PtrType)(int);PtrType pm=0; class num_sequence{public:\ttypedef void(num_sequence::*PtrType)(int); //_pmf可指向下列任何一个函数 void fibonacci(int); void pell(int); void lucas(int); void triangular(int); void sequare(int); void pentagonal(int);private: vector&lt;int&gt;* _elem; //指向目前所用的vector PtrType _pmf; static const int num_seq=7; static PtrType func_tbl[num_seq]; static vector&lt;vector&lt;int&gt; &gt;seq;};定义一个指针，并指向成员函数fibonacci()PtrType pm = &amp;num_sequence::fibonacci;对static data member定义const int num_sequence::num_seq;vector&lt;vector&lt;int&gt; &gt;num_sequence::seq(num_seq);num_sequence::PtrType num_sequence::func_tbl[num_seq] = {\t0,\t&amp;num_sequence::fibonacci,\t&amp;num_sequence::pell,\t&amp;num_sequence::lucas,\t&amp;num_sequence::triangular,\t&amp;num_sequence::sequare,\t&amp;num_sequence::pentagonal}习题enum枚举enum Roster {Tom, Sharon, Bill, Teresa, John};该语句将创建一个名为 Roster 的数据类型。因为单词 enum 是 C++ 关键字，所以它必须小写，值得注意的是，数据类型本身的名字是以大写字母开头的。虽然这并非必须，但是绝大多数程序员都会釆用首字母大写的形式。和 Roster 数据类型关联的命名整数常量被称为枚举量，Roster 数据类型的变量可能只是关联到这些枚举量的值之一，但它们的值是什么呢？默认情况下，编译器设置第一个枚举量为 0，下一个为 1，以此类推。在上述示例中，Tom 的值将是 0，Sharon 的值为 1，等等。最后一个枚举量 John 的值为 4。itoa()itoa():char *itoa( int value, char *string,int radix);value：欲转换的数据。string：目标字符串的地址。radix：转换后的进制数，可以是10进制、16进制等，范围必须在 2-36。功能：将整数value 转换成字符串存入string 指向的内存空间 ,radix 为转换时所用基数(保存到字符串中的数据的进制基数)。返回值：函数返回一个指向 str，无错误返回。第5章 面向对象编程风格5.1 面向对象编程概念特征：继承、多态、动态绑定。多态：让基类的pointer或reference得以十分透明地指向任何一个派生类地对象。动态绑定：每次loan_check_in()执行时，仅能在执行过程中依据mat所指地实际对象来决定调用哪一个check_in()。找出实际被调用地究竟是哪一个派生类地check_in()函数这一解析操作会延时到运行时进行。5.2 漫游：面向对象编程地思维默认情况下，成员函数的解析在编译时静态地进行，若要令其在运行时动态进行，需要在它的声明前加上关键字virtual。静态成员函数无法被声明为虚函数。被声明为protected的所有成员都可以被派生类直接访问，除了派生类之外，都不能直接访问protected成员。#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;class LibMat{public:\tLibMat()\t{\t\tcout&lt;&lt;\"LibMat::LibMat() default constructor!\\n\";\t}\tvirtual ~LibMat()\t{\t\tcout&lt;&lt;\"LibMat::~LibMat() destructor!\\n\";\t}\tvirtual void print()const\t{\t\tcout&lt;&lt;\"LibMat::print()--I am a LibMat object!\\n\";\t}};class Book:public LibMat{public:\tBook(const string &amp;title,const string &amp;author):_title(title),_author(author)\t{\t\tcout&lt;&lt;\"Book::Book(\"&lt;&lt;_title&lt;&lt;\",\"&lt;&lt;_author&lt;&lt;\") constructor\\n\";\t}\tvirtual ~Book()\t{\t\tcout&lt;&lt;\"Book::~Book()destructor!\\n\";\t}\tvirtual void print()const\t{\t\tcout&lt;&lt;\"Book::print()--I am a Book object!\\n\"\t\t\t&lt;&lt;\"My title is: \"&lt;&lt;_title&lt;&lt;'\\n'\t\t\t&lt;&lt;\"My author is: \"&lt;&lt;_author&lt;&lt;'\\n';\t}\tconst string&amp; title()const\t{\t\treturn _title;\t}\tconst string&amp; author()const\t{\t\treturn _author;\t}protected:\tstring _title;\tstring _author;};class AudioBook:public Book{public:\tAudioBook(const string &amp;title,const string &amp;author,const string &amp;narrator):Book(title,author),_narrator(narrator)\t{\t\tcout&lt;&lt;\"AudioBook::AudioBook( \"&lt;&lt;_title&lt;&lt;\", \"&lt;&lt;_author&lt;&lt;\", \"&lt;&lt;_narrator&lt;&lt;\") constructor\\n\";\t}\t~AudioBook()\t{\t\tcout&lt;&lt;\"AudioBook::~AudioBook()destructor\\n\";\t}\t\tvirtual void print()const\t{\t\tcout&lt;&lt;\"AudioBook::print()--I am a AudioBook object!\\n\"\t\t\t&lt;&lt;\"My title is:\" &lt;&lt;_title&lt;&lt;'\\n'\t\t\t&lt;&lt;\"My author is: \"&lt;&lt;_author&lt;&lt;'\\n'\t\t\t&lt;&lt;\"My narrator is: \"&lt;&lt;_narrator&lt;&lt;endl;\t}\tconst string&amp; narrator()\t{\t\treturn _narrator;\t}protected:\tstring _narrator;};void print(const LibMat &amp;mat){\tcout&lt;&lt;\"in global print():about to print mat.print()\\n\";\tmat.print();}int main(){//\tcout&lt;&lt;\"\\n\"&lt;&lt;\"Creating a LibMat object to print()\\n\";//\tLibMat libmat;//\tprint(libmat);\tAudioBook ab(\"Mason and Dixon\",\"Thomax Pynchon\",\"Edwin Leonard\");\t\tprint(ab);\t\treturn 0;}5.4 定义一个抽象的基类设计抽象类的步骤：（1）找到所有子类共通的操作行为；（2）找到哪些操作行为与类型相关，也就是说，有哪些行为必须根据不同的派生类而有不同的实现方式。这些操作行为应该成为整个类继承体系中的虚函数。（3）找到每个操作行为的访问层级：public、private、protected注意：（1）每个虚函数。要么得有其定义，要么可设为纯虚函数。如果对于该类而言，这个虚函数并无实际意义，那么令他为纯虚函数。virtual int elem(int pos)const = 0;（2）根据一般规则，凡基类定义有一个或多个虚函数，应将其析构函数声明为virtual。class num_sequence(){public: virtual ~num_sequence(){};}；class num_sequence{public: virtual ~num_sequence(){}; virtual int elem(int pos)const = 0; virtual const char* what_am_i()const = 0; static int max_elems() {return _max_elems;} virtual ostream&amp; print(ostream &amp;os=cout)const = 0;protected: virtual void gen_elems(int pos)const = 0; bool check_integrity(int pos,int size)const; const static int _max_elems = 1024;};bool num_sequence::check_integrity(int pos,int size)const{ if(pos&lt;=0||pos&gt;_max_elems) { cerr&lt;&lt;\"!!invalid position: \"&lt;&lt;pos&lt;&lt;\"Cannot honor request\\n\"; return false; } if(pos&gt;size) { gen_elems(pos); } return true;}ostream&amp; operator&lt;&lt;(ostream &amp;os,const num_sequence &amp;ns){ return ns.print(os);}5.5 定义一个派生类派生类由两部分组成：（1）基类构成的子对象，由基类的non-static data member组成；（2）派生类的部分（由派生类的non-static data member组成）。注意：（1）基类的public member在派生类中同样是public，开放给派生类的用户使用；基类中的protected member在派生类中是protected，只能给后续派生类使用，无法给目前这个派生类使用；（2）当派生类有某个member与其基类的member同名时，便会遮掩住基类的那份member。如果要在派生类内使用继承来的那份member，必须利用class scope运算符加以限定。class Fibonacci:public num_sequence{public: Fibonacci(int len=1,int beg_pos=1):_length(len),_beg_pos(beg_pos){} virtual int elem(int pos)const; virtual const char* what_am_i()const{return \"Fibonacci\";} virtual ostream&amp; print(ostream &amp;os=cout)const; int length()const {return _length;} int beg_pos()const {return _beg_pos;}protected: virtual void gen_elems(int pos)const; int _length; int _beg_pos; static vector&lt;int&gt; _elems;};int Fibonacci::elem(int pos)const{ if(!check_integrity(pos,_elems.size())) //调用的是继承来的check_integrity() { return 0; } if(pos&gt;_elems.size()) { Fibonacci::gen_elems(pos);//通过class scope运算符，告诉编译器，调用的是Fibonacci派生类的gen_elems() } return _elems[pos-1];}void Fibonacci::gen_elems(int pos)const{ if(_elems.empty()) { _elems.push_back(1); _elems.push_back(1); } if(_elems.size()&lt;=pos) { int ix = _elems.size(); int n_2 = _elems[ix-2]; int n_1 = _elems[ix-1]; for(;ix&lt;=pos;++ix) { int elem = n_2+n_1; _elems.push_back(elem); n_2=n_1;n_1=elem; } }}ostream&amp; Fibonacci::print(ostream &amp;os)const{ int elem_pos = _beg_pos-1; int end_pos = elem_pos + _length; if(end_pos&gt;_elems.size()) { Fibonacci::gen_elems(end_pos); } while(elem_pos&lt;end_pos) { os&lt;&lt;_elems[elem_pos++]&lt;&lt;' '; } return os;}int main(){ Fibonacci fib; cout&lt;&lt;\"fib:beginning at element 1 for 1 element: \"&lt;&lt;fib&lt;&lt;endl; Fibonacci fib2(16); cout&lt;&lt;\"fib2:beginning at element 1 for 16 element: \"&lt;&lt;fib2&lt;&lt;endl; Fibonacci fib3(8,12); cout&lt;&lt;\"fib3:beginning at element 12 for 8 element: \"&lt;&lt;fib3&lt;&lt;endl;}5.7 基类应该多么抽象在当前的设计下，抽象基类只提供接口，并未提供任何实现。当需要加入新的数列类时，比较繁琐。提出新的设计方式：将所以派生类共有的实现内容剥离出来，移至基类内。class num_sequence{public: virtual ~num_sequence(){}; virtual int elem(int pos)const = 0; virtual const char* what_am_i()const = 0; int elem(int pos)const; ostream&amp; print(ostream &amp;os=cout)const; int length()const {return _length;} int beg_pos()const {return _beg_pos;} static int max_elems() {return 64;}protected: virtual void gen_elems(int pos)const = 0; bool check_integrity(int pos,int size)const; //num_sequence是抽象基类，无法为它定义任何对象，所以声明为protected num_sequence(int len,int bp,vector&lt;int&gt;&amp;re):_length(len),_beg_pos(bp),_relems(re){} int _length; int _beg_pos; vector&lt;int&gt; &amp;_relems;};class Fibonacci:public num_sequence{public: Fibonacci(int len=1,int beg_pos=1); virtual const char* what_am_i()const { return \"Fibonacci\"; } protected: virtual void gen_elems(int pos)const; static vector&lt;int&gt;_elems;};5.9 在派生类中定义一个虚函数当定义派生类时，必须决定，究竟要将基类中的虚函数覆盖掉，还是原封不动地加以继承。如果我们继承了纯虚函数，那么派生类也将被视为抽象类，无法为它定义任何对象。如果要覆盖掉基类提供地虚函数，派生类定义地函数原型必须完全符合基类所声明的函数原型，包括：参数列表、返回类型、常量性(const-ness)。虚函数的静态解析派生类虚函数机制不会出现预期行为：（1）基类的constructor和destructor内当我们构造派生类对象时，基类的constructor会先被调用，如果在基类的constructor中调用某个虚函数，此时调用的就是基类中的虚函数。（2）当我们使用的是基类的对象，而非基类对象的pointer或reference时void print(LibMat object,const LibMat *pointer,const LibMat &amp;reference){ object.print();//此时调用的是LibMat::print() //会通过虚函数机制进行解析 pointer-&gt;print(); reference.print();}5.10 运行时的类型鉴定机制每一个类中都有一份what_am_i()函数，为了简化，利用运行时的类型鉴定机制。#include&lt;typeinfo&gt;inline const char* num_sequence::what_am_i()const{ return typeid(*this).name();}if(typeid(*ps)==typeid(Fibonacci)){ Fibonacci *pf = static_cast&lt;Fibonacci*&gt;(ps);//经过static_cast运算符转换指针变为派生类指针 pf-&gt;gen_elems(64);}//static_cast有个潜在危险，就是编译器无法确定转换类指针从基类到派生类是不是转换对了//使用dynamic_cast运算符提供有条件的转换，可避免这个危险if(Fibonacci *pf = dynamic_cast&lt;Fibonacci*&gt;(os)) pf-&gt;gen_elems(64);第6章 以template进行编程二叉树的遍历：（1）前序遍历：被遍历的节点本身先被打印，然后打印左子树内容，最后打印右子树内容；（2）中序遍历：先打印左子树，然后是节点本身，最后是右子树；（3）后续遍历：先打印左子树，再打印右子树，最后是节点本身。6.2 Class Template的定义template&lt;typename elemType&gt;class BinaryTree{public: BinaryTree(); BinaryTree(const BinaryTree&amp;); ~BinaryTree(); BinaryTree&amp; operator=(const BinaryTree&amp;); bool empty(){return _root==0}; void clear() { if(_root) clear(_root); _root = 0; }private: void clear(BTnode&lt;elemType&gt;*) BTnode&lt;elemType&gt; *_root; //将src所指子树复制到tar所指子树 void copy(BTnode&lt;elemType&gt;*tar,BTnode&lt;elemType&gt;*src);};template&lt;typename elemType&gt;inline BinaryTree&lt;elemType&gt;::BinaryTree():_root(0){}template&lt;typename elemType&gt;inline BinaryTree&lt;elemType&gt;::BinaryTree(const BinaryTree &amp;rhs){ copy(_root,rhs._root);}template&lt;typename elemType&gt;inline BinaryTree&lt;elemType&gt;::~BinaryTree(){ clear();}template&lt;typename elemType&gt;inline BinaryTree&lt;elemType&gt;&amp; BinaryTree&lt;elemType&gt;::operator=(const BinaryTree &amp;rhs){ if(this!=&amp;rhs) { clear(); copy(_root,rhs._root); } return *this;}6.3 Template类型参数的处理将所有的template类型参数视为”class类型”，应使用下列的初始化方法。template&lt;typename valType&gt;inline BTnode&lt;valType&gt;::BTnode(const valType &amp;val):_val(val) //将valType视为class类型{ _cnt=1; _lchild = _rchild=0;}6.4 实现一个Class Template插入某个新值：template&lt;typename elemType&gt;inline void BinaryTree&lt;elemType&gt;::insert(const elemType &amp;elem){ if(!_root) { _root = new BTnode&lt;elemType&gt;(elem);//如果根节点设为设定，就分配一块新的BTnode需要的内存空间 } else { _root-&gt;insert_value(elem); }}//小于根节点的值放在左子树，大于的放在右子树，每个数值在数中只出现一次，使用cnt记录该节点插入的次数template&lt;typename valType&gt;void BTnode&lt;valType&gt;::insert_value(const valType &amp;val){ if(val==_val) { _cnt++; return; } if(val&lt;_val)//小于根节点，放在左子树 { if(!_lchild) { _lchild = new BTnode(val); } else { _lchild-&gt;insert_value(val); } } else //大于根节点，放在右子树 { if(!_rchild) { _rchild = new BTnode(val); } else { _rchild-&gt;insert_value(val); } }}移除某值：以节点的右子节点取代节点本身，然后搬移左子节点，使它成为右子节点的左子树的叶节点。如果此刻无右子节点，那么就以左子节点取代节点本身。将根节点的移除操作以特例处理。template&lt;typename elemType&gt;inline void BinaryTree&lt;elemType&gt;::remove(const elemType &amp;elem){ if(_root) { if(_root-&gt;val==elem) remove_root(); else _root-&gt;remove_value(elem,_root); }}//搬移左子节点template&lt;typename elemType&gt;void BTnode&lt;valType&gt;::lchild_leaf(BTnode *leaf,BTnode *subtree){ while(subtree-&gt;_lchild) subtree = subtree-&gt;_lchild; subtree-&gt;_lchild = leaf;}//根节点的移除template&lt;typename elemType&gt;void BinaryTree&lt;elemType&gt;::remove_root(){ if(!_root) return; BTnode&lt;elemType&gt;*tmp = _root; if(_root-&gt;_rchild) { //右子节点设为根节点 _root = _root-&gt;_rchild; //将左子节点搬移到右子节点的左子树底部 if(tmp-&gt;_lchild) { BTnode&lt;elemType&gt;*lc = tmp-&gt;_lchild;//原左子节点 BTnode&lt;elemType&gt;*newlc = _root-&gt;_lchild;//原右子树的左子节点 if(!newlc) { //没有左子节点， _root-&gt;_lchild = lc; } else { BTnode&lt;elemType&gt;::lchild_leaf(lc,newlc); } } } else _root = _root-&gt;_lchild; delete tmp;}//其他节点的移除,remove_value()拥有两个参数：将被删除的值以及一个指向目前关注的节点的父节点的指针template&lt;typename valType&gt;void BTnode&lt;valType&gt;::remove_value(const valType &amp;val,BTnode *&amp;prev){ if(val&lt;_val) { if(!_lchild) return; else _lchild-&gt;remove(val,_lchild); } else if(val&gt;_val) { if(!_rchild) return; else _rchild-&gt;remove_value(val,_rchild); } //找到了，重置此树，并删除这一节点 else { if(_rchild) { prev = _rchild; if(_lchild) { if(!prev-&gt;_lchild) prev-&gt;_lchild = _lchild; else BTnode&lt;valType&gt;::lchild_leaf(_lchild,prev-&gt;lchild); } } else prev = _lchild; delete this; } }移除整颗二叉树template&lt;typename elemType&gt;void BinaryTree&lt;elemType&gt;::clear(BTnode&lt;elemType&gt;*pt){ if(pt) { clear(pt-&gt;_lchild); clear(pt-&gt;_rchild); delete pt; }}遍历算法//前序遍历template&lt;typename valType&gt;void BTnode&lt;valType&gt;::preorder(BTnode *pt,ostream &amp;os)const{ if(pt) { display_val(pt,os); if(pt-&gt;_lchild) preorder(pt-&gt;_lchild,os); if(pt-&gt;_rchild) preorder(pt-&gt;_rchild,os); }}//中序遍历template&lt;typename valType&gt;void BTnode&lt;valType&gt;::inorder(BTnode *pt,ostream &amp;os)const{ if(pt) { if(pt-&gt;_lchild) inorder(_lchild,os); display_val(pt,os); if(pt-&gt;_rchild) inorder(_rchild,os); }}//后序遍历template&lt;typename valType&gt;void BTnode&lt;valType&gt;::postorder(BTnode *pt,ostream &amp;os)const{ if(pt) { if(pt-&gt;_lchild) postorder(pt-&gt;_lchild,os); if(pt-&gt;_rchild) postorder(pt-&gt;_rchild,os); display_val(pt,os); }}6.5 一个以Function Template完成的Output运算符template&lt;typename valType&gt;inline ostream&amp; operator&lt;&lt;(ostream &amp;os,const BinaryTree&lt;elemType&gt; &amp;bt){ os&lt;&lt;\"Tree: \"&lt;&lt;endl; bt.print(os); return os;}6.6 常量表达式与默认参数值可以用常量表达式作为template的参数，将“对象所含的元素个数”参数化。template&lt;int len,int beg_pos=1&gt;Fibonacci&lt;16&gt;fib;全局作用域内的函数及对象，其地址也是一种常量表达式，因此也可以被拿来表达这一形式的参数。template&lt;void (*pf)(int pos,vector&lt;int&gt;&amp;seq)&gt;class numeric_sequence{public: numeric_sequence(int len,int geb_pos=1) { if(!pf) //产生错误信息并退出 _len = len&gt;0?len:1; _beg_pos = beg_pos&gt;0?beg_pos:1; pf(beg_pos+len-1,_elems); }private: int _len; int _beg_pos; vector&lt;int&gt;_elems;};//pf是一个指向“依据特定数列类型，产生pos个元素，放到vector seq内”的函数//用法如下：void fibonacci(int pos,vector&lt;int&gt;&amp;seq);void pell(int pos,vector&lt;int&gt;&amp;seq);numeric_sequence&lt;fibonacci&gt; ns_fib(12);numeric_sequence&lt;pell&gt; ns_pell(18,8);6.7 以Template参数作为一种设计策略template&lt;tyename elemType,typename Comp = less&lt;elemType&gt; &gt;class LessThanPred{public: LessThanPred(const elemType &amp;val):_val(val){} bool operator()(const elemType &amp;val)const { return Comp(val,_val); } void val(const elemType &amp;newval){_val = newval;} elemType val()const {return _val;}private: elemType _val;};class StringLen{public: bool operator()(const string &amp;s1,const string &amp;s2) { return s1.size()&lt;s2.size(); }};//应用LessThanPred&lt;int&gt; ltpi(1024);LessThanPred&lt;int,StringLen&gt; ltps(\"Pooh\");6.8 Member Template Functionclass PrintIt{public: PrintIt(ostream &amp;os):_os(os){} //成员模板函数 template&lt;typename elemType&gt; void print(const elemType &amp;elem,char delimiter = '\\n') { _os&lt;&lt;elem&lt;&lt;delimiter; }private: ostream&amp; _os;};int main(){ PrintIt to_standard_out(cout); to_standard_out.print(\"hello\"); to_standard_out.print(1024); string my_string(\"i am a string\"); to_standard_out.print(my_string);}第7章 异常处理7.1 抛出异常异常处理机制：异常的鉴定与发出、异常的处理方式。异常出现之后，程序被暂停，异常处理机制开始搜索程序中有能力处理这一异常的地点，异常被处理完毕之后，程序的执行便会继续，从异常处理点接着执行下去。C++通过throw表达式产生异常，throw表达式类似函数调用inline void Triangular_iterator::check_interity(){ if(_index&gt;=Triangular::_max_elems) throw iterator_overflow(index,Triangular::_max_elems); if(_index&gt;=Triangular::_elems.size()) Triangular::gen_elements(_index+1);}class iterator_overflow{public: iterator_overflow(int index,int max):_index(index),_max(max){} int index(){return _index;} int max(){return _max;} void what_happened(ostream &amp;os=cerr) { os&lt;&lt;\"Internal error current index \" &lt;&lt;_index&lt;&lt;\" exceeds maximum bound: \"&lt;&lt;_max; }private: int _index; int _max;};7.2 捕获异常利用单条或一连串的catch子句来捕获(catch)被抛出的异常。catch子句由三部分组成：关键字catch、小括号内的一个类型或对象、大括号内的一组语句（用来处理异常）。extern void log_message(const char*);extern string err_messages[];extern ostream log_file;bool some_function(){ bool status = true; catch(int errno) { log_message(err_messages[errno]); status = false; } catch(const char *str) { log_message(str); status = false; } catch(iterator_overflow &amp;iof) { iof.what_happened(log_file); status = false; } return status;}上述语句可以处理前一节所抛出的三个异常对象：throw 42;throw \"panic:no buffer!\";throw iterator_overflow(_index,Triangular::_max_elems);有时候我们无法完成异常的完整处理，需要重新抛出异常，寻求其它catch子句的协助：catch(iterator_overflow &amp;iof){ log_message(iof.what_happened()); //重新抛出异常，令另一个catch子句接手处理 throw; //重新抛出时，只需写下throw关键字即可}如果想捕获任何类型的异常，在异常声明部分指定省略号即可：catch(...){ log_message(\"exception of unknown type\"); //清理(clean up)然后退出}7.3 提炼异常catch子句和try块一起使用。try块以关键字try作为开始，然后是大括号内的一连串程序语句。catch子句放在try块的末尾，这表示如果try块内有任何异常发生，便由接下来的catch子句加以处理。C++规定每一个异常都应该被处理。bool has_elem(Triangular_iterator first,Triangular_iterator last,int elem){ bool status = true; try { while(first!=last) { if(*first==elem) return; ++first; } } //捕获异常 catch(iterator_overflow &amp;iof) { log_message(iof.what_happened()); log_message(\"check if iterator address same container\"); } status = false; return status;}7.5 标准异常如果new表达式无法从程序的空闲空间分配到足够的内存，它会抛出bad_alloc异常对象。vector&lt;string&gt;* init_text_vector(ifstream &amp;infile){ vector&lt;string&gt;*ptext = 0; try{ ptext = new vector&lt;string&gt;; } catch(bad_alloc){ cerr&lt;&lt;\"ouch.headp memory exhausted!\\n\"; } return ptext;}标准库定义了一套异常类体系，其根部是名为exception的抽象基类。exception声明有一个what()虚函数，会返回一个const char*，来表示被抛出异常的文字描述。将自己编写的iterator_overflow继承于exception基类之下，并提供自己的what()。#include&lt;exception&gt;class iterator_overflow : public exception{public: iterator_overflow(int index,int max):_index(index),_max(max){} int index(){return _index;} int max(){return _max;} const char* what()const;private: int _index; int _max;};#include&lt;sstream&gt; //使用ostringstream class#include&lt;string&gt;const char* iterator_overflow::what()const{ ostringstream ex_msg;//可以将int等对象转化为对应的字符串 static string msg; ex_msg&lt;&lt;\"Internal error: current index \"&lt;&lt;_index&lt;&lt;\" exceeds maxium bound: \"&lt;&lt;_max; //萃取出string对象 msg = ex_msg.str(); //萃取出const char*表达式 return msg.c_str();; //将string转化为C-Style字符串}" }, { "title": "C++学习路线", "url": "/posts/C++%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/", "categories": "C++learning", "tags": "学习路线", "date": "2021-12-23 12:00:00 +0000", "snippet": "1. C++语言Essential C++ 侯捷C++primer第五版Effective C++ 侯捷More Effective C++侯捷C++标准程序库2. 数据结构和算法大话数据结构、剑指offer 配合刷题3. 计算机网络TCP/IP详解4. 操作系统深入理解操作系统5. 设计模式大话设计模式6. 应用与编程实践Linux：鸟哥的Linux私房菜 或 Linux就该这么学编译和调试工具材料：英语好的看GNU官方关于GCC和GDB的官方文档。中文版《debugging with gdb》,跟我一起写makefileLinux的环境编程：Unix环境高级编程 ，Linux高性能服务器编程，POSIX多线程程序设计" }, { "title": "动手学深度学习", "url": "/posts/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/", "categories": "算法", "tags": "算法", "date": "2021-12-08 12:00:00 +0000", "snippet": "第2章 预备知识2.1 数据操作Numpy仅支持CPU计算，而其他的深度学习框架支持GPU计算。张量表示数值组成的数组，具有一个轴的张量称为向量，两个轴的张量称为矩阵。# 使用arange创建一个行向量xx = torch.arange(12)# 张量中元素的总数x.numel()reshape时，要生成(3,4)的矩阵，一直行数，可以将列数归为-1，张量自动推断维度，即reshape(3,-1)。# 随机初始化torch.randn()对于任意具有相同形状的张量，常见的标准算术运算符（+、-、*、/和**）都可以被升级为按元素运算。广播机制：当张量形状不同时，利用广播机制执行按元素操作。工作方式为：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。a = torch.arange(3).reshape((3, 1))b = torch.arange(2).reshape((1, 2))a + b# 输出tensor([[0, 1], [1, 2], [2, 3]])节省内存：运行Y = X + Y时，会重新分配内存，id(Y)与之前不一样。使用Y[:] = X + Y，id(Y)与之前的一样。2.2 数据预处理2.2.1 读取数据集将数据集按行写入CSV文件import osos.makedirs(os.path.join('..', 'data'), exist_ok=True) # 选择路径为data文件夹data_file = os.path.join('..', 'data', 'house_tiny.csv') # 创建house_tiny.csvwith open(data_file, 'w') as f: f.write('NumRooms,Alley,Price\\n') # 列名 f.write('NA,Pave,127500\\n') # 每行表示一个数据样本 f.write('2,NA,106000\\n') f.write('4,NA,178100\\n') f.write('NA,NA,140000\\n')# 输出 NumRooms Alley Price0 NaN Pave 1275001 2.0 NaN 1060002 4.0 NaN 1781003 NaN NaN 1400002.2.2 处理缺失值# iloc进行位置索引# fillna() 替换缺失值inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]inputs = inputs.fillna(inputs.mean())print(inputs)one-hot encoding：将离散型特征的每一种取值都看成一种状态，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。# get_dummies() 转换为one-hot encodinginputs = pd.get_dummies(inputs, dummy_na=True)print(inputs)# 输出 NumRooms Alley_Pave Alley_nan0 3.0 1 01 2.0 0 12 4.0 0 13 3.0 0 12.2.5 练习# 获取缺失值最多的列，并删除该列nan_num = data.isna().sum(axis=0) #获取每列NAN的数nan_maxid = nan_num.idxmax()data = data.drop([nan_maxid],axis=1) #删除NAN最多的列2.3 线性代数2.3.5 张量算法的基本性质两个矩阵的按元素乘法称为哈达玛积（Hadamard product）（数学符号⊙）。2.3.6 降维沿某个轴计算A元素的累积总和：A = torch.arange(20).reshape(5,4)A.cumsum(axis=0)# 输出A = tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]])tensor([[ 0., 1., 2., 3.], [ 4., 6., 8., 10.], [12., 15., 18., 21.], [24., 28., 32., 36.], [40., 45., 50., 55.]])2.3.7 点积torch.dot(x,y)2.3.8 矩阵-向量积np.dot(A,x)torch.mv(A,x)2.3.9 矩阵-矩阵乘法torch.mm(A,B)2.3.10 范数L1范数：向量元素的绝对值之和L2范数：向量元素平方和的平方根(常用)u = torch.tensor([3.0, -4.0])torch.abs(u).sum() #L1范数torch.norm(u) #L2范数矩阵X的弗罗贝尼乌斯范数：矩阵元素平方和的平方根torch.norm(A)2.4 微分字符串的格式化输出# 指定matplotlib输出svg图表以获得更清晰的图像def use_svg_display():\tdisplay.set_matplotlib_formats('svg')matplotlib中plt.rcParams用法(设置图像细节)pyplot.gca()：获取当前axes对象，利用plt.gca()进行坐标轴的移动hasattr()用法：用于判断对象是否包含对应的属性。isinstance() 函数来判断一个对象是否是一个已知的类型，类似 type()。X = [X] # 将X列表化X = [[]] * len(X) # 创建包含X长度个空列表的大列表2.5 自动求导# 求梯度import torchx = torch.arange(4.0)x.requires_grad_(True)x.grady = 2 * torch.dot(x, x)y.backward() # 反向传播函数x.grad2.5.2 非标量变量的反向传播# 对非标量调用`backward`需要传入一个`gradient`参数，该参数指定微分函数关于`self`的梯度。在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的x.grad.zero_()y = x * x# 等价于y.backward(torch.ones(len(x)))y.sum().backward()x.grad2.5.3 分离计算计算z=u*x关于x的偏导数，同时将u作为常数处理，而不是z=x*x*x关于x的偏导数x.grad.zero_()y = x * xu = y.detach()z = u * x # 把u看作常数z.sum().backward()x.grad == utensor([True, True, True, True])2.6 概率2.6.1 基本概率论将概率分配给一些离散选择的分布称为多项分布（multinomial distribution）。import torchfrom torch.distributions import multinomialfrom d2l import torch as d2lfair_probs = torch.ones([6]) / 6 # 骰子概率multinomial.Multinomial(1, fair_probs).sample() # 随机数生成器# 输出tensor([0., 0., 0., 0., 0., 1.])#看这些概率如何随着时间的推移收敛到真实概率。让我们进行500组实验，每组抽取10个样本counts = multinomial.Multinomial(10, fair_probs).sample((500,)) #500组cum_counts = counts.cumsum(dim=0) # cumsum累加，dim=0指定行，列不变，行变，从第一行到最后一行的累加estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True) #计算每组概率d2l.set_figsize((6, 4.5))for i in range(6): d2l.plt.plot(estimates[:, i].numpy(), label=(\"P(die=\" + str(i + 1) + \")\"))d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')d2l.plt.gca().set_xlabel('实验次数') #plt.gca()坐标轴移动d2l.plt.gca().set_ylabel('估算概率')d2l.plt.legend();# 解决在Python的matplotlib.pyplot图表中显示中文from pylab import *mpl.rcParams['font.sans-serif'] = ['SimHei']mpl.rcParams['axes.unicode_minus'] = False2.7 查阅文档help(torch.ones)# 在Jupyter记事本中，我们可以使用?在另一个窗口中显示文档。例如，list?将创建与help(list)几乎相同的内容，并在新的浏览器窗口中显示它。此外，如果我们使用两个问号，如list??，将显示实现该函数的Python代码。第3章 线性回归网络3.1 线性回归损失函数：\\(L(w,b)=\\frac 1n\\sum_{i=1}^nl^{(i)}(w,b)=\\frac 1n\\sum_{i=1}^n\\frac 12(w^⊤x^{(i)}+b−y^{(i)})^2\\)小批量梯度下降：\\(w=w-\\frac {\\eta}{|B|}\\sum_{i\\in B}x^{(i)}(w^⊤x^{(i)}+b-y^{(i)})\\)\\[b=b-\\frac {\\eta}{|B|}\\sum_{i\\in B}(w^⊤x^{(i)}+b-y^{(i)})\\] B 表示每个小批量中的样本数，这也称为批量大小（batch size）。η表示学习率（learning rate）。 批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）。 调参（hyperparameter tuning）是选择超参数的过程。# 定义定时器class Timer: #@save \"\"\"记录多次运行时间。\"\"\" def __init__(self): self.times = [] self.start() def start(self): \"\"\"启动计时器。\"\"\" self.tik = time.time() def stop(self): \"\"\"停止计时器并将时间记录在列表中。\"\"\" self.times.append(time.time() - self.tik) return self.times[-1] def avg(self): \"\"\"返回平均时间。\"\"\" return sum(self.times) / len(self.times) def sum(self): \"\"\"返回时间总和。\"\"\" return sum(self.times) def cumsum(self): \"\"\"返回累计时间。\"\"\" return np.array(self.times).cumsum().tolist()3.2 线性回归的从零开始实现使用线性模型参数w=[2,-3.4]^T、b=4.2和噪声项ϵ生成数据集及其标签：\\(y=Xw+b+\\in\\)def synthetic_data(w, b, num_examples): \"\"\"生成 y = Xw + b + 噪声。\"\"\" X = torch.normal(0, 1, (num_examples, len(w)))#均值为零，方差为1的随机数 y = torch.matmul(X, w) + b #torch的矩阵乘法 y += torch.normal(0, 0.01, y.shape) #均值为零，方差为0.01的随机噪音 return X, y.reshape((-1, 1))true_w = torch.tensor([2, -3.4])true_b = 4.2features, labels = synthetic_data(true_w, true_b, 1000)features中的每一行都包含一个二维数据样本，labels中的每一行都包含一维标签值（一个标量）。d2l.set_figsize()d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1);# 第一个“1”表示列，第二个“1”表示散点大小3.2.2 读取数据集定义一个data_iter函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为batch_size的小批量。每个小批量包含一组特征和标签。def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) #索引 # 这些样本是随机读取的，没有特定的顺序 random.shuffle(indices) #打乱下标 for i in range(0, num_examples, batch_size): #从0开始，到num_examples结束，每次跳batch_size个距离 batch_indices = torch.tensor( indices[i: min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices] #yield就是返回一个值，并且记住这个返回的位置，下次迭代就从这个位置开始batch_size = 10#给定一个样本标号，每次随机从里面选取1个样本返回出来for X, y in data_iter(batch_size, features, labels): print(X, '\\n', y) break3.3 线性回归的简介实现#生成数据集import numpy as npimport torchfrom torch.utils import datafrom d2l import torch as d2ltrue_w = torch.tensor([2, -3.4])true_b = 4.2features, labels = d2l.synthetic_data(true_w, true_b, 1000)#读取数据集def load_array(data_arrays, batch_size, is_train=True): #@save \"\"\"构造一个PyTorch数据迭代器。\"\"\" dataset = data.TensorDataset(*data_arrays) return data.DataLoader(dataset, batch_size, shuffle=is_train)batch_size = 10data_iter = load_array((features, labels), batch_size)next(iter(data_iter)) #将data_iter用iter()函数转为迭代器，再使用next()函数从迭代器中获取数据#定义模型# `nn` 是神经网络的缩写from torch import nnnet = nn.Sequential(nn.Linear(2, 1))#第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1#初始化模型参数net[0].weight.data.normal_(0, 0.01)net[0].bias.data.fill_(0)#定义损失函数loss = nn.MSELoss()#平方L2范数，返回所有样本损失的平均值#定义优化算法trainer = torch.optim.SGD(net.parameters(), lr=0.03)#训练num_epochs = 3for epoch in range(num_epochs): for X, y in data_iter: l = loss(net(X) ,y) #通过调用 `net(X)` 生成预测并计算损失 `l`（正向传播） trainer.zero_grad() #trainer优化器，先把梯度清零 l.backward() #反向传播计算梯度 trainer.step() # 调用优化器更新模型参数 l = loss(net(features), labels) print(f'epoch {epoch + 1}, loss {l:f}')#比较生成数据集的真实参数和通过有限数据训练获得的模型参数w = net[0].weight.dataprint('w的估计误差：', true_w - w.reshape(true_w.shape))b = net[0].bias.dataprint('b的估计误差：', true_b - b)TensorDataset对给定的tensor数据(样本和标签)，将它们包装成dataset。也就是生成数据集DataLoader数据加载器，组合数据集和采样器，并在数据集上提供单进程或多进程迭代器。它可以对我们上面所说的数据集Dataset作进一步的设置。训练步骤： 通过调用 net(X) 生成预测并计算损失 l（正向传播）。 通过进行反向传播来计算梯度。 通过调用优化器来更新模型参数。3.4 softmax回归交叉熵损失：用下式定义损失l，它是所有标签分布的预期损失值。\\(l(y,\\hat x)=-\\sum^q_{j=1}y_ilog\\hat y_j\\)3.4.7 信息论基础惊异：当我们赋予一个事件较低的概率时，我们的惊异会更大。用下式量化惊异：\\(log\\frac {1}{P(j)}=-logP(j)\\)熵：知道真实概率的人所经历的惊异程度。\\(H[P]=\\sum_j-P(j)logP(j)\\)交叉熵：交叉熵从P到Q，记为H(P,Q)，是主观概率为Q的观察者在看到根据概率P实际生成的数据时的预期惊异。当P=Q时，交叉熵达到最低。在这种情况下，从P到Q的交叉熵是H(P,P)=H(P)。3.5 图像分类数据集import torchimport torchvision #pytorch对于计算机视觉模型实现的一些库from torch.utils import datafrom torchvision import transformsfrom d2l import torch as d2ld2l.use_svg_display() #使用svg来显示图片，清晰度会高一点#读取数据集# 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式# 并除以255使得所有像素的数值均在0到1之间#下载到上一级目录的data下面#train=True，表示下载的是训练数据集#transform=trans，表示下载的要转为pytorch的tensor，而不是一堆图片#download=True 意思是默认从网上下载trans = transforms.ToTensor()mnist_train = torchvision.datasets.FashionMNIST( root=\"../data\", train=True, transform=trans, download=True)#测试数据集，验证模型的好坏mnist_test = torchvision.datasets.FashionMNIST( root=\"../data\", train=False, transform=trans, download=True)len(mnist_train), len(mnist_test) #下载成功后输出发现，训练数据集有60000张图片，测试数据集有10000张图片 #输出结果 (60000, 10000)# fashionmnist数据集同时包含图形和标签，第二个零表示取图片，如果是【1】则是标签 mnist_train[0][0].shape#第一个零：就是第零个样例#第二个零：表示取图片，如果是1就表示标签 #输出1表示是黑白图片#28，28分表表示长和宽&lt;br&gt;&lt;br&gt;#输出结果#以下函数用于在数字标签索引及其文本名称之间进行转换def get_fashion_mnist_labels(labels): \"\"\"返回Fashion-MNIST数据集的文本标签。\"\"\" text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'] return [text_labels[int(i)] for i in labels] #创建一个函数来可视化这些样本def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #@save \"\"\"Plot a list of images.\"\"\" figsize = (num_cols * scale, num_rows * scale) _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize) axes = axes.flatten() #将axes由n*m的Axes组展平成1*nm的Axes组 #在for循环里enumerate()函数是一个枚举函数，用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标。 for i, (ax, img) in enumerate(zip(axes, imgs)): if torch.is_tensor(img): # 图片张量 ax.imshow(img.numpy()) else: # PIL图片 ax.imshow(img) ax.axes.get_xaxis().set_visible(False) ax.axes.get_yaxis().set_visible(False) if titles: ax.set_title(titles[i]) return axes #构造了pytorch数据集之后放在DateLoader中，指定一个batch_size#next()拿到第一个小批量。batch_size：批量大小X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))#2行9列show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));#y是一个数值的标号#读取小批量batch_size = 256def get_dataloader_workers(): #@save \"\"\"使用4个进程来读取数据。\"\"\" return 4#shuffle=True表明需要随机，打乱顺序#训练集需要打乱顺序，测试集不用打乱顺序#num_workers表明要给多少进程train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers())#读取训练数据所需的时间 timer = d2l.Timer()for X, y in train_iter: continuef'{timer.stop():.2f} sec'#整合所有组件def load_data_fashion_mnist(batch_size, resize=None): #@save \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中。\"\"\" trans = [transforms.ToTensor()] if resize: #resize调整图像大小 trans.insert(0, transforms.Resize(resize)) trans = transforms.Compose(trans) #下载数据集 mnist_train = torchvision.datasets.FashionMNIST( root=\"../data\", train=True, transform=trans, download=True) mnist_test = torchvision.datasets.FashionMNIST( root=\"../data\", train=False, transform=trans, download=True) #返回小批量 return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()), data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers()))#通过指定resize参数来测试load_data_fashion_mnist函数的图像大小调整功能#32表示为batch_size的大小，resize表示重新调整的图片大小train_iter, test_iter = load_data_fashion_mnist(32, resize=64)for X, y in train_iter: print(X.shape, X.dtype, y.shape, y.dtype) break #输出结果 torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int643.6 softmax回归的从零开始实现import torchfrom IPython import displayfrom d2l import torch as d2lbatch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)3.6.1 初始化模型参数num_inputs = 784 # 样本的长和宽都是28，将其展平为空间向量，长度变为784num_outputs = 10 # 数据集类别，也是输出数#size=(num_inputs, num_outputs)；行数等于输入的个数，列数等于输出的个数#requires_grad=True表明要计算梯度#权重#在这里W被定义为一个784*10的矩阵W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)#偏离b = torch.zeros(num_outputs, requires_grad=True)3.6.2 定义softmax操作softmax由三个步骤组成：（1）对每个项求幂（使用exp）；（2）对每一行求和（小批量中每个样本是一行），得到每个样本的归一化常数；（3）将每一行除以其归一化常数，确保结果的和为1。 \\(softmax(X)_{ij}=\\frac {exp(X_{ij})}{\\sum_kexp(X_{ik})}\\)# 定义softmax()函数 def softmax(X): #torch.exp()对每个元素做指数计算 X_exp = torch.exp(X) #对矩阵的每一行求和，重新生成新的矩阵 partition = X_exp.sum(1, keepdim=True) # X_exp / partition：每一行代表一个样本，行中的每个数据代表在该类别的概率 return X_exp / partition # 这里应用了广播机制3.6.3 定义模型#在将数据传递到我们的模型之前，我们使用reshape函数将每张原始图像展平为向量。def net(X): return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)3.6.4 定义损失函数下面，我们创建一个数据y_hat，其中包含2个样本在3个类别的预测概率，它们对应的标签y。 有了y，我们知道在第一个样本中，第一类是正确的预测，而在第二个样本中，第三类是正确的预测。 然后使用y作为y_hat中概率的索引，我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。y = torch.tensor([0, 2]) # y_hat为预测值，此次是对两个样本做预测值y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) # 对第0个样本，拿出y0的数据=0.1；对第1个样本，y的值为2，对应y_hat的第三类0.5# 第一个参数[0,1]表示样本号，第二个参数y表示在第一个参数确定的样本中取数的序号y_hat[[0, 1], y] #输出结果 tensor([0.1000, 0.5000])#实现交叉熵损失函数def cross_entropy(y_hat, y): return - torch.log(y_hat[range(len(y_hat)), y]) #对于每一行range(len(y_hat))：生成一个从零开始一直到len(y_hat)的向量，参考上面的y_hat[[0,1],y]cross_entropy(y_hat, y)3.6.5 分类准确率def accuracy(y_hat, y): \"\"\"计算预测正确的数量。\"\"\" # len是查看矩阵的行数 # y_hat.shape[1]就是去列数，y_hat if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1: y_hat = y_hat.argmax(axis=1) #用argmax获得每行中最大元素的索引来获得预测类别 #将y_hat转换为y的数据类型然后作比较 #使用cmp函数存储bool类型 cmp = y_hat.type(y.dtype) == y #将cmp转化为y的数据类型再求和——得到找出来预测正确的类别数 return float(cmp.type(y.dtype).sum())accuracy(y_hat, y) / len(y)#输出结果 0.5# 评估任意模型的准确率 def evaluate_accuracy(net, data_iter): #@save \"\"\"计算在指定数据集上模型的精度。\"\"\" # isinstance()：判断一个对象是否是一个已知的类型 # 判断输入的net模型是否是torch.nn.Module类型 if isinstance(net, torch.nn.Module): net.eval() # 将模型设置为评估模式（不用计算梯度） metric = Accumulator(2) # 存储正确预测数、预测总数 # 每次从迭代器中拿出一个x和y for X, y in data_iter: # 1、net(X)：X放在net模型中进行softmax操作 # 2、accuracy(net(X), y)：再计算所有预算正确的样本数 # numel()函数：返回数组中元素的个数，在此可以求得样本数 metric.add(accuracy(net(X), y), y.numel()) #metric[0]:分类正确的样本数，metric[1]:总的样本数 return metric[0] / metric[1] evaluate_accuracy(net, test_iter) #输出结果0.1001class Accumulator: #@save \"\"\"在`n`个变量上累加。\"\"\" def __init__(self, n): self.data = [0.0] * n def add(self, *args): self.data = [a + float(b) for a, b in zip(self.data, args)] def reset(self): self.data = [0.0] * len(self.data) def __getitem__(self, idx): return self.data[idx]3.6.6 训练def train_epoch_ch3(net, train_iter, loss, updater): \"\"\"训练模型一个迭代周期（定义见第3章）。\"\"\"&lt;br&gt; # 将模型设置为训练模式 if isinstance(net, torch.nn.Module): net.train()#告诉pytorch我要计算梯度 # 训练损失总和、训练准确度总和、样本数 # Accumulator 是一个实用程序类，用于对多个变量进行累加 #在此创建了一个长度为三的迭代器，用于累加信息 metric = Accumulator(3) for X, y in train_iter: # 计算梯度并更新参数 y_hat = net(X) l = loss(y_hat, y) if isinstance(updater, torch.optim.Optimizer): # 使用PyTorch内置的优化器和损失函数 updater.zero_grad()#先把梯度设置为零 l.backward() #计算梯度 updater.step()#自更新 metric.add( float(l) * len(y), accuracy(y_hat, y), y.size().numel()) else: # 使用定制的优化器和损失函数 # 如果是自我实现的话，l出来就是向量，我们先做求和，再求梯度 l.sum().backward() updater(X.shape[0]) metric.add(float(l.sum()), accuracy(y_hat, y), y.numel()) # 返回训练损失和训练准确率 # metric[0]就是损失样本数目；metric[1]是训练正确的样本数；metric[2]是总的样本数 return metric[0] / metric[2], metric[1] / metric[2]定义一个在动画中绘制数据的实用程序类。（不懂）class Animator: #@save \"\"\"在动画中绘制数据。\"\"\" def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, figsize=(3.5, 2.5)): # 增量地绘制多条线 if legend is None: legend = [] d2l.use_svg_display() self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize) if nrows * ncols == 1: self.axes = [self.axes, ] # 使用lambda函数捕获参数 self.config_axes = lambda: d2l.set_axes( self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend) self.X, self.Y, self.fmts = None, None, fmts def add(self, x, y): # 向图表中添加多个数据点 if not hasattr(y, \"__len__\"): y = [y] n = len(y) if not hasattr(x, \"__len__\"): x = [x] * n if not self.X: self.X = [[] for _ in range(n)] if not self.Y: self.Y = [[] for _ in range(n)] for i, (a, b) in enumerate(zip(x, y)): if a is not None and b is not None: self.X[i].append(a) self.Y[i].append(b) self.axes[0].cla() for x, y, fmt in zip(self.X, self.Y, self.fmts): self.axes[0].plot(x, y, fmt) self.config_axes() display.display(self.fig) display.clear_output(wait=True)def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): #@save \"\"\"训练模型（定义见第3章）。\"\"\" animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9], legend=['train loss', 'train acc', 'test acc']) # num_epochs：训练次数 for epoch in range(num_epochs): #train_epoch_ch3：训练模型，返回准确度和错误度 train_metrics = train_epoch_ch3(net, train_iter, loss, updater) #在测试数据集上评估精度 test_acc = evaluate_accuracy(net, test_iter) animator.add(epoch + 1, train_metrics + (test_acc,)) train_loss, train_acc = train_metrics assert train_loss &lt; 0.5, train_loss assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc#使用小批量随机梯度下降来优化模型的损失函数lr = 0.1 #学习率def updater(batch_size): return d2l.sgd([W, b], lr, batch_size)#迭代周期设为10num_epochs = 10train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)3.6.7 预测def predict_ch3(net, test_iter, n=6): #@save \"\"\"预测标签（定义见第3章）。\"\"\" for X, y in test_iter: break #真实标号 trues = d2l.get_fashion_mnist_labels(y) #预测标号 preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1)) titles = [true + '\\n' + pred for true, pred in zip(trues, preds)] d2l.show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n]) predict_ch3(net, test_iter)3.7 softmax回归的简洁实现import torchfrom torch import nnfrom d2l import torch as d2lbatch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)3.7.1 初始化模型参数# PyTorch不会隐式地调整输入的形状。因此，我们在线性层前定义了展平层（flatten），来调整网络输入的形状# Flatten()将任何维度的tensor转化为2D的tensor# Linear(784, 10)定义线性层，输入是784，输出是10# nn.Sequential:一个时序容器。Modules 会以他们传入的顺序被添加到容器中net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10)) def init_weights(m): if type(m) == nn.Linear: # normal_正态分布 nn.init.normal_(m.weight, std=0.01) # apply()用途：当一个函数的参数存在于一个元组或者一个字典中时，用来间接的调用这个函数，并将元组或者字典中的参数按照顺序传递给参数# 意思也就是在每层跑一下该函数net.apply(init_weights);#在交叉熵损失函数中传递未归一化的预测，并同时计算softmax及其对数loss = nn.CrossEntropyLoss()3.7.3 优化算法trainer = torch.optim.SGD(net.parameters(), lr=0.1)3.7.4 训练#调用3.6定义的训练函数来训练模型num_epochs = 10d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)第4章 多层感知机4.1 多层感知机多层感知机(MLP)：也称为人工神经网络(ANN)，输入层与输出层之间含有隐藏层。从线性到非线性：在仿射变换之后，对每个隐藏层单元应用非线性的激活函数σ。\\(H=\\sigma(XW^{(1)}+b^{(1)})\\)\\[O=HW^{(2)}+b^{(2)}\\]ReLU函数：\\(ReLU(X)=max(x,0)\\)x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)y = torch.relu(x)d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))#求导#当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1y.backward(torch.ones_like(x), retain_graph=True) #torch.ones_like(x),生成x形状的元素都为1的向量d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))sigmoid函数：\\(sigmoid(x)=\\frac 1{1+exp(-x)}\\)y = torch.sigmoid(x)sigmoid导数计算公式：\\(\\frac d{dx}sigmoid(x)=sigmoid(x)(1-sigmoid(x))\\)#计算导数时，先清空之前的梯度x.grad.data.zero_()tanh函数：\\(tanh(x)=\\frac {1-exp(-2x)}{1+exp(-2x)}\\)tanh导数计算公式：\\(\\frac d{dx}tanh(x)=1-tanh^2(x)\\)4.2 多层感知机的从零开始实现import torchfrom torch import nnfrom d2l import torch as d2lbatch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)4.2.1 初始化模型参数通常，我们选择2的若干次幂作为层的宽度。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。# num_inputs：输入；num_outputs：输出；输入与输出是由数据决定的#num_hiddens：人为决定，隐藏层的大小num_inputs, num_outputs, num_hiddens = 784, 10, 256 # nn.Parameter()函数的目的就是让该变量在学习的过程中不断的修改其值以达到最优化。# nn.Parameter()参考：https://www.jianshu.com/p/d8b77cc02410# torch.randn()：返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数 W1 = nn.Parameter( torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)#*0.01意思是方差变为0.01b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))W2 = nn.Parameter( torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))params = [W1, b1, W2, b2]4.2.2 激活函数def relu(X): a = torch.zeros_like(X) return torch.max(X, a)4.2.3 模型def net(X): X = X.reshape((-1, num_inputs)) #将二维转化为num_inputs H = relu(X@W1 + b1) # 这里“@”代表矩阵乘法 return (H@W2 + b2)4.2.4 损失函数loss = nn.CrossEntropyLoss()4.2.5 训练多层感知机的训练函数与softmax训练函数一致# torch.optim.SGD：实现随机梯度下降，params: 待优化参数的iterable或者是定义了参数组的dictnum_epochs, lr = 10, 0.1updater = torch.optim.SGD(params, lr=lr)d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)# 对学习的模型进行评估d2l.predict_ch3(net, test_iter)4.3 多层感知机的简洁实现import torchfrom torch import nnfrom d2l import torch as d2l#模型# 因为图片是一个3D的东西，然后使用nn.Flatten()为二维# nn.Linear(784, 256)线性层，输入为784，输出为256# nn.Linear(256, 10)线性层，输入为256，输出为10net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: #从给定均值和标准差的正态分布(mean, std)中生成值，填充输入的张量或变量 nn.init.normal_(m.weight, std=0.01) # net.apply：会先遍历子线性层，再遍历父线性层net.apply(init_weights);#训练过程# num_epochs：表示跑多少轮batch_size, lr, num_epochs = 256, 0.1, 10loss = nn.CrossEntropyLoss()# 损失函数# 更新数据trainer = torch.optim.SGD(net.parameters(), lr=lr)# 下载测试数据集和训练数据集train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)nn.init.normal_# 直接调用d2l包的train_ch3函数d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)4.4 模型选择、欠拟合和过拟合4.4.1 训练误差和泛化误差训练误差：模型在训练集的误差；泛化误差：将模型应用到无限多的数据样本时的误差。现实中只能用测试集误差来估计泛化误差。4.4.2 模型选择模型选择：评估几个候选模型后选择出最终的模型，为了确定候选模型中的最佳模型，我们通常会使用验证集。当训练数据较少时，采用K折交叉验证，即将原始训练数据分为K个不重叠的子集，每次在K-1个子集进行训练，在剩余的一个子集进行验证，执行K次模型训练和验证。最后，通过对KK次实验的结果取平均来估计训练和验证误差。4.4.3 欠拟合还是过拟合？欠拟合：训练误差和验证误差都很大，而且训练误差与测试误差很接近。过拟合：训练误差明显低于验证误差。模型复杂度对欠拟合和过拟合的影响4.4.4 多项式回归import mathimport numpy as npimport torchfrom torch import nnfrom d2l import torch as d2l给定三阶多项式来生成训练和测试数据的标签：\\(y=5+1.2x-3.4\\frac {x^2}{2!}+5.6\\frac {x^3}{3!}+\\in,where\\in \\sim N(0,0.01^2)\\)# 生成数据集max_degree = 20 # 多项式的最大阶数n_train, n_test = 100, 100 # 训练和测试数据集大小true_w = np.zeros(max_degree) # 分配大量的空间true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])# features初始化为xfeatures = np.random.normal(size=(n_train + n_test, 1))#随机生成正态分布，200行1列np.random.shuffle(features)poly_features = np.power(features, np.arange(max_degree).reshape(1, -1)) #reshape(1,-1)为1行for i in range(max_degree): poly_features[:, i] /= math.gamma(i + 1) # `gamma(n)` = (n-1)!# `labels`的维度: (`n_train` + `n_test`,)labels = np.dot(poly_features, true_w) #向量labels += np.random.normal(scale=0.1, size=labels.shape)#高斯噪声# NumPy ndarray转换为tensortrue_w, features, poly_features, labels = [torch.tensor(x, dtype= d2l.float32) for x in [true_w, features, poly_features, labels]]features[:2], poly_features[:2, :], labels[:2]#定义损失函数def evaluate_loss(net, data_iter, loss): #@save \"\"\"评估给定数据集上模型的损失。\"\"\" metric = d2l.Accumulator(2) # 损失的总和, 样本数量 for X, y in data_iter: out = net(X) y = y.reshape(out.shape) l = loss(out, y) metric.add(l.sum(), l.numel()) return metric[0] / metric[1]#定义训练损失函数def train(train_features, test_features, train_labels, test_labels, num_epochs=400): loss = nn.MSELoss()#均方损失函数，(x-y)^2 input_shape = train_features.shape[-1]#返回的是train_features的shape # 不设置偏置，因为我们已经在多项式特征中实现了它 net = nn.Sequential(nn.Linear(input_shape, 1, bias=False)) batch_size = min(10, train_labels.shape[0]) train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)), batch_size) test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)), batch_size, is_train=False) trainer = torch.optim.SGD(net.parameters(), lr=0.01) #在动画中绘制数据的实用程序类 animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log', xlim=[1, num_epochs], ylim=[1e-3, 1e2], legend=['train', 'test']) for epoch in range(num_epochs): d2l.train_epoch_ch3(net, train_iter, loss, trainer) #20轮绘制一次 if epoch == 0 or (epoch + 1) % 20 == 0: animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss), evaluate_loss(net, test_iter, loss))) print('weight:', net[0].weight.data.numpy())#三阶多项式函数拟合# 从多项式特征中选择前4个维度，即 1, x, x^2/2!, x^3/3!train(poly_features[:n_train, :4], poly_features[n_train:, :4], labels[:n_train], labels[n_train:])#线性函数拟合（欠拟合）# 从多项式特征中选择前2个维度，即 1, xtrain(poly_features[:n_train, :2], poly_features[n_train:, :2], labels[:n_train], labels[n_train:])#高阶多项式拟合（过拟合）# 从多项式特征中选取所有维度train(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:], num_epochs=1500)4.5 权重衰减4.5.1 范数与权重衰减权重衰减也称为L2正则化，目的是为了让权重衰减到更小的值，在一定程度上减少模型过拟合的问题。思考：L2正则化项有让w变小的效果，但是为什么w变小可以防止过拟合呢？原理：（1）从模型的复杂度上解释：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合更好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。（2）从数学方面的解释：过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。损失函数：\\(L(w,b)=\\frac 1n\\sum_{i=1}^n\\frac 12(w^⊤x^{(i)}+b−y^{(i)})^2+\\frac {\\lambda}{2}||w||^2\\)\\[w=(1-\\eta \\lambda) w-\\frac {\\eta}{|B|}\\sum_{i\\in B}x^{(i)}(w^⊤x^{(i)}+b-y^{(i)})\\]4.5.2 高维线性回归\\[y=0.05+\\sum ^d_{i=1}0.01x_i+\\in,where \\in \\sim N(0,0.01^2)\\]维数d=200，训练集个数为20# 生成数据import torchfrom torch import nnfrom d2l import torch as d2ln_train, n_test, num_inputs, batch_size = 20, 100, 200, 5true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05 # true_w：是一个200*1的矩阵，矩阵内容全为0.01# synthetic_data：合成数据集# train_data：是一个包含20个样本的训练数据集train_data = d2l.synthetic_data(true_w, true_b, n_train)train_iter = d2l.load_array(train_data, batch_size)test_data = d2l.synthetic_data(true_w, true_b, n_test)test_iter = d2l.load_array(test_data, batch_size, is_train=False)4.5.3 从零开始实现# 初始化模型参数def init_params(): w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True) b = torch.zeros(1, requires_grad=True) return [w, b]# 定义L2范数惩罚def l2_penalty(w): return torch.sum(w.pow(2)) / 2# 训练代码实现def train(lambd): w, b = init_params() # linreg：线性回归 # squared_loss：平方损失函数 # lambda函数也称为匿名函数（在这里相当于定义模型） https://www.cnblogs.com/curo0119/p/8952536.html net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss # num_epochs：表示迭代次数；lr=0.003：表示学习率 num_epochs, lr = 100, 0.003 animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test']) for epoch in range(num_epochs): for X, y in train_iter: with torch.enable_grad():#类似requires_grad,https://www.jianshu.com/p/1cea017f5d11 # 增加了L2范数惩罚项，广播机制使l2_penalty(w)成为一个长度为`batch_size`的向量。 l = loss(net(X), y) + lambd * l2_penalty(w) l.sum().backward() d2l.sgd([w, b], lr, batch_size) if (epoch + 1) % 5 == 0: animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss))) print('w的L2范数是：', torch.norm(w).item()) #torch.norm()求范数,.item()是输出tensor的元素# 忽略正则化训练train(lambd=0)#输出w的L2范数是： 14.599337577819824# 使用正则化train(lambd=3)#输出w的L2范数是： 0.36418250203132634.5.4 简洁实现'''在实例化优化器时直接通过weight_decay指定weight decay超参数。默认情况下，PyTorch同时衰减权重和偏移。这里我们只为权重设置了weight_decay，所以bias参数 b 不会衰减。'''def train_concise(wd): net = nn.Sequential(nn.Linear(num_inputs, 1)) for param in net.parameters(): param.data.normal_() # L2范数 loss = nn.MSELoss() num_epochs, lr = 100, 0.003 # 偏置参数没有衰减。 trainer = torch.optim.SGD([ {\"params\":net[0].weight,'weight_decay': wd}, {\"params\":net[0].bias}], lr=lr) animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log', xlim=[5, num_epochs], legend=['train', 'test']) for epoch in range(num_epochs): for X, y in train_iter: with torch.enable_grad(): trainer.zero_grad() l = loss(net(X), y) l.backward() trainer.step() if (epoch + 1) % 5 == 0: animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss))) print('w的L2范数：', net[0].weight.norm().item())4.6 DropoutDropout：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。可以解决过拟合问题。https://blog.csdn.net/program_developer/article/details/807377244.6.4 从零开始实现要实现单层的dropout函数，我们必须从伯努利（二元）随机变量中提取与我们的层的维度一样多的样本，其中随机变量以概率1−p取值1（保持），以概率p取值0（丢弃）。实现这一点的一种简单方式是首先从均匀分布U[0,1]中抽取样本。那么我们可以保留那些对应样本大于p的节点，把剩下的丢弃。import torchfrom torch import nnfrom d2l import torch as d2l#实现单层的dropout函数def dropout_layer(X, dropout): # assert：断言。表示程序只有在符合以下条件下才能正常运行 assert 0 &lt;= dropout &lt;= 1 # 在本情况中，所有元素都被丢弃。 if dropout == 1: return torch.zeros_like(X) # 在本情况中，所有元素都被保留。 # torch.randn()随机生成了一个和X.shape相同的mask，均值为“0”，方差为“1” # 且大于dropout的地方设置为1，其他地方设置为0 if dropout == 0: return X # mask * X / (1.0 - dropout)没有丢弃的输入部分的值会因为表达式的分母存在而改变，而训练数据的标签还是原来的值 # mask * X 不是矩阵的乘法，而是哈达马积，若A=(aij)和B=(bij)是两个同阶矩阵，若cij=aij×bij,则称矩阵C=(cij)为A和B的哈达玛积 mask = (torch.Tensor(X.shape).uniform_(0, 1) &gt; dropout).float() return mask * X / (1.0 - dropout)# 定义模型参数# 定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256# 我们可以分别为每一层设置丢弃概率。 一种常见的技巧是在靠近输入层的地方设置较低的丢弃概率。dropout1, dropout2 = 0.2, 0.5class Net(nn.Module): # self是实例化的对象， # super(Net, self).__init__()：子类把父类的__init__()放到自己的__init__()当中，这样子类就有了父类的__init__()的 # 那些东西 def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training = True): super(Net, self).__init__() self.num_inputs = num_inputs self.training = is_training self.lin1 = nn.Linear(num_inputs, num_hiddens1) self.lin2 = nn.Linear(num_hiddens1, num_hiddens2) self.lin3 = nn.Linear(num_hiddens2, num_outputs) self.relu = nn.ReLU() def forward(self, X): # H1：第一个隐藏层的输出 H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs)))) # 只有在训练模型时才使用dropout if self.training == True: # 在第一个全连接层之后添加一个dropout层 H1 = dropout_layer(H1, dropout1) # 把H1作为输入进第二个隐藏层 H2 = self.relu(self.lin2(H1)) if self.training == True: # 在第二个全连接层之后添加一个dropout层 H2 = dropout_layer(H2, dropout2) # 把 H2 作为输入传递给输出层 out = self.lin3(H2) return outnet = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)# 训练和测试num_epochs, lr, batch_size = 10, 0.5, 256loss = nn.CrossEntropyLoss()train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)trainer = torch.optim.SGD(net.parameters(), lr=lr)d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)4.7 正向传播、反向传播和计算图正向传播：按顺序计算和储存神经网络中每层的结果。反向传播：计算梯度的方法。按相反的顺序从输出层到输入层遍历网络。4.8 数值稳定性和模型初始化4.8.1 梯度消失和梯度爆炸梯度消失：参数更新过小，在每次更新时几乎不会移动，导致无法学习。如sigmoid激活函数，当输入过大或过小时，梯度接近于0。采用ReLU函数缓解梯度消失问题，加速收敛。梯度爆炸：参数更新过大，破坏了模型的稳定收敛。4.8.2 参数初始化Xavier初始化：\\(\\frac 12(n_{in}+n_{out})\\sigma^2=1\\)\\[n_{in}输入层的数量，n_{out}输出层的数量\\]4.9 环境和分布偏移4.9.1 分布偏移的类型当数据分布发生变化时，要求算法实时更新，动态调整。协变量偏移：输入的分布改变，但标签函数不变。如训练集是真实的猫狗图像，测试集却是卡通图像。 标签偏移：标签边缘概率P(y)改变，但类别条件分布P(y x)在不同的领域之间保持不变。 概念偏移：类别条件分布P(y x)发生变化，比如机器翻译系统在不同地区翻译的语言不同。 4.9.3 分布偏移纠正协变量偏移：logistic回归。概念偏移：用新数据执行更新步骤。4.10 实战Kaggle比赛：预测房价Python split() 通过指定分隔符对字符串进行切片，如果参数 num 有指定值，则分隔 num+1 个子字符串。str.split(str=\"\", num=string.count(str))f.read(1048576) # 每次读取1048576字节，即1MBnumeric_features = all_features.dtypes[all_features.dtypes != 'object'].index #提取出所有数字的列all_features = pd.get_dummies(all_features, dummy_na=True) #pandas实现one-hot编码torch.clamp(input, min, max, out=None)# 将输入input张量每个元素的夹紧到区间 [min,max][min,max]，并返回结果到一个新张量。5. 深度学习计算5.1 层和块super().__init__() # 调用父类的__init__函数# 混合搭配组合块class NestMLP(nn.Module): def __init__(self): super().__init__() self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU()) self.linear = nn.Linear(32, 16) def forward(self, X): return self.linear(self.net(X))chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())chimera(X)5.2 参数管理# 内置初始化# 将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0。def init_normal(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, mean=0, std=0.01) nn.init.zeros_(m.bias)net.apply(init_normal)net[0].weight.data[0], net[0].bias.data[0]# 将所有参数初始化为给定的常数（比如1）def init_constant(m): if type(m) == nn.Linear: nn.init.constant_(m.weight, 1) nn.init.zeros_(m.bias)net.apply(init_constant)net[0].weight.data[0], net[0].bias.data[0]# 自定义初始化def my_init(m): if type(m) == nn.Linear: print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()][0]) nn.init.uniform_(m.weight, -10, 10) m.weight.data *= m.weight.data.abs() &gt;= 5 # 将绝对值&gt;=5的数保留，其余置为0net.apply(my_init)net[0].weight[:2]5.4 自定义层5.4.1 不带参数的层class CenteredLayer(nn.Module): def __init__(self): super().__init__() def forward(self, X): return X - X.mean()5.4.2 带参数的层class MyLinear(nn.Module): def __init__(self, in_units, units): super().__init__() self.weight = nn.Parameter(torch.randn(in_units, units)) #初始化 self.bias = nn.Parameter(torch.randn(units,)) def forward(self, X): linear = torch.matmul(X, self.weight.data) + self.bias.data return F.relu(linear)5.5 读写文件# 保存张量x = torch.arange(4)torch.save(x, 'x-file')x2 = torch.load('x-file') # 加载张量# 保存张量列表y = torch.zeros(4)torch.save([x, y],'x-files')# 保存字典mydict = {'x': x, 'y': y}torch.save(mydict, 'mydict')# 加载和保存模型参数# 创建多层感知机class MLP(nn.Module): def __init__(self): super().__init__() self.hidden = nn.Linear(20, 256) self.output = nn.Linear(256, 10) def forward(self, x): return self.output(F.relu(self.hidden(x)))net = MLP()X = torch.randn(size=(2, 20))Y = net(X)# 保存模型参数torch.save(net.state_dict(), 'mlp.params')# 读取模型参数clone = MLP()clone.load_state_dict(torch.load('mlp.params'))clone.eval()5.6 GPU!nvidia-smi # 查看GPUtorch.cuda.device('cuda:{i}') # 表示第i块GPU，(i从0开始)torch.cuda.device_count() # 查看GPU数量张量默认在CPU上创建，在GPU上存储张量X = torch.ones(2, 3, device=try_gpu())Y = torch.rand(2, 3, device=try_gpu(1))# 复制Z = X.cuda(1) # 将X复制到cuda1print(X)print(Z)# 将神经网络模型放在GPU上net = nn.Sequential(nn.Linear(3, 1))net = net.to(device=try_gpu())6. 卷积神经网络6.1 从全连接层到卷积平移不变性：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。局部性：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，在后续神经网络，整个图像级别上可以集成这些局部特征用于预测。卷积：\\((f*g)(X)=\\int f(Z)g(X-Z)dZ\\)6.2 图像卷积6.2.1 互相关运算互相关运算：也就是卷积层卷积层输出大小：\\((n_h-k_h+1)×(n_w-k_w+1)\\)import torchfrom torch import nnfrom d2l import torch as d2ldef corr2d(X, K): #@save \"\"\"计算二维互相关运算。\"\"\" h, w = K.shape Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = (X[i:i + h, j:j + w] * K).sum() return Y6.2.2 卷积层class Conv2D(nn.Module): def __init__(self, kernel_size): super().__init__() self.weight = nn.Parameter(torch.rand(kernel_size)) self.bias = nn.Parameter(torch.zeros(1)) def forward(self, x): return corr2d(x, self.weight) + self.bias6.2.4 学习卷积核我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较 Y 与卷积层输出的平方误差，然后计算梯度来更新卷积核。# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），# 其中批量大小和通道数都为1X = X.reshape((1, 1, 6, 8))Y = Y.reshape((1, 1, 6, 7))for i in range(10): Y_hat = conv2d(X) l = (Y_hat - Y) ** 2 conv2d.zero_grad() l.sum().backward() # 迭代卷积核 conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad ## 3e-2是学习率 if (i + 1) % 2 == 0: print(f'batch {i+1}, loss {l.sum():.3f}')6.2.6 特征映射和感受野特征映射：输出的卷积层。感受野：卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点对应输入图上的区域。6.3 填充和步幅6.3.1 填充在应用多层卷积时，我们常常丢失边缘像素。解决这个问题的简单方法即为填充（padding），在输入图像的边界填充元素（通常填充元素是 0）。添加ph行填充（一半在顶部，一半在底部），pw列填充（一半在左侧，一半在右侧），则输出形状为\\((n_h-k_h+p_h+1)×(n_w-k_w+p_w+1)\\)卷积核的高度和宽度通常为奇数，例如1，3，5，7。选择奇数的好处是，保持空间维度的同时，可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。当卷积内核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。在如下示例中，我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))comp_conv2d(conv2d, X).shape6.3.2 步幅步幅：每次滑动元素的数量。当垂直步幅为sh、水平步幅为sw时，输出的形状为：\\([(n_h-k_h+p_h+s_h)/s_h]×[(n_w-k_w+p_w+s_w)/s_w]\\)6.4 多输入多输出通道6.4.1 多输入通道假设输入通道数为Ci，则卷积核的通道数也为Ci，对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和，得到输出的二维张量。6.4.2 多输出通道假设输入通道数为Ci，输出通道数为C0，则卷积核的形状为C0×Ci×kh×kw。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。6.4.3 1×1卷积用来升/降维，可以看成全连接层。6.5 汇聚层（池化层）最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。池化层：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。6.5.1 最大汇聚成和平均汇聚层与卷积层类似，固定窗口滑动，通常计算池化窗口中所有元素的最大值或平均值，称为最大汇聚层(maximun pooling)和平均汇聚层(average pooling)。6.5.3 多个通道汇聚层在每个输入通道单独运算，而不是像卷积层一样在通道上对输入进行汇总。因此，汇聚层的输出通道数和输入通道数相同。6.6 卷积神经网络(LeNet)6.6.1 LeNetLeNet：由两个卷积块，三个全连接层组成。每个卷积块包含一个卷积层、一个sigmoid激活函数和平均池化层。import torchfrom torch import nnfrom d2l import torch as d2lclass Reshape(torch.nn.Module): def forward(self, x): return x.view(-1, 1, 28, 28) # view()与reshape作用类似net = torch.nn.Sequential( Reshape(), nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), #输入1通道，输出6通道 nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(), nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10))# 打印出网络结构X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)for layer in net: X = layer(X) print(layer.__class__.__name__,'output shape: \\t',X.shape)6.6.2 模型训练使用GPU训练batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)def evaluate_accuracy_gpu(net, data_iter, device=None): #@save \"\"\"使用GPU计算模型在数据集上的精度。\"\"\" # isinstance()：判断一个对象是否是一个已知的类型\t# 判断输入的net模型是否是torch.nn.Module类型 if isinstance(net, torch.nn.Module): net.eval() # 设置为评估模式（不用计算梯度） if not device: device = next(iter(net.parameters())).device # 正确预测的数量，总预测的数量 metric = d2l.Accumulator(2) # 存储正确预测数、预测总数 for X, y in data_iter: if isinstance(X, list): # BERT微调所需的（之后将介绍） X = [x.to(device) for x in X] else: X = X.to(device) y = y.to(device) # accuracy(net(X), y)：再计算所有预算正确的样本数 # numel()函数：返回数组中元素的个数，在此可以求得样本数 metric.add(d2l.accuracy(net(X), y), y.numel()) #metric[0]:分类正确的样本数，metric[1]:总的样本数 return metric[0] / metric[1]#@savedef train_ch6(net, train_iter, test_iter, num_epochs, lr, device): \"\"\"用GPU训练模型(在第六章定义)。\"\"\" def init_weights(m): if type(m) == nn.Linear or type(m) == nn.Conv2d: nn.init.xavier_uniform_(m.weight) net.apply(init_weights) print('training on', device) net.to(device) optimizer = torch.optim.SGD(net.parameters(), lr=lr) loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], legend=['train loss', 'train acc', 'test acc']) timer, num_batches = d2l.Timer(), len(train_iter) for epoch in range(num_epochs): # 训练损失之和，训练准确率之和，范例数 metric = d2l.Accumulator(3) net.train() for i, (X, y) in enumerate(train_iter): timer.start() optimizer.zero_grad() X, y = X.to(device), y.to(device) y_hat = net(X) l = loss(y_hat, y) l.backward() optimizer.step() with torch.no_grad(): # 强制之后的内容不进行计算图构建 metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0]) timer.stop() train_l = metric[0] / metric[2] train_acc = metric[1] / metric[2] if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1: animator.add(epoch + (i + 1) / num_batches, (train_l, train_acc, None)) test_acc = evaluate_accuracy_gpu(net, test_iter) animator.add(epoch + 1, (None, None, test_acc)) print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, ' f'test acc {test_acc:.3f}') print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec ' f'on {str(device)}')lr,num_epochs = 1.5,15train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())7. 现代卷积神经网络7.1 深度卷积神经网络（AlexNet）AlexNet：由八层组成，五个卷积层，两个全连接隐藏层，一个全连接输出层。使用ReLU作为激活函数。AlexNet通过dropout控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。 这使得模型更健壮，更大的样本量有效地减少了过拟合。import torchfrom torch import nnfrom d2l import torch as d2lnet = nn.Sequential( # 这里，我们使用一个11*11的更大窗口来捕捉对象。 # 同时，步幅为4，以减少输出的高度和宽度。 # 另外，输出通道的数目远大于LeNet nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数 nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # 使用三个连续的卷积层和较小的卷积窗口。 # 除了最后的卷积层，输出通道的数量进一步增加。 # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度 nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(), # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过度拟合 nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000 nn.Linear(4096, 10))# 构造一个高度和宽度都为224的单通道数据，来观察每一层输出的形状X = torch.randn(1, 1, 224, 224)for layer in net: X=layer(X) print(layer.__class__.__name__,'Output shape:\\t',X.shape)# 读取数据集batch_size = 128train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)# 训练lr, num_epochs = 0.01, 10d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())7.2 使用块的网络7.2.1 VGG块经典CNN基本组成部分：带填充以保持分辨率的卷积层、非线性激活函数(ReLU等)、池化层。VGG块：由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。import torchfrom torch import nnfrom d2l import torch as d2l# 带有 3×3 卷积核、填充为 1（保持高度和宽度）的卷积层# 带有 2×2 池化窗口、步幅为 2（每个块后的分辨率减半）的最大汇聚层def vgg_block(num_convs, in_channels, out_channels): layers = [] for _ in range(num_convs): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) layers.append(nn.ReLU()) in_channels = out_channels layers.append(nn.MaxPool2d(kernel_size=2,stride=2)) return nn.Sequential(*layers)7.2..2 VGG网络超参数变量conv_arch：指定每个VGG块里卷积层个数和输出通道数。原始 VGG 网络有 5 个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有 64 个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到 512。由于该网络使用 8 个卷积层和 3 个全连接层，因此它通常被称为 VGG-11。# 定义超参数conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))def vgg(conv_arch): conv_blks = [] in_channels = 1 # 卷积层部分 for (num_convs, out_channels) in conv_arch: conv_blks.append(vgg_block(num_convs, in_channels, out_channels)) in_channels = out_channels return nn.Sequential( *conv_blks, nn.Flatten(), # 全连接层部分 nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 10))net = vgg(conv_arch)X = torch.randn(size=(1, 1, 224, 224))for blk in net: X = blk(X) print(blk.__class__.__name__,'output shape:\\t',X.shape)# 输出Sequential output shape: torch.Size([1, 64, 112, 112])Sequential output shape: torch.Size([1, 128, 56, 56])Sequential output shape: torch.Size([1, 256, 28, 28])Sequential output shape: torch.Size([1, 512, 14, 14])Sequential output shape: torch.Size([1, 512, 7, 7])Flatten output shape: torch.Size([1, 25088])Linear output shape: torch.Size([1, 4096])ReLU output shape: torch.Size([1, 4096])Dropout output shape: torch.Size([1, 4096])Linear output shape: torch.Size([1, 4096])ReLU output shape: torch.Size([1, 4096])Dropout output shape: torch.Size([1, 4096])Linear output shape: torch.Size([1, 10])在每个块的高度和宽度减半，最终高度和宽度都为7。最后再展平表示，送入全连接层处理。7.2.3 训练模型# 减少通道数ratio = 4small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch] # //表示取整除net = vgg(small_conv_arch)lr, num_epochs, batch_size = 0.05, 10, 128train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())7.3 网络中的网络（NiN）想要早期使用全连接层，必须使用稠密层(Flatten)展开，但会完全放弃表征的空间结构。网络中的网络 (NiN) 提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机。7.3.1 NiN块NiN 的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。 如果我们将权重连接到每个空间位置，我们可以将其视为 1×1卷积层，或作为在每个像素位置上独立作用的全连接层。 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。NiN块：以一个普通卷积层开始，后面是两个1×1 的卷积层。这两个1×1 卷积层充当带有 ReLU 激活函数的逐像素全连接层。 第一层的卷积窗口形状通常由用户设置。 随后的卷积窗口形状固定为 1×1。import torchfrom torch import nnfrom d2l import torch as d2ldef nin_block(in_channels, out_channels, kernel_size, strides, padding): return nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())7.3.2 NiN模型NiN 和 AlexNet 之间的一个显著区别是 NiN 完全取消了全连接层。 相反，NiN 使用一个 NiN块，其输出通道数等于标签类别的数量。最后放一个 全局平均汇聚层（global average pooling layer），生成一个多元逻辑向量。net = nn.Sequential( nin_block(1, 96, kernel_size=11, strides=4, padding=0), nn.MaxPool2d(3, stride=2), nin_block(96, 256, kernel_size=5, strides=1, padding=2), nn.MaxPool2d(3, stride=2), nin_block(256, 384, kernel_size=3, strides=1, padding=1), nn.MaxPool2d(3, stride=2), nn.Dropout(0.5), # 标签类别数是10 nin_block(384, 10, kernel_size=3, strides=1, padding=1), nn.AdaptiveAvgPool2d((1, 1)), # 设置输出形状，自适应设置核的大小和步长 # 将四维的输出转成二维的输出，其形状为(批量大小, 10) nn.Flatten())7.4 含并行连结的网络（GoogLeNet）7.4.1 Inception块这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道的数量。import torchfrom torch import nnfrom torch.nn import functional as Ffrom d2l import torch as d2lclass Inception(nn.Module): # `c1`--`c4` 是每条路径的输出通道数 def __init__(self, in_channels, c1, c2, c3, c4, **kwargs): super(Inception, self).__init__(**kwargs) # 线路1，单1 x 1卷积层 self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1) # 线路2，1 x 1卷积层后接3 x 3卷积层 self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1) # 线路3，1 x 1卷积层后接5 x 5卷积层 self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2) # 线路4，3 x 3最大汇聚层后接1 x 1卷积层 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1) def forward(self, x): p1 = F.relu(self.p1_1(x)) p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) p3 = F.relu(self.p3_2(F.relu(self.p3_1(x)))) p4 = F.relu(self.p4_2(self.p4_1(x))) # dim=1 在通道维度上连结输出 return torch.cat((p1, p2, p3, p4), dim=1)7.4.2 GoogLeNet模型# 第一个模块使用 64 个通道、 7×7 卷积层b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))# 第二个模块使用两个卷积层：第一个卷积层是 64个通道、 1×1 卷积层；第二个卷积层使用将通道数量增加三倍的 3×3 卷积层。b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(), nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))# 第三个模块串联两个完整的Inception块b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32), Inception(256, 128, (128, 192), (32, 96), 64), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))# 第四个模块串联五个Inception块b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64), Inception(512, 160, (112, 224), (24, 64), 64), Inception(512, 128, (128, 256), (24, 64), 64), Inception(512, 112, (144, 288), (32, 64), 64), Inception(528, 256, (160, 320), (32, 128), 128), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))# 第五个模块包含两个Inception块，全局平均汇聚层b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128), Inception(832, 384, (192, 384), (48, 128), 128), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))7.5 批量归一化批量归一化：加速深层网络的收敛速度。7.5.1 训练神经网络为什么要批量归一化？答：将参数的量级进行统一；越深层的网络越容易过拟合，因此正则化很重要。7.5.2 批量归一化层对于全连接层：将批量归一化层置于全连接层中的仿射变换和激活函数之间。对于卷积层：在卷积层之后和非线性激活函数之前应用批量归一化。7.5.3 从零实现import torchfrom torch import nnfrom d2l import torch as d2l# 拉伸gamma，偏移betadef batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum): # 通过 `is_grad_enabled` 来判断当前模式是训练模式还是预测模式 if not torch.is_grad_enabled(): # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差 X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps) else: assert len(X.shape) in (2, 4) if len(X.shape) == 2: # 使用全连接层的情况，计算特征维上的均值和方差 mean = X.mean(dim=0) var = ((X - mean) ** 2).mean(dim=0) else: # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。 # 这里我们需要保持X的形状以便后面可以做广播运算 mean = X.mean(dim=(0, 2, 3), keepdim=True) var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True) # 训练模式下，用当前的均值和方差做标准化 X_hat = (X - mean) / torch.sqrt(var + eps) # 更新移动平均的均值和方差 moving_mean = momentum * moving_mean + (1.0 - momentum) * mean moving_var = momentum * moving_var + (1.0 - momentum) * var Y = gamma * X_hat + beta # 缩放和移位 return Y, moving_mean.data, moving_var.dataclass BatchNorm(nn.Module): # `num_features`：完全连接层的输出数量或卷积层的输出通道数。 # `num_dims`：2表示完全连接层，4表示卷积层 def __init__(self, num_features, num_dims): super().__init__() if num_dims == 2: shape = (1, num_features) else: shape = (1, num_features, 1, 1) # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0 self.gamma = nn.Parameter(torch.ones(shape)) self.beta = nn.Parameter(torch.zeros(shape)) # 非模型参数的变量初始化为0和1 self.moving_mean = torch.zeros(shape) self.moving_var = torch.ones(shape) def forward(self, X): # 如果 `X` 不在内存上，将 `moving_mean` 和 `moving_var` # 复制到 `X` 所在显存上 if self.moving_mean.device != X.device: self.moving_mean = self.moving_mean.to(X.device) self.moving_var = self.moving_var.to(X.device) # 保存更新过的 `moving_mean` 和 `moving_var` Y, self.moving_mean, self.moving_var = batch_norm( X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9) return Ynet = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(), nn.Linear(84, 10))lr, num_epochs, batch_size = 1.0, 10, 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())# 查看从第一个批量归一化层中学到的拉伸参数 gamma 和偏移参数 betanet[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))7.6 残差网络（ResNet）残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。7.6.2 残差块拟合出残差映射f(x)-x，残差映射更容易优化。ResNet 沿用了 VGG 完整的 3×3卷积层设计。 残差块里首先有 2 个有相同输出通道数的 3×3卷积层。 每个卷积层后接一个批量归一化层和 ReLU 激活函数。 然后我们通过跨层数据通路，跳过这 2 个卷积运算，将输入直接加在最后的 ReLU 激活函数前。 这样的设计要求 2 个卷积层的输出与输入形状一样，从而可以相加。 如果想改变通道数，就需要引入一个额外的 1×1 卷积层来将输入变换成需要的形状后再做相加运算。import torchfrom torch import nnfrom torch.nn import functional as Ffrom d2l import torch as d2lclass Residual(nn.Module): #@save def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1): super().__init__() self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides) self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm2d(num_channels) self.bn2 = nn.BatchNorm2d(num_channels) self.relu = nn.ReLU(inplace=True) # inplace=True:从上层网络Conv2d中传递下来的tensor直接进行修改，这样能够节省运算内存，不用多存储其他变量 def forward(self, X): Y = F.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) Y += X return F.relu(Y)7.6.3 ResNet模型ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样： 在输出通道数为 64、步幅为 2 的 7×77×7 卷积层后，接步幅为 2 的 3×33×3 的最大汇聚层。 不同之处在于 ResNet 每个卷积层后增加了批量归一化层。GoogLeNet 在后面接了 4 个由Inception块组成的模块。 ResNet 则使用 4 个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为 2 的最大汇聚层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。每个模块有 4 个卷积层（不包括恒等映射的 1×11×1 卷积层）。 加上第一个 7×77×7 卷积层和最后一个全连接层，共有 18 层。 因此，这种模型通常被称为 ResNet-18。def resnet_block(input_channels, num_channels, num_residuals, first_block=False): blk = [] for i in range(num_residuals): if i == 0 and not first_block: blk.append(Residual(input_channels, num_channels, use_1x1conv=True, strides=2)) else: blk.append(Residual(num_channels, num_channels)) return blkb1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))b3 = nn.Sequential(*resnet_block(64, 128, 2))b4 = nn.Sequential(*resnet_block(128, 256, 2))b5 = nn.Sequential(*resnet_block(256, 512, 2))net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512, 10))7.7 稠密连接网络（DenseNet）稠密连接网络在某种程度上是 ResNet 的逻辑扩展，ResNet 和 DenseNet 的关键区别在于，DenseNet 输出是连接，而不是如 ResNet 的简单相加。稠密网络主要由 2 部分构成： 稠密块（dense block）和 过渡层 （transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。7.7.2 稠密块体import torchfrom torch import nnfrom d2l import torch as d2ldef conv_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))class DenseBlock(nn.Module): def __init__(self, num_convs, input_channels, num_channels): super(DenseBlock, self).__init__() layer = [] for i in range(num_convs): layer.append(conv_block( num_channels * i + input_channels, num_channels)) self.net = nn.Sequential(*layer) def forward(self, X): for blk in self.net: Y = blk(X) # 连接通道维度上每个块的输入和输出 X = torch.cat((X, Y), dim=1) return X7.7.3 过渡层由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过 1×11×1 卷积层来减小通道数，并使用步幅为 2 的平均汇聚层减半高和宽，从而进一步降低模型复杂度。def transition_block(input_channels, num_channels): return nn.Sequential( nn.BatchNorm2d(input_channels), nn.ReLU(), nn.Conv2d(input_channels, num_channels, kernel_size=1), nn.AvgPool2d(kernel_size=2, stride=2))7.7.4 DenseNet模型b1 = nn.Sequential( nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))# `num_channels`为当前的通道数num_channels, growth_rate = 64, 32num_convs_in_dense_blocks = [4, 4, 4, 4]blks = []for i, num_convs in enumerate(num_convs_in_dense_blocks): blks.append(DenseBlock(num_convs, num_channels, growth_rate)) # 上一个稠密块的输出通道数 num_channels += num_convs * growth_rate # 在稠密块之间添加一个转换层，使通道数量减半 if i != len(num_convs_in_dense_blocks) - 1: blks.append(transition_block(num_channels, num_channels // 2)) num_channels = num_channels // 2 net = nn.Sequential( b1, *blks, nn.BatchNorm2d(num_channels), nn.ReLU(), nn.AdaptiveMaxPool2d((1, 1)), nn.Flatten(), nn.Linear(num_channels, 10))8. 循环神经网络（RNN）卷积神经网络可以有效处理空间信息，循环神经网络适合处理序列信息，如预测股市波动等。循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。8.1 序列模型8.1.1 统计工具8.1.1.1 自回归模型自回归模型：用观测序列xt-1，…，xt-τ来预测。隐变量自回归模型：保留一些对过去观测的总结。8.2 文本预处理步骤：（1）将文本作为字符串加载到内存中；（2）将字符串拆分为词元（如单词和字符）；（3）建立一个词汇表，将拆分的词元映射到数字索引；（4）将文本转换为数字索引序列，方便模型操作。8.2.1 读取数据集import collections # collections模块包含了除list、dict、和tuple之外的容器数据类型import refrom d2l import torch as d2ld2l.DATA_HUB['time_machine'] = (d2l.DATA_URL+'timemachine.txt','090b5e7e70c295757f55df93cb0a180b9691891a')def read_time_machine(): with open(d2l.download('time_machine'),'r')as f: lines = f.readlines() #re.sub(r'[A-Za-z]', '*', s) 这句话则表示只匹配单一字母，并将每一个字母替换为一个星号 #加上^ 取反 #.strip()去除字符串首尾的空格 #.lower()将字符串中的所有大写字母转换为小写字母 return [re.sub('[^A-Za-z+]',' ',line).strip().lower()for line in lines]lines = read_time_machine()print(f'# text lines:{len(lines)}')print(lines[0])print(lines[10])8.2.2 词元化把每条文本行拆分为词元。def tokenize(lines, token='word'): #@save \"\"\"将文本行拆分为单词或字符词元。\"\"\" if token == 'word': return [line.split() for line in lines]# 通过指定分隔符对字符串进行切片 elif token == 'char': return [list(line) for line in lines] else: print('错误：未知词元类型：' + token)tokens = tokenize(lines)for i in range(11): print(tokens[i])8.2.3 词汇表构建一个字典，即词汇表，将字符串类型的词元映射到从0开始的数字序列中。class Vocab: #@save \"\"\"文本词汇表\"\"\" def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): if tokens is None: tokens = [] if reserved_tokens is None: reserved_tokens = [] # 按出现频率排序 counter = count_corpus(tokens) self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # 未知词元的索引为0 self.unk, uniq_tokens = 0, ['&lt;unk&gt;'] + reserved_tokens uniq_tokens += [token for token, freq in self.token_freqs if freq &gt;= min_freq and token not in uniq_tokens] self.idx_to_token, self.token_to_idx = [], dict() for token in uniq_tokens: self.idx_to_token.append(token) self.token_to_idx[token] = len(self.idx_to_token) - 1 def __len__(self): return len(self.idx_to_token) def __getitem__(self, tokens): if not isinstance(tokens, (list, tuple)): return self.token_to_idx.get(tokens, self.unk) return [self.__getitem__(token) for token in tokens] def to_tokens(self, indices): if not isinstance(indices, (list, tuple)): return self.idx_to_token[indices] return [self.idx_to_token[index] for index in indices]def count_corpus(tokens): \"\"\"统计词元的频率。\"\"\" # 这里的 `tokens` 是 1D 列表或 2D 列表 if len(tokens) == 0 or isinstance(tokens[0], list): # 将词元列表展平成使用词元填充的一个列表 tokens = [token for line in tokens for token in line] return collections.Counter(tokens) #collections.Counter统计词元出现的次数vocab = Vocab(tokens)print(list(vocab.token_to_idx.items())[:10])8.2.4 整合所有功能def load_corpus_time_machine(max_tokens=-1): #@save \"\"\"返回时光机器数据集的词元索引列表和词汇表。\"\"\" lines = read_time_machine() tokens = tokenize(lines, 'char') vocab = Vocab(tokens) # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落， # 所以将所有文本行展平到一个列表中 corpus = [vocab[token] for line in tokens for token in line] if max_tokens &gt; 0: corpus = corpus[:max_tokens] return corpus, vocabcorpus, vocab = load_corpus_time_machine()len(corpus), len(vocab)8.3 语言模型和数据集语言模型：目标是估计序列的联合概率。一个理想的语言模型能够基于模型本身生成自然文本。为了计算语言模型，我们需要计算单词的概率和给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。\\(P(deep,learning,is,fun) = P(deep)P(learning|deep)P(is|deep,learning)P(fun|deep,learning,is)\\)通常，涉及一个、两个和三个变量的概率公式分别被称为“一元语法”（unigram）、“二元语法”（bigram）和“三元语法”（trigram）模型。8.3.3 自然语言统计import randomimport torchfrom d2l import torch as d2ltokens = d2l.tokenize(d2l.read_time_machine())# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起corpus = [token for line in tokens for token in line]vocab = d2l.Vocab(corpus)vocab.token_freqs[:10] #打印前10个最常用的（频率最高的）单词。# 绘制词频图freqs = [freq for token, freq in vocab.token_freqs]d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)', xscale='log', yscale='log')# 查看二元语法和三元语法bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]bigram_vocab = d2l.Vocab(bigram_tokens)bigram_vocab.token_freqs[:10]trigram_tokens = [triple for triple in zip( corpus[:-2], corpus[1:-1], corpus[2:])]trigram_vocab = d2l.Vocab(trigram_tokens)trigram_vocab.token_freqs[:10]bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x', ylabel='frequency: n(x)', xscale='log', yscale='log', legend=['unigram', 'bigram', 'trigram'])8.3.4 读取长序列数据随机采样：每个样本都是在原始的长序列上任意捕获的子序列，在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。def seq_data_iter_random(corpus, batch_size, num_steps): #@save \"\"\"使用随机抽样生成一个小批量子序列。\"\"\" # 从随机偏移量开始对序列进行分区，随机范围包括`num_steps - 1` corpus = corpus[random.randint(0, num_steps - 1):] # 减去1，是因为我们需要考虑标签 num_subseqs = (len(corpus) - 1) // num_steps # 长度为`num_steps`的子序列的起始索引 initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # 在随机抽样的迭代过程中， # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻 random.shuffle(initial_indices) def data(pos): # 返回从`pos`位置开始的长度为`num_steps`的序列 return corpus[pos: pos + num_steps] num_batches = num_subseqs // batch_size for i in range(0, batch_size * num_batches, batch_size): # 在这里，`initial_indices`包含子序列的随机起始索引 initial_indices_per_batch = initial_indices[i: i + batch_size] X = [data(j) for j in initial_indices_per_batch] Y = [data(j + 1) for j in initial_indices_per_batch] yield torch.tensor(X), torch.tensor(Y)my_seq = list(range(35))for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5): print('X: ', X, '\\nY:', Y)# 输出X: tensor([[23, 24, 25, 26, 27], [18, 19, 20, 21, 22]])Y: tensor([[24, 25, 26, 27, 28], [19, 20, 21, 22, 23]])X: tensor([[28, 29, 30, 31, 32], [ 8, 9, 10, 11, 12]])Y: tensor([[29, 30, 31, 32, 33], [ 9, 10, 11, 12, 13]])X: tensor([[ 3, 4, 5, 6, 7], [13, 14, 15, 16, 17]])Y: tensor([[ 4, 5, 6, 7, 8], [14, 15, 16, 17, 18]])顺序分区：在迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。def seq_data_iter_sequential(corpus, batch_size, num_steps): #@save \"\"\"使用顺序分区生成一个小批量子序列。\"\"\" # 从随机偏移量开始划分序列 offset = random.randint(0, num_steps) num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size Xs = torch.tensor(corpus[offset: offset + num_tokens]) Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens]) Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) num_batches = Xs.shape[1] // num_steps for i in range(0, num_steps * num_batches, num_steps): X = Xs[:, i: i + num_steps] Y = Ys[:, i: i + num_steps] yield X, Yfor X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5): print('X: ', X, '\\nY:', Y)# 输出X: tensor([[ 4, 5, 6, 7, 8], [19, 20, 21, 22, 23]])Y: tensor([[ 5, 6, 7, 8, 9], [20, 21, 22, 23, 24]])X: tensor([[ 9, 10, 11, 12, 13], [24, 25, 26, 27, 28]])Y: tensor([[10, 11, 12, 13, 14], [25, 26, 27, 28, 29]])X: tensor([[14, 15, 16, 17, 18], [29, 30, 31, 32, 33]])Y: tensor([[15, 16, 17, 18, 19], [30, 31, 32, 33, 34]])将上面的两个采样函数包装到一个类中，以便稍后可以将其用作数据迭代器class SeqDataLoader: #@save \"\"\"加载序列数据的迭代器。\"\"\" def __init__(self, batch_size, num_steps, use_random_iter, max_tokens): if use_random_iter: self.data_iter_fn = d2l.seq_data_iter_random else: self.data_iter_fn = d2l.seq_data_iter_sequential self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens) self.batch_size, self.num_steps = batch_size, num_steps def __iter__(self): return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)# 定义函数 load_data_time_machine ，它同时返回数据迭代器和词汇表def load_data_time_machine(batch_size, num_steps, #@save use_random_iter=False, max_tokens=10000): \"\"\"返回时光机器数据集的迭代器和词汇表。\"\"\" data_iter = SeqDataLoader( batch_size, num_steps, use_random_iter, max_tokens) return data_iter, data_iter.vocab8.4 循环神经网络隐变量模型：\\(P(x_t|x_{t-1},...,x_1)\\approx P(x_t|h_{t-1})\\)其中ht-1是隐藏状态，也称为隐藏变量。可以基于当前输入xt和先前隐藏状态ht-1来计算时间步t处的任何时间的隐藏状态：\\(h_t=f(x_t,h_{t-1})\\)循环神经网络：具有隐藏状态的神经网络。与无隐藏状态的神经网络不同的是，当前时间步隐藏变量的计算由当前时间步的输入与前一个时间步的隐藏变量一起确定：\\(H_t=\\phi(X_tW_xh+H_{t-1}W_{hh}+b_h)\\)这些变量捕获并保留了序列直到其当前时间步的历史信息，就如当前时间步下神经网络的状态或记忆，因此这样的隐藏变量被称为隐藏状态。基于循环计算的隐状态神经网络被命名为循环神经网络。在循环神经网络中执行计算的层称为循环层。困惑度(Perplexity)：度量语言模型的质量，一个序列中所有的n个词元的交叉熵损失平均数的指数。\\(exp(-\\frac 1n\\sum_{t=1}^nlogP(x_t|x_{t-1},...,x_1))\\) 在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。 在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。 在基线上，该模型的预测是词汇表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词汇表中唯一词元的数量。8.5 循环神经网络的从零开始实现读取数据集%matplotlib inlineimport mathimport torchfrom torch import nnfrom torch.nn import functional as Ffrom d2l import torch as d2l# 读取数据集batch_size, num_steps = 32, 35train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)# one-hot编码# F.one_hot(torch.tensor([0, 2]), len(vocab))初始化模型参数，隐藏单元数num_hiddens是一个可调的超参数def get_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size def normal(shape): return torch.randn(size=shape, device=device) * 0.01 # 隐藏层参数 W_xh = normal((num_inputs, num_hiddens)) W_hh = normal((num_hiddens, num_hiddens)) b_h = torch.zeros(num_hiddens, device=device) # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # 附加梯度 params = [W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.requires_grad_(True) #表明要计算梯度 return params8.5.3 循环神经网络模型# 初始化隐藏状态def init_rnn_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), )def rnn(inputs, state, params): # `inputs`的形状：(`时间步数量`，`批量大小`，`词表大小`) W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] # `X`的形状：(`批量大小`，`词表大小`) for X in inputs: H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h) Y = torch.mm(H, W_hq) + b_q outputs.append(Y) return torch.cat(outputs, dim=0), (H,)class RNNModelScratch: #@save \"\"\"从零开始实现的循环神经网络模型\"\"\" def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn): self.vocab_size, self.num_hiddens = vocab_size, num_hiddens self.params = get_params(vocab_size, num_hiddens, device) self.init_state, self.forward_fn = init_state, forward_fn def __call__(self, X, state): X = F.one_hot(X.T, self.vocab_size).type(torch.float32) return self.forward_fn(X, state, self.params) def begin_state(self, batch_size, device): return self.init_state(batch_size, self.num_hiddens, device)输出形状是（时间步数×批量大小，词汇表大小），而隐藏状态形状保持不变，即（批量大小, 隐藏单元数）。8.5.4 预测首先循环遍历prefix中的字符，不断地将隐藏状态传递到下一个时间步，但是不生成任何输出。这被称为“预热”（warm-up）期，因为在此期间模型会自我更新（例如，更新隐藏状态），但不会进行预测。预热期结束后，隐藏状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。def predict_ch8(prefix, num_preds, net, vocab, device): #@save \"\"\"在`prefix`后面生成新字符。\"\"\" state = net.begin_state(batch_size=1, device=device) outputs = [vocab[prefix[0]]] get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) for y in prefix[1:]: # 预热期 _, state = net(get_input(), state) outputs.append(vocab[y]) for _ in range(num_preds): # 预测`num_preds`步 y, state = net(get_input(), state) outputs.append(int(y.argmax(dim=1).reshape(1))) return ''.join([vocab.idx_to_token[i] for i in outputs])8.5.5 梯度裁剪为了防止梯度爆炸，将梯度g投影回给定半径的球来裁剪梯度g。\\(g\\leftarrow min(1,\\frac {\\theta}{||g||})g\\)def grad_clipping(net, theta): #@save \"\"\"裁剪梯度。\"\"\" if isinstance(net, nn.Module): params = [p for p in net.parameters() if p.requires_grad] else: params = net.params norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params)) if norm &gt; theta: for param in params: param.grad[:] *= theta / norm8.5.6 训练#@savedef train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter): \"\"\"训练模型一个迭代周期（定义见第8章）。\"\"\" state, timer = None, d2l.Timer() metric = d2l.Accumulator(2) # 训练损失之和, 词元数量 for X, Y in train_iter: if state is None or use_random_iter: # 在第一次迭代或使用随机抽样时初始化`state` state = net.begin_state(batch_size=X.shape[0], device=device) else: if isinstance(net, nn.Module) and not isinstance(state, tuple): # `state`对于`nn.GRU`是个张量 state.detach_() else: # `state`对于`nn.LSTM`或对于我们从零开始实现的模型是个张量 for s in state: s.detach_() y = Y.T.reshape(-1) X, y = X.to(device), y.to(device) y_hat, state = net(X, state) l = loss(y_hat, y.long()).mean() if isinstance(updater, torch.optim.Optimizer): updater.zero_grad() l.backward() grad_clipping(net, 1) updater.step() else: l.backward() grad_clipping(net, 1) # 因为已经调用了`mean`函数 updater(batch_size=1) metric.add(l * y.numel(), y.numel()) return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()#@savedef train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False): \"\"\"训练模型（定义见第8章）。\"\"\" loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel='epoch', ylabel='perplexity', legend=['train'], xlim=[10, num_epochs]) # 初始化 if isinstance(net, nn.Module): updater = torch.optim.SGD(net.parameters(), lr) else: updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size) predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device) # 训练和预测 for epoch in range(num_epochs): ppl, speed = train_epoch_ch8( net, train_iter, loss, updater, device, use_random_iter) if (epoch + 1) % 10 == 0: print(predict('time traveller')) animator.add(epoch + 1, [ppl]) print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}') print(predict('time traveller')) print(predict('traveller'))num_epochs, lr = 500, 1train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())8.6 循环神经网络的简洁实现import torchfrom torch import nnfrom torch.nn import functional as Ffrom d2l import torch as d2lbatch_size, num_steps = 32, 35train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)# 构造一个具有256个隐藏单元的单隐藏层的循环神经网络层 rnn_layernum_hiddens = 256rnn_layer = nn.RNN(len(vocab), num_hiddens)#@saveclass RNNModel(nn.Module): \"\"\"循环神经网络模型。\"\"\" def __init__(self, rnn_layer, vocab_size, **kwargs): super(RNNModel, self).__init__(**kwargs) self.rnn = rnn_layer self.vocab_size = vocab_size self.num_hiddens = self.rnn.hidden_size # 如果RNN是双向的（之后将介绍），`num_directions`应该是2，否则应该是1。 if not self.rnn.bidirectional: self.num_directions = 1 self.linear = nn.Linear(self.num_hiddens, self.vocab_size) else: self.num_directions = 2 self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size) def forward(self, inputs, state): X = F.one_hot(inputs.T.long(), self.vocab_size) X = X.to(torch.float32) Y, state = self.rnn(X, state) # 全连接层首先将`Y`的形状改为(`时间步数`*`批量大小`, `隐藏单元数`)。 # 它的输出形状是 (`时间步数`*`批量大小`, `词表大小`)。 output = self.linear(Y.reshape((-1, Y.shape[-1]))) return output, state def begin_state(self, device, batch_size=1): if not isinstance(self.rnn, nn.LSTM): # `nn.GRU` 以张量作为隐藏状态 return torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device) else: # `nn.LSTM` 以张量作为隐藏状态 return (torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device), torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device))device = d2l.try_gpu()net = RNNModel(rnn_layer, vocab_size=len(vocab))net = net.to(device)d2l.predict_ch8('time traveller', 10, net, vocab, device)num_epochs, lr = 500, 1d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)9. 现代循环神经网络9.1 门控循环单元(GRU)9.1.1 门控隐藏状态门控循环单元：有专门的机制来确定应该何时更新隐藏状态，以及应该何时重置隐藏状态。重置门、更新门：(0,1)区间的向量，重置门允许我们控制可能还想记住的过去状态的数量，更新门将允许我们控制新状态中有多少个是旧状态的副本。候选隐藏状态：\\(\\overline H_t=tanh(X_tW_{xh}+(R_t\\oplus H_{t-1})W_{hh}+b_h)\\)每当重置门 Rt中的项接近 1时,为普通的循环神经网络，对于重置门 Rt中所有接近 0 的项，候选隐藏状态是以 Xt作为输入的多层感知机的结果。。因此，任何预先存在的隐藏状态都会被 重置 为默认值。隐藏状态：\\(H_t=Z_t\\oplus H_{t-1}+(1-Z_t)\\oplus \\overline H_t\\)每当更新门 Zt接近 1 时，我们就只保留旧状态。此时，来自 Xt的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步 t。相反，当 Zt接近 0 时，新的隐藏状态 Ht就会接近候选的隐藏状态。总之，门控循环单元具有以下两个显著特征： 重置门有助于捕获序列中的短期依赖关系。 更新门有助于捕获序列中的长期依赖关系。9.1.2 从零开始实现import torchfrom torch import nnfrom d2l import torch as d2lbatch_size, num_steps = 32, 35train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)# 初始化模型参数def get_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size def normal(shape): return torch.randn(size=shape, device=device)*0.01 def three(): return (normal((num_inputs, num_hiddens)), normal((num_hiddens, num_hiddens)), torch.zeros(num_hiddens, device=device)) W_xz, W_hz, b_z = three() # 更新门参数 W_xr, W_hr, b_r = three() # 重置门参数 W_xh, W_hh, b_h = three() # 候选隐藏状态参数 # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # 附加梯度 params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.requires_grad_(True) return params# 定义模型def init_gru_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), )def gru(inputs, state, params): W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] for X in inputs: Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z) R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r) H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h) H = Z * H + (1 - Z) * H_tilda Y = H @ W_hq + b_q outputs.append(Y) return torch.cat(outputs, dim=0), (H,)# 训练与预测vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()num_epochs, lr = 500, 1model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params, init_gru_state, gru)d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)9.1.3 简洁实现num_inputs = vocab_sizegru_layer = nn.GRU(num_inputs, num_hiddens)model = d2l.RNNModel(gru_layer, len(vocab))model = model.to(device)d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)9.2 长短期记忆网络(LSTM)9.2.1 门控记忆单元输入门It：用来决定何时将数据读入单元，控制采用多少来自候选记忆单元的新数据；输出门Ot：用来从单元中读出条目，只要输出门接近 11，我们就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近 00，我们只保留存储单元内的所有信息，并且没有进一步的过程需要执行；遗忘门Ft：重置单元的内容，控制保留了多少就记忆单元的内容。候选记忆单元：\\(\\overline C_t=tanh(X_tW_{xc}+H_{t-1}W{hc}+b_c)\\)记忆单元：\\(C_t=F_t\\oplus C_{t-1}+I_t\\oplus \\overline C_t\\)如果遗忘门始终为 1 且输入门始终为 0，则过去的记忆单元 Ct−1将随时间被保存并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。隐藏状态：\\(H_t=O_t\\oplus tanh(C_t)\\)9.2.2 从零开始实现import torchfrom torch import nnfrom d2l import torch as d2lbatch_size, num_steps = 32, 35train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)# 初始化模型参数def get_lstm_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size def normal(shape): return torch.randn(size=shape, device=device)*0.01 def three(): return (normal((num_inputs, num_hiddens)), normal((num_hiddens, num_hiddens)), torch.zeros(num_hiddens, device=device)) W_xi, W_hi, b_i = three() # 输入门参数 W_xf, W_hf, b_f = three() # 遗忘门参数 W_xo, W_ho, b_o = three() # 输出门参数 W_xc, W_hc, b_c = three() # 候选记忆单元参数 # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # 附加梯度 params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] for param in params: param.requires_grad_(True) return params# 定义模型def init_lstm_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), torch.zeros((batch_size, num_hiddens), device=device))def lstm(inputs, state, params): [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params (H, C) = state outputs = [] for X in inputs: I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i) F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f) O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o) C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c) C = F * C + I * C_tilda H = O * torch.tanh(C) Y = (H @ W_hq) + b_q outputs.append(Y) return torch.cat(outputs, dim=0), (H, C)# 训练和预测vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()num_epochs, lr = 500, 1model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params, init_lstm_state, lstm)d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)9.2.3 简洁实现num_inputs = vocab_sizelstm_layer = nn.LSTM(num_inputs, num_hiddens)model = d2l.RNNModel(lstm_layer, len(vocab))model = model.to(device)d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)9.3 深度循环神经网络深度循环神经网络：将多层循环神经网络堆叠在一起，通过对几个简单层的组合，来产生灵活的机制。import torchfrom torch import nnfrom d2l import torch as d2lbatch_size, num_steps = 32, 35train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)vocab_size, num_hiddens, num_layers = len(vocab), 256, 2num_inputs = vocab_sizedevice = d2l.try_gpu()lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)model = d2l.RNNModel(lstm_layer, len(vocab))model = model.to(device)num_epochs, lr = 500, 2d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)9.4 双向循环神经网络在很多情况下，每个短语的下文传达了重要的信息，单纯用序列模型表现不佳。双向循环神经网络：添加了反向传递信息的隐藏层。使用来自过去和未来的观测信息来预测当前的观测。双向循环网络不能预测未来，并且其计算量大，速度慢。前向和反向隐状态的更新如下：\\(\\begin{split}\\begin{aligned}\\overrightarrow{\\mathbf{H}}_t &amp;= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)} + \\mathbf{b}_h^{(f)}),\\\\\\overleftarrow{\\mathbf{H}}_t &amp;= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)} + \\mathbf{b}_h^{(b)}),\\end{aligned}\\end{split}\\)将前向隐状态和反向隐状态连接起来，获得需要送入输出层的隐状态Ht。在具有多个隐藏层的深度双向循环神经网络中， 该信息作为输入传递到下一个双向层。 最后，输出层计算得到的输出为 （q是输出单元的数目）：\\(\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.\\)# 双向循环神经网络的错误应用import torchfrom torch import nnfrom d2l import torch as d2l# 加载数据batch_size, num_steps, device = 32, 35, d2l.try_gpu()train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)# 通过设置“bidirective=True”来定义双向LSTM模型vocab_size, num_hiddens, num_layers = len(vocab), 256, 2num_inputs = vocab_sizelstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)model = d2l.RNNModel(lstm_layer, len(vocab))model = model.to(device)# 训练模型num_epochs, lr = 500, 1d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)9.6 编码器-解码器结构编码器：接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。解码器：将固定形状的编码状态映射到长度可变的序列。from torch import nn#@saveclass Encoder(nn.Module): \"\"\"编码器-解码器架构的基本编码器接口\"\"\" def __init__(self, **kwargs): super(Encoder, self).__init__(**kwargs) def forward(self, X, *args): raise NotImplementedError #raise抛出异常#@saveclass Decoder(nn.Module): \"\"\"编码器-解码器架构的基本解码器接口\"\"\" def __init__(self, **kwargs): super(Decoder, self).__init__(**kwargs) def init_state(self, enc_outputs, *args): raise NotImplementedError def forward(self, X, state): raise NotImplementedError# 合并编码器和解码器class EncoderDecoder(nn.Module): \"\"\"编码器-解码器架构的基类\"\"\" def __init__(self, encoder, decoder, **kwargs): super(EncoderDecoder, self).__init__(**kwargs) self.encoder = encoder self.decoder = decoder def forward(self, enc_X, dec_X, *args): enc_outputs = self.encoder(enc_X, *args) dec_state = self.decoder.init_state(enc_outputs, *args) return self.decoder(dec_X, dec_state)10. 注意力机制10.1 注意力提示非自主性提示：基于环境中物体的突出行和易见性，如注意力集中在红色而不是灰色。自主性提示：依赖于任务的意志提示（想读一本书），注意力被自主引导到书上。在注意力机制的背景下，我们将自主性提示称为查询，给定任何查询，注意力机制通过注意力汇聚， 将选择引导至感官输入。在注意力机制中，这些感官输入被称为值。10.1.3 注意力的可视化平均汇聚层可以被视为输入的加权平均值， 其中各输入的权重是一样的。 实际上，注意力汇聚得到的是加权平均的总和值， 其中权重是在给定的查询和不同的键之间计算得出的。import torchfrom d2l import torch as d2l# 输入matrices的形状是 （要显示的行数，要显示的列数，查询的数目，键的数目）def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5), cmap='Reds'): \"\"\"显示矩阵热图\"\"\" d2l.use_svg_display() num_rows, num_cols = matrices.shape[0], matrices.shape[1] fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize, sharex=True, sharey=True, squeeze=False) for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)): for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)): pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap) #绘制热图 if i == num_rows - 1: ax.set_xlabel(xlabel) if j == 0: ax.set_ylabel(ylabel) if titles: ax.set_title(titles[j]) fig.colorbar(pcm, ax=axes, shrink=0.6); # 给子图添加colorbar（颜色条或渐变色条）10.2 注意力汇聚：Nadaraya-Watson 核回归生成人工数据集：\\(y_i = 2\\sin(x_i) + x_i^{0.8} + \\epsilon\\)# 生成数据集n_train = 50 # 训练样本数x_train, _ = torch.sort(torch.rand(n_train) * 5) # 排序后的训练样本def f(x): return 2 * torch.sin(x) + x**0.8y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,)) # 训练样本的输出x_test = torch.arange(0, 5, 0.1) # 测试样本y_truth = f(x_test) # 测试样本的真实输出n_test = len(x_test) # 测试样本数n_testNadaraya-Watson核回归：根据输入的位置对输出yi进行加权：\\(\\begin{split}\\begin{aligned} f(x) &amp;=\\sum_{i=1}^n \\alpha(x, x_i) y_i\\\\ &amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \\\\&amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}(x - x_i)^2\\right) y_i. \\end{aligned}\\end{split}\\)如果一个键xixi越是接近给定的查询x， 那么分配给这个键对应值yi的注意力权重就会越大， 也就“获得了更多的注意力”。带参数的Nadaraya-Watson核回归：\\(\\begin{split}\\begin{aligned}f(x) &amp;= \\sum_{i=1}^n \\alpha(x, x_i) y_i \\\\&amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}((x - x_j)w)^2\\right)} y_i \\\\&amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}((x - x_i)w)^2\\right) y_i.\\end{aligned}\\end{split}\\)# 批量矩阵乘法X = torch.ones((2, 1, 4))Y = torch.ones((2, 4, 6))torch.bmm(X, Y).shape# 输出torch.Size([2, 1, 6])# 定义模型class NWKernelRegression(nn.Module): def __init__(self, **kwargs): super().__init__(**kwargs) self.w = nn.Parameter(torch.rand((1,), requires_grad=True)) def forward(self, queries, keys, values): # `queries` 和 `attention_weights` 的形状为 (查询个数，“键－值”对个数) queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1])) self.attention_weights = nn.functional.softmax( -((queries - keys) * self.w)**2 / 2, dim=1) # `values` 的形状为 (查询个数，“键－值”对个数) return torch.bmm(self.attention_weights.unsqueeze(1), values.unsqueeze(-1)).reshape(-1) # `X_tile` 的形状: (`n_train`，`n_train`)，每一行都包含着相同的训练输入X_tile = x_train.repeat((n_train, 1))# `Y_tile` 的形状: (`n_train`，`n_train`)，每一行都包含着相同的训练输出Y_tile = y_train.repeat((n_train, 1))# `keys` 的形状: ('n_train'，'n_train' - 1)keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))# `values` 的形状: ('n_train'，'n_train' - 1)values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))net = NWKernelRegression()loss = nn.MSELoss(reduction='none')trainer = torch.optim.SGD(net.parameters(), lr=0.5)animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])for epoch in range(5): trainer.zero_grad() # L2 Loss = 1/2 * MSE Loss l = loss(net(x_train, keys, values), y_train) / 2 l.sum().backward() trainer.step() print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}') animator.add(epoch + 1, float(l.sum()))10.3 注意力评分函数掩蔽softmax操作：softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。为了仅将有意义的词元作为值来获取注意力汇聚， 我们可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。#@savedef masked_softmax(X, valid_lens): \"\"\"通过在最后一个轴上掩蔽元素来执行 softmax 操作\"\"\" # `X`: 3D张量，`valid_lens`: 1D或2D 张量 if valid_lens is None: return nn.functional.softmax(X, dim=-1) else: shape = X.shape if valid_lens.dim() == 1: valid_lens = torch.repeat_interleave(valid_lens, shape[1]) else: valid_lens = valid_lens.reshape(-1) # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0 X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6) return nn.functional.softmax(X.reshape(shape), dim=-1)加性注意力：当查询和键是不同长度的矢量时， 我们可以使用加性注意力作为评分函数。\\(a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R}\\)#@saveclass AdditiveAttention(nn.Module): \"\"\"加性注意力\"\"\" def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs): super(AdditiveAttention, self).__init__(**kwargs) self.W_k = nn.Linear(key_size, num_hiddens, bias=False) self.W_q = nn.Linear(query_size, num_hiddens, bias=False) self.w_v = nn.Linear(num_hiddens, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens): queries, keys = self.W_q(queries), self.W_k(keys) # 在维度扩展后， # `queries` 的形状：(`batch_size`，查询的个数，1，`num_hidden`) # `key` 的形状：(`batch_size`，1，“键－值”对的个数，`num_hiddens`) # 使用广播方式进行求和 features = queries.unsqueeze(2) + keys.unsqueeze(1) features = torch.tanh(features) # `self.w_v` 仅有一个输出，因此从形状中移除最后那个维度。 # `scores` 的形状：(`batch_size`，查询的个数，“键-值”对的个数) scores = self.w_v(features).squeeze(-1) self.attention_weights = masked_softmax(scores, valid_lens) # `values` 的形状：(`batch_size`，“键－值”对的个数，值的维度) return torch.bmm(self.dropout(self.attention_weights), values)queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))# `values` 的小批量，两个值矩阵是相同的values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat( 2, 1, 1)valid_lens = torch.tensor([2, 6])attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8, dropout=0.1)attention.eval()attention(queries, keys, values, valid_lens)d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)), xlabel='Keys', ylabel='Queries')缩放点积注意力：使用点积可以得到计算效率更高的评分函数， 但是点积操作要求查询和键具有相同的长度d。评分函数：\\(a(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k} /\\sqrt{d}\\)缩放点积的注意力：\\(\\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}\\)#@saveclass DotProductAttention(nn.Module): \"\"\"缩放点积注意力\"\"\" def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) # `queries` 的形状：(`batch_size`，查询的个数，`d`) # `keys` 的形状：(`batch_size`，“键－值”对的个数，`d`) # `values` 的形状：(`batch_size`，“键－值”对的个数，值的维度) # `valid_lens` 的形状: (`batch_size`，) 或者 (`batch_size`，查询的个数) def forward(self, queries, keys, values, valid_lens=None): d = queries.shape[-1] # 设置 `transpose_b=True` 为了交换 `keys` 的最后两个维度 scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d) self.attention_weights = masked_softmax(scores, valid_lens) return torch.bmm(self.dropout(self.attention_weights), values)queries = torch.normal(0, 1, (2, 1, 2))attention = DotProductAttention(dropout=0.5)attention.eval()attention(queries, keys, values, valid_lens)d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)), xlabel='Keys', ylabel='Queries')10.4 Bahdanau注意力Bahdanau注意力模型：\\(\\mathbf{c}_{t'} = \\sum_{t=1}^T \\alpha(\\mathbf{s}_{t' - 1}, \\mathbf{h}_t) \\mathbf{h}_t\\)10.5 多头注意力多头注意力：用独立学习得到的h组不同的线性投影来变换查询、键和值。 然后，这h组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这hh个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。10.6 自注意力和位置编码将词元序列输入注意力池化中， 以便同一组词元同时充当查询、键和值。 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为自注意力。卷积神经网络和自注意力都拥有并行计算的优势， 而且自注意力的最大路径长度最短。 但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，我们通过在输入表示中添加位置编码来注入绝对的或相对的位置信息。" } ]
