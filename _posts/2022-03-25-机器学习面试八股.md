---
title: 机器学习面试八股
date: 2022-03-25 20:00:00 +0800
categories: [八股,算法]
tags: [算法]
pin: true
author: 王家乐

toc: true
comments: true
typora-root-url: ../../jlwang1998.github.io
math: false
mermaid: true
---

# **1. 线性回归模型**

就是拟合方程

假设特征和结果满足线性关系；经过最⼤似然估计推导出来的待优化的⽬标函数与平⽅损失函数是等价的。 

- 岭回归
  加入L2正则项，等价于对参数w引入协方差为a的零均值高斯先验，不能做variable selection。 
- LASSO回归
  加入L1正则项，等价于对参数w引入拉普拉斯先验，可以做variable selection。 

# **2. 逻辑回归LR原理**

LR是一中常见的用于分类的模型，本质上还是一个线性回归，先把特征线性组合，然后使用sigmoid函数（单调可微）将结果约束到0~1之间，结果用于二分类或者回归预测。 用来估计某种事物的可能性。

把线性回归的结果通过sigmoid映射到0-1之间。

- 模型参数估计
  最大似然估计法估计模型参数，使用梯度下降或者拟牛顿法进行学习。 
- 损失函数
  最小化交叉熵误差，等价于最大似然估计 
- 解决非线性分类问题
  加核函数或特征变换，显式地把特征映射到高维空间。 

## 2.1 线性回归和逻辑回归的关系

线性回归解决连续值预测问题，逻辑回归解决分类或问题。

# **3. SVM原理**（二分类）

一个样本集线性可分的时候，存在无穷个可以将两类数据正确分开，而SVM是通过最大化间隔来选择分类超平面，最大化这个间隔可以构造成一个约束最优化问题，这是一个凸二次规划问题，然后使用拉格朗日乘子法把约束优化问题转为无约束优化问题，令各个变量偏导为0代入拉格朗日函数得到它的对偶问题，最后用SMO[算法]()来求解这个对偶问题。 

- 解对偶问题的好处
  一是对偶问题更好解，因为原问题是凸二次规划问题，对偶问题那里只需要求解alpha系数，而alpha系数只有支持向量才非0（KKT条件）。二是自然引入核函数，进而推广到非线性问题。 
- 软间隔
  当数据近似线性可分时，可以引入松弛变量，使间隔加上松弛变量大于等于1，针对每一个松弛变量都有一个惩罚。 
- 核技巧
  用一个变换把数据从原空间映射到新空间，数据在新空间是线性可分的。由于新空间有可能非常高维甚至无限维，难以计算且难以表示，所以不显式的定义该映射函数，而是定义一个函数把两个样本直接映射到他们在新空间的内积，因为求解对偶问题时只需要用到内积。常用核函数有高斯核、多项式核、线性核、拉普拉斯核、sigmoid核。 
- 核函数的选择
  根据专家先验知识，或采用交叉验证，试用不同的核函数（线性核，多项式核和径向基核函数）。 

**问题**：什么样的决策边界是最好的？

<img src="/assets/blog_res/2022-03-25-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%85%AB%E8%82%A1.assets/image-20220707213000595.png" alt="image-20220707213000595" style="zoom:33%;" />

找到一条线(w,b)，使离该线最近的点能够最远。支持向量指的是真正能发挥作用的数据点。

软间隔：有时候数据中有一些噪音点，如果考虑它们，决策边界就不好了。为了解决，引入松弛因子，（不一定要把所有点分开），
$$
y_i(w*x_i+b)\ge 1-\delta_i
$$
SVM优点：解决低维不可分问题（二维不可分，三维可分），找到一种变换方法$\phi(x)$，把结果映射到高维空间，高斯核函数，$K(x,y)=exp(- \frac {||x-y||^2}{2\theta^2})$



# **4. LR与SVM的异同** 

- 相同之处
  都可以处理分类问题，都可以添加正则项。
- 不同之处  
  - 损失函数不同，LR使用logistical loss（交叉熵），SVM使用hingeloss，SVM的损失函数自带正则项，而LR则需要自己添加正则项。 
  - 解决非线性问题时，SVM采用核函数机制，而LR一般不用，因为复杂核函数需要很大计算量，SVM中只有支持向量参与计算，而LR是全部样本都需要参与计算，若使用核函数计算量太大。 
  - 对异常值的敏感度不一样。LR中所有样本都对分类平面有影响，所以异常点的影响会被掩盖。但SVM的分类平面取决于支持向量，如果支持向量受到异常值影响，则结果难以预测。 
  - 在高维空间LR的表现比SVM更稳定，因为SVM是要最大化间隔，这个间隔依赖与距离测度，在高维空间时这个距离测度往往不太好。（所以需要做归一化） 

# **5. 分类任务常用的目标函数**

交叉熵误差，hinge loss等。 

- 相对熵
  也称KL散度，相对熵可以用来衡量两个概率分布之间的差异。 
- 交叉熵
  可以用来计算学习模型分布与训练分布之间的差异，交叉熵损失通常适用于Softmax 分类器。最小化相对熵（KL散度）等价于最小化交叉熵，也等价于最大化似然估计。 
- Hinge loss（折页损失函数）
  在二分类情况下，公式如下：L(y) =max(0 , 1–t*y)，其中，y是预测值(-1到1之间)，t为目标值(1或 -1)。其含义为，y的值在 -1到1之间即可，并不鼓励|y|>1，即让某个样本能够正确分类就可以了，不鼓励分类器过度自信，当样本与分割线的距离超过1时并不会有任何奖励。目的在于使分类器更专注于整体的分类误差。 

# **6. LR逻辑回归为什么对特征进行离散化** 

- 离散特征的增加和减少都很容易，易于模型的快速迭代； 
- 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 
- 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 
- 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 
- 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 
- 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 
- 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。 

# **7. 正则项** 

- 结构风险最小化
  即正则化。在经验风险最小化基础上加上正则项，惩罚复杂的模型。结构风险小同时需要经验风险小和模型简单，如贝叶斯估计中的最大后验概率估计。 

- 经验风险最小化
  即误差函数最小，样本数量大时，经验风险最小化学习效果好，如极大似然估计。样本少时会出现过拟合。 

- 范数

  其非负性使得它天然适合作为[机器学习]()的正则项。

  - L1范数：向量中各个元素绝对值之和。 
  - L2范数：向量中各个元素平方和的开二次方根。 
  - Lp范数：向量中各个元素绝对值的p次方和的开p次方根。 

- L1正则
  项目标函数中增加所有权重w参数的绝对值之和, 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0, 奔向零的速度不如L1给力了)，倾向于使参数稀疏化； 

- L2正则项
  倾向于使参数稠密地接近于0，目标函数中增加所有权重w参数的平方之和, 逼迫所有w尽可能趋向零但不为零。 

# **8. 决策树**

概念：决策树一种基本的分类和回归方法，呈树形结构，表示基于特征对实例进行分类的过程。决策树的学习过程包括三个步骤：**特征选择、决策树的生成以及决策树的修剪（预剪枝和后剪枝）**。特征选择的准则是信息增益或者信息增益比（基尼系数）。 

- ID3生成[算法]()（使用信息增益进行特征选择）
  从根节点开始，对结点结算所有可能的特征的信息增益，选择信息增益最大的特征作为结点特征，然后对子节点递归的调用这个方法，构建决策树。信息增益偏向于选择取值较多的特征，容易过拟合。 
- C4.5生成[算法]()（使用信息增益比进行特征选择）
  与ID3[算法]()相似，用信息增益率（偏好可取值数目较少）来选择特征。小技巧：先找出信息增益高于平均水平的属性，再从中选择增益率最高的。 
- CART生成[算法]()（使用平方误差最小化来选择特征）
  全称分类树与回归树，可用于分类也可用于回归。分类树用基尼系数（越小越好）进行特征选择，用最小二乘回归树生成[算法]()生成回归树（结点下所有点的均值作为该结点的预测值）。 
- 基尼指数
  反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。因此基尼指数越小，则数据集的纯度越高。 
- 决策树的剪枝（防止过拟合）
  通过极小化决策树整体的损失函数来进行剪枝。从树的叶节点开始向上回缩，假设回缩前树为Ta，回缩到父节点后树为Tb，计算两棵树的损失函数，如果回缩后损失函数变小，则剪枝，把父节点设为叶节点，这样迭代。 

## 8.1 如何避免过拟合

- 设置每个叶子节点的最小样本数；
- 设置树的最大深度；
- 设置叶子节点的总数量；
- 剪枝：对每个结点或子树进行裁剪，评估剪枝前后模型的预测能力。

# **9. 随机森林（样本随机性和特征随机性）** 

- 原理
  由很多棵决策树组成，每棵决策树之间没有关联。每棵树的生成方法是，随机而且有放回地（如果没有放回那就每棵树都训练集完全不相交，太片面）从训练集选取N个（训练集大小为N）训练样本作为训练集，所以每棵树训练集都不一样，但包含重复样本，生成决策树选择特征时，从特征子集里面选择最优的特征用于分裂。得到森林后，输入样本让每棵决策树进行判断，输出结果为被最多决策树选择的分类。 
- 与Bagging区别
  与Bagging不一样的是会抽取与样本数目同样大小的样本来训练（Bagging一般少于n），且只用部分特征得到分类器（Bagging用全部特征）。 
- 森林中任意两棵树的相关性
  相关性越大，错误率越大。 
- 森林中每棵树的分类能力
  每棵树的分类能力越强，整个森林的错误率越低。 
- 随机森林唯一的一个参数
  特征个数m，减小m会降低树的相关性和分类能力，增加m两者也会随之增大。 

# **10. 集成学习** 

结合多个学习器组合成一个性能更好的学习器。

- Bagging
  对原数据有放回抽取，构建出多个样本数据集，训练出多个分类器。 

  并行训练K个模型，采取投票方式得到分类结果。

  代表：随机森林，数据采样随机、特征选择随机、很多个决策树并行放在一起。

  理论上数越多效果越好，但超过一定数量就不变了。

- Boosting（AdaBoost，Xgboost)

  AdaBoost：根据前一次的分类结果调整数据权重。

  使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权)。通过改变样本分布，对容易错分的数据加强学习，增加错分样本数据的权重。 

  每轮训练改变样本权重。

- 两者区别：  
  - 样本选择不同
    Bagging是有放回地抽取数据集，每轮训练集是独立的。而Boosting每一轮的训练集不变，但是训练集中每个样本的权重会发生变化，根据上一轮分类结果调整。 
  - 样本权重不同
    Bagging每个样本权重相等，Boosting根据错误率不断调整样本的权重，被错分的样本权重会增加。 
  - 生成方式不同
    Bagging各个分类器可以并行生成，而Boosting只能顺序生成。 
  - 优化目标有差异
    Bagging减小的是方差，Boosting减小的是偏差。Bagging减小方差是因为对多个用不同训练集得到的分类器取了平均（取平均比单一的方差要小）。Boosting减小偏差是因为每次迭代都在之前误差的基础上进一步降低错误率，所以就是减小偏差，对于训练集的拟合程度好。 
  - Boosting不会显著降低方差是因为其训练过程中各基学习器是强相关的，缺少独立性。

- Stacking模型

堆叠各种各样的分类器（KNN，SVM，RF）

# **11. Boosting方法** 

- AdaBoost
  初始化样本权重，每个样本最开始的权重一样。训练弱分类器，若样本在这次训练中被正确分类，则降低它的权重，反之增加。这样多次训练得到多个弱分类器组成一个强分类器，误差率小的弱分类器拥有更大的权重。 

- 提升树

  提升树是迭代多棵回归树来共同决策， 当采用平方误差函数，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，最后累加所有树的结果作为最终结果。  

  - GBDT
    梯度提升树。利用损失函数的负梯度在当前模型的值作为提升树[算法]()的残差的近似值。因为当损失函数不是平方误差时，残差不是简单的真实值减去预测值，把目标函数对当前模型求梯度，就可以得到模型改进的方向，负梯度就可以近似成残差。 
  - XGBoost
    基于C++通过多线程实现了回归树的并行构建，并在原有GBDT基础上加以改进，在目标函数增加了正则化项，正则项里包括了树的叶子结点个数、每个叶子结点上输出分数的L2模的平方和，且对目标函数做了二阶泰勒展开，从而极大提升了模型的训练速度和预测精度。 

# **12. XGBoost为什么要泰勒展开**

二阶导数有利于梯度下降更快更准；

统一损失函数求导的形式以支持自定义损失函数。 

## 12.1 为什么不三阶展开

要求三阶可导，对精度提升不大。

# **13. GBDT和XGBoost的区别** 

- 基分类器的选择：GBDT以CART做基分类器，XGBoost还支持线性分类器。 
- GBDT在优化时只用到了一阶导数信息，XGBoost对目标函数进行了二阶泰勒展开，同时用到一阶和二阶导数。 
- XGBoost在目标函数里加入了正则项，控制模型的复杂度。 
- XGBoost可以为缺失值或者指定的值指定分支的默认方向，对特征值有缺失的样本可以自动学习出他的分裂方向。 
- XGBoost支持并行，可以在特征粒度上并行，在训练前对特征进行分块[排序]()，在寻找最佳分裂点的时候可以并行化计算，大大提升速度。 

# **14. RF和GBDT的区别** 

- 相同点
  都是由多棵树组成，最终的结果都是由多棵树一起决定。 
- 不同点  
  - 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成； 
  - 组成随机森林的树可以并行生成，而GBDT是串行生成； 
  - 随机森林的结果是多棵树投票表决的，而GBDT则是多棵树累加之和； 
  - 随机森林对异常值不敏感，而GBDT对异常值比较敏感； 
  - 随机森林是减少模型的方差，而GBDT是减少模型的偏差； 
  - 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化。 

## 14.1 GBDT为什么需要进行特征归一化

因为GBDT的树是在上一颗树的基础上通过梯度下降求解最优解，归一化能收敛的更快。

# **15. XGBoost和LightGBM的区别** 

- XGBoost采用的是level-wise的分裂策略，而lightGBM采用了leaf-wise的策略，区别是XGBoost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是XGBoost也进行了分裂，带来了务必要的开销。 leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。 
- LightGBM使用了基于histogram的决策树[算法]()，这一点不同与XGBoost中的exact[算法]()，histogram[算法]()在内存和计算代价上都有不小优势。 

# **16. EM[算法]()** 

- 定义：在概率模型中寻找参数最大似然估计或者最大后验估计的[算法]()，其中概率模型依赖于无法观测的隐性变量。 
- 方法：最大期望[算法]()经过两个步骤交替进行计算：第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值； 第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。 

# **17. 归一化处理** 

- 标准化：把特征值变为均值0，方差1 
- 归一化：把每个特征向量的值放缩到相同数值范围，如[0,1] 
- 归一化的作用  
  - 加快梯度下降求最优解的速度。因为若两个特征的值区间相差大，数据会呈扁长形，梯度下降很可能会走之字形路线（垂直等高线走）。 
  - 可能提高精度。有些分类器依赖于计算样本之间的距离，而区间大特征值对距离影响会更大，但若区间小的特征值才是真正重要的特征，则容易被忽略。 
- 哪些[机器学习]()[算法]()需要归一化
  利用梯度下降法求解的模型一般需要归一化，如线性回归、LR、SVM、KNN、神经网络等。树形模型一般不需要归一化，因为他们不关心变量的值（数值缩放不影响分裂位置），而是关心变量的分布，如决策树、随机森林。 

# **18. 如何进行特征选择** 

- 特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解. 
- 常见的特征选择方式   
  - 去除方差较小的特征； 
  - 正则化：L1正则化能够生成稀疏的模型；L2正则化的表现更加稳定，由于有用的特征往往对应系数非零; 
  - 稳定性选择：在不同的数据子集和特征子集上运行特征选择[算法]()，不断的重复，最终汇总特征选择结果。选择[算法]()可以是回归、SVM或其他类似的方法。 

# **19. 样本不均匀** 

- 加权：不同类别分错的代价设为不同。 
- 采样：上采样和下采样，上采样是把小众类复制多份，下采样是从大众类中选取部分样本。  
  - 上采样
    上采样会把小众样本复制多份，一个点会在高维空间中反复出现，这会导致一个问题，那就是运气好就能分对很多点，否则分错很多点。为了解决这一问题，可以在每次生成新数据点时加入轻微的随机扰动。 
  - 下采样
    因为下采样会丢失信息，为了减少信息的损失，第一种方法可以利用模型融合的方法（bagging）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。第二种方法利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。 

# **20. 模型评价指标** 

- TP：正类预测为正类 

- FN：正类预测为负类 

- FP：负类预测为正类 

- TN：负类预测为负类 

- 精确率：预测为正的样本当中有多少预测正确了，P = TP/(TP+FP) 

- 召回率：真正为正的样本当中有多少被预测，R = TP/(TP+FN) 

- F1值: 综合考虑了精确率和召回率，2/F1 = 1/R + 1/P 

- ROC：横轴是假阳性，FPR=FP/(FP+TN)，真实的正例中，被预测正确的比例；纵轴是真阳性，TPR=TP/(TP+FN)，真实的反例中，被预测正确的比例；绘制方法：假设已有一系列样本被分为正类的概率，按概率值从大到小[排序]()，依次将该概率值作为阈值来预测分类，每个阈值都对应一对FPR、TPR，这样一来就能绘制出曲线。 

- $$
  FDP=\frac {FP}{FP+TN}
  $$

  $$
  TPR=\frac {TP}{TP+FN}
  $$

  

- AUC：ROC曲线的面积，AUC大的分类器效果更好。 AUC<=1

# **21. 常用计算距离的方法** 

- 欧氏距离：将样本的不同属性之间的差别等同对待，但实际上可能有些属性更重要。适用于各分量的度量标准统一的情况。 
- 曼哈顿距离：两个点在标准坐标系上的绝对轴距之和。计算速度快 
- 切比雪夫距离：国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子(x1,y1)走到格子(x2,y2)最少需要的步数叫切比雪夫距离。 

# **22. 聚类**

常用方法有：层次的方法（hierarchical method）、划分方法（partitioning method）、基于密度的方法（density-based method）、基于网格的方法（grid-based method）、基于模型的方法（model-based method）等。 

- 经典K-means（划分）[算法]()流程
  1. 随机地选择k个对象，每个对象初始地代表了一个簇的中心； 
  2. 对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇； 
  3. 重新计算每个簇的平均值，更新为新的簇中心； 
  4. 不断重复ii、iii，直到准则函数收敛。 
- K个初始类簇点的选取还有两种方法  
  - 选择彼此距离尽可能远的K个点； 
  - 先对数据用层次聚类算法进行聚类，得到K个簇之后，从每个类簇中选择一个点，该点可以是该类簇的中心点，或者是距离类簇中心点最近的那个点。 
- K值的选择
  轮廓系数，求出所有样本的轮廓系数后再求平均值就得到了平均轮廓系数。平均轮廓系数的取值范围为[-1,1]，且簇内样本的距离越近，簇间样本距离越远，平均轮廓系数越大，聚类效果越好。那么，很自然地，平均轮廓系数最大的k便是最佳聚类数。 
- 经典DBSCAN算法流程
  1. DBSCAN通过检查数据集中每点的Eps邻域来搜索簇，如果点p的Eps邻域包含的点多于MinPts个，则创建一个以p为核心对象的簇； 
  2. 然后，DBSCAN迭代地聚集从这些核心对象直接密度可达的对象，这个过程可能涉及一些密度可达簇的合并； 
  3. 当没有新的点添加到任何簇时，该过程结束。 
- **K-means与DBSCAN对比**
  和传统的K-Means算法相比，DBSCAN最大的不同就是不需要输入类别数k，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点 
- DBSCAN的主要优点有  
  - 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集； 
  - 可以在聚类的同时发现异常点，对数据集中的异常点不敏感； 
  - 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。 
- DBSCAN的主要缺点有  
  - 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合； 
  - 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进； 
  - 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。

# 23.监督学习和无监督学习

监督学习：决策树，RF，KNN，SVM，贝叶斯

无监督学习：K-means，PCA

# 24.为什么分类问题不用MSE损失函数？

- MSE：预测值与目标值的欧氏距离；
- 交叉熵：真实概率分布与预测概率分布的差异。
- MSE对二分类问题是非凸的，不能保证损失函数最小化

# 25. 过拟合

泛化性不好。variance会变大。

减轻过拟合的方法：增加数据量，L1、L2正则化（使损失函数的权重减小），dropout（删除部分的隐藏单元，一般用于全连接层，池化层，LSTM层之后，）

# 26.bias和variance

bias：反映样本上的输出与真实值的误差（精确度）；

偏差描述的是算法的预测的平均值和真实值的关系（可以想象成算法的拟合能力如何），而方差描述的是同一个算法在不同数据集上的预测值和所有数据集上的平均预测值之间的关系（可以想象成算法的稳定性如何）。

# 27. 梯度下降

梯度下降就是用来求某个函数最小值时自变量对应取值

其中这句话中的某个函数是指：损失函数（cost/loss function），直接点就是误差函数。

一个算法不同参数会产生不同拟合曲线，也意味着有不同的误差。

损失函数就是一个自变量为算法的参数，函数值为误差值的函数。所以梯度下降就是找让误差值最小时候算法取的参数。

# 28. 决策树

决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。

**优点：**

可以做分析、可视化、速度快、集成。

衡量标准：信息熵（表示随机变量不确定性的度量。使熵值最小）

决策树的剪枝：（原因）过拟合风险很大；设置参数不要让树模型过于复杂。

# 29. K-MEANS算法（聚类）

无监督（无标签），无法证明算法的有效性。

需要指定K值（类别），质心：均值。

半监督：样本中有的有标签，有的无标签。然后用聚类，将无标签的打上标签。

# 30. 贝叶斯

先验概率。

朴素贝叶斯：假设特征之间相互独立，为了计算方便。

# 31. 线性判别LDA

用于数据预处理中的降维，分类任务。

将特征空间投影到一个维度更小的K维子空间中，同时保持区分类别的信息。相同类别的点离得更近，不同的更远。有监督学习。

# 32. 主成分分析PCA

无监督学习。用于降维，每个维度独立无关。提取最有价值的信息(基于方差)。方差越大，效果越好，数据分得越开。

# 33. LSTM和RNN的区别

- RNN没有细胞状态，LSTM通过细胞状态记忆信息；
- RNN的激活函数只有tanh，LSTM通过输入门、遗忘门、输出门引入sigmoid函数，并结合tanh函数，添加求和操作；
- RNN有梯度爆炸问题；
- RNN只能处理短期依赖问题，LSTM既可以长期，也可以短期。

## 33.1 RNN梯度爆炸和消失的原因

**梯度消失**：一句话，RNN梯度消失是因为激活函数tanh函数的倒数在0到1之间，反向传播时更新前面时刻的参数时，当参数W初始化为小于1的数，则多个(tanh函数’ * W)相乘，将导致求得的偏导极小（小于1的数连乘），从而导致梯度消失。
**梯度爆炸**：当参数初始化为足够大，使得tanh函数的导数乘以W大于1，则将导致偏导极大（大于1的数连乘），从而导致梯度爆炸。

LSTM引入了细胞状态，在梯度前加了常量。

# 34. 激活函数

sigmoid：容易出现梯度消失，计算量大。
$$
f(x)=\frac 1{1+e^{-x}}
$$
tanh：收敛速度比sigmoid快，产生梯度消失。
$$
f(x)=\frac {1-e^{-2x}}{1+e^{-2x}}
$$
Relu：计算简单，避免梯度爆炸和梯度消失。
$$
f(x)=
\begin{cases}
x,& {x > 0}\\
0,& {x\le0}
\end{cases}
$$
全连接层：sigmoid，tanh

池化层：Relu

# 35. 池化层的作用（特征减少，参数减少）

降低卷积层对位置的敏感性，降低对空间降采样表示的敏感性。

maxpooling：减少卷积层参数误差造成估计均值的偏移，更多保留纹理信息；

meanpooling：减少邻域大小受限造成的估计方差增大，更多保留图像的背景信息。

