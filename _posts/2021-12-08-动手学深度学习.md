---
title: 动手学深度学习
date: 2021-12-08 20:00:00 +0800
categories: [算法]
tags: [算法]
pin: true
author: 王家乐

toc: false
comments: true
typora-root-url: ../../jlwang1998.github.io
math: false
mermaid: true
---

# 第2章 预备知识

## 2.1 数据操作

Numpy仅支持CPU计算，而其他的深度学习框架支持GPU计算。

张量表示数值组成的数组，具有一个轴的张量称为向量，两个轴的张量称为矩阵。

```python
# 使用arange创建一个行向量x
x = torch.arange(12)
```

```python
# 张量中元素的总数
x.numel()
```

reshape时，要生成(3,4)的矩阵，一直行数，可以将列数归为-1，张量自动推断维度，即reshape(3,-1)。

```
# 随机初始化
torch.randn()
```

对于任意具有相同形状的张量，常见的标准算术运算符（`+`、`-`、`*`、`/`和`**`）都可以被升级为按元素运算。

**广播机制：**当张量形状不同时，利用广播机制执行按元素操作。工作方式为：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。

```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a + b
```

```
# 输出
tensor([[0, 1],
        [1, 2],
        [2, 3]])
```

**节省内存：**运行Y = X + Y时，会重新分配内存，id(Y)与之前不一样。使用Y[:] = X + Y，id(Y)与之前的一样。

## 2.2 数据预处理

### 2.2.1 读取数据集

将数据集按行写入CSV文件

```python
import os

os.makedirs(os.path.join('..', 'data'), exist_ok=True) # 选择路径为data文件夹
data_file = os.path.join('..', 'data', 'house_tiny.csv') # 创建house_tiny.csv
with open(data_file, 'w') as f:
    f.write('NumRooms,Alley,Price\n')  # 列名
    f.write('NA,Pave,127500\n')  # 每行表示一个数据样本
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')
```

```
# 输出
   NumRooms Alley   Price
0       NaN  Pave  127500
1       2.0   NaN  106000
2       4.0   NaN  178100
3       NaN   NaN  140000
```

### 2.2.2 处理缺失值

```
# iloc进行位置索引
# fillna() 替换缺失值
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
print(inputs)
```

one-hot encoding：将离散型特征的每一种取值都看成一种状态，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20210907213247381.png" alt="image-20210907213247381" style="zoom:50%;" />

```python
# get_dummies() 转换为one-hot encoding
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)
```

```
# 输出
   NumRooms  Alley_Pave  Alley_nan
0       3.0           1          0
1       2.0           0          1
2       4.0           0          1
3       3.0           0          1
```

### 2.2.5 练习

```python
# 获取缺失值最多的列，并删除该列
nan_num = data.isna().sum(axis=0) #获取每列NAN的数
nan_maxid = nan_num.idxmax()
data = data.drop([nan_maxid],axis=1) #删除NAN最多的列
```

## 2.3 线性代数

### 2.3.5 张量算法的基本性质

两个矩阵的按元素乘法称为**哈达玛积**（Hadamard product）（数学符号⊙）。

### 2.3.6 降维

沿某个轴计算`A`元素的累积总和：

```
A = torch.arange(20).reshape(5,4)
A.cumsum(axis=0)
```

```
# 输出
A = tensor([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11],
           [12, 13, 14, 15],
           [16, 17, 18, 19]])
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
```

### 2.3.7 点积

```
torch.dot(x,y)
```

### 2.3.8 矩阵-向量积

```
np.dot(A,x)
torch.mv(A,x)
```

### 2.3.9 矩阵-矩阵乘法

```
torch.mm(A,B)
```

### 2.3.10 范数

L1范数：向量元素的绝对值之和

L2范数：向量元素平方和的平方根(常用)

```
u = torch.tensor([3.0, -4.0])
torch.abs(u).sum() #L1范数
torch.norm(u) #L2范数
```

矩阵X的**弗罗贝尼乌斯范数**：矩阵元素平方和的平方根

```
torch.norm(A)
```

## 2.4 微分

[字符串的格式化输出](https://blog.csdn.net/mnpy2019/article/details/98761643)

```
# 指定matplotlib输出svg图表以获得更清晰的图像
def use_svg_display():
	display.set_matplotlib_formats('svg')
```

[matplotlib中plt.rcParams用法(设置图像细节)](https://www.cnblogs.com/douzujun/p/10327963.html)

[pyplot.gca()：获取当前axes对象，利用plt.gca()进行坐标轴的移动](https://zhuanlan.zhihu.com/p/110976210)

[hasattr()用法：用于判断对象是否包含对应的属性。](https://blog.csdn.net/brucewong0516/article/details/82813219)

[isinstance() 函数来判断一个对象是否是一个已知的类型，类似 type()。](https://blog.csdn.net/laobai1015/article/details/90903410)

```
X = [X] # 将X列表化
X = [[]] * len(X) # 创建包含X长度个空列表的大列表
```

## 2.5 自动求导

```
# 求梯度
import torch
x = torch.arange(4.0)
x.requires_grad_(True)
x.grad
y = 2 * torch.dot(x, x)
y.backward() # 反向传播函数
x.grad
```

### 2.5.2 非标量变量的反向传播

```
# 对非标量调用`backward`需要传入一个`gradient`参数，该参数指定微分函数关于`self`的梯度。在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```

### 2.5.3 分离计算

计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，而不是`z=x*x*x`关于`x`的偏导数

```
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x # 把u看作常数

z.sum().backward()
x.grad == u
```

```
tensor([True, True, True, True])
```

## 2.6 概率

### 2.6.1 基本概率论

 将概率分配给一些离散选择的分布称为*多项分布*（multinomial distribution）。

```
import torch
from torch.distributions import multinomial
from d2l import torch as d2l

fair_probs = torch.ones([6]) / 6 # 骰子概率
multinomial.Multinomial(1, fair_probs).sample() # 随机数生成器
```

```
# 输出
tensor([0., 0., 0., 0., 0., 1.])
```

```
#看这些概率如何随着时间的推移收敛到真实概率。让我们进行500组实验，每组抽取10个样本
counts = multinomial.Multinomial(10, fair_probs).sample((500,)) #500组
cum_counts = counts.cumsum(dim=0) # cumsum累加，dim=0指定行，列不变，行变，从第一行到最后一行的累加
estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True) #计算每组概率

d2l.set_figsize((6, 4.5))
for i in range(6):
    d2l.plt.plot(estimates[:, i].numpy(),
                 label=("P(die=" + str(i + 1) + ")"))
d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')
d2l.plt.gca().set_xlabel('实验次数')  #plt.gca()坐标轴移动
d2l.plt.gca().set_ylabel('估算概率')
d2l.plt.legend();
```

```
# 解决在Python的matplotlib.pyplot图表中显示中文
from pylab import *
mpl.rcParams['font.sans-serif'] = ['SimHei']
mpl.rcParams['axes.unicode_minus'] = False
```

## 2.7 查阅文档

```
help(torch.ones)
# 在Jupyter记事本中，我们可以使用?在另一个窗口中显示文档。例如，list?将创建与help(list)几乎相同的内容，并在新的浏览器窗口中显示它。此外，如果我们使用两个问号，如list??，将显示实现该函数的Python代码。
```

# 第3章 线性回归网络

## 3.1 线性回归

损失函数：
$$
L(w,b)=\frac 1n\sum_{i=1}^nl^{(i)}(w,b)=\frac 1n\sum_{i=1}^n\frac 12(w^⊤x^{(i)}+b−y^{(i)})^2
$$
小批量梯度下降：
$$
w=w-\frac {\eta}{|B|}\sum_{i\in B}x^{(i)}(w^⊤x^{(i)}+b-y^{(i)})
$$

$$
b=b-\frac {\eta}{|B|}\sum_{i\in B}(w^⊤x^{(i)}+b-y^{(i)})
$$

 |B|表示每个小批量中的样本数，这也称为*批量大小*（batch size）。η表示*学习率*（learning rate）。

批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的参数称为*超参数*（hyperparameter）。 调参（hyperparameter tuning）是选择超参数的过程。

```
# 定义定时器
class Timer:  #@save
    """记录多次运行时间。"""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        """启动计时器。"""
        self.tik = time.time()

    def stop(self):
        """停止计时器并将时间记录在列表中。"""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """返回平均时间。"""
        return sum(self.times) / len(self.times)

    def sum(self):
        """返回时间总和。"""
        return sum(self.times)

    def cumsum(self):
        """返回累计时间。"""
        return np.array(self.times).cumsum().tolist()
```

## 3.2 线性回归的从零开始实现

使用线性模型参数`w=[2,-3.4]^T`、`b=4.2`和噪声项`ϵ`生成数据集及其标签：
$$
y=Xw+b+\in
$$

```
def synthetic_data(w, b, num_examples):  
    """生成 y = Xw + b + 噪声。"""
    X = torch.normal(0, 1, (num_examples, len(w)))#均值为零，方差为1的随机数
    y = torch.matmul(X, w) + b #torch的矩阵乘法
    y += torch.normal(0, 0.01, y.shape) #均值为零，方差为0.01的随机噪音
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

`features`中的每一行都包含一个二维数据样本，`labels`中的每一行都包含一维标签值（一个标量）。

```
d2l.set_figsize()
d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1);
# 第一个“1”表示列，第二个“1”表示散点大小
```

### 3.2.2 读取数据集

定义一个`data_iter`函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为`batch_size`的小批量。每个小批量包含一组特征和标签。

```
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples)) #索引
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices) #打乱下标
    for i in range(0, num_examples, batch_size): #从0开始，到num_examples结束，每次跳batch_size个距离
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
         #yield就是返回一个值，并且记住这个返回的位置，下次迭代就从这个位置开始
```

```
batch_size = 10
#给定一个样本标号，每次随机从里面选取1个样本返回出来
for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
```

## 3.3 线性回归的简介实现

```python
#生成数据集
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

#读取数据集
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器。"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)
next(iter(data_iter)) #将data_iter用iter()函数转为迭代器，再使用next()函数从迭代器中获取数据

#定义模型
# `nn` 是神经网络的缩写
from torch import nn
net = nn.Sequential(nn.Linear(2, 1))#第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1

#初始化模型参数
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

#定义损失函数
loss = nn.MSELoss()#平方L2范数，返回所有样本损失的平均值

#定义优化算法
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

#训练
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y) #通过调用 `net(X)` 生成预测并计算损失 `l`（正向传播）
        trainer.zero_grad() #trainer优化器，先把梯度清零
        l.backward() #反向传播计算梯度
        trainer.step() # 调用优化器更新模型参数
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

#比较生成数据集的真实参数和通过有限数据训练获得的模型参数
w = net[0].weight.data
print('w的估计误差：', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('b的估计误差：', true_b - b)
```

### TensorDataset

对给定的`tensor`数据(样本和标签)，将它们包装成`dataset`。也就是生成数据集

### DataLoader

数据加载器，组合数据集和采样器，并在数据集上提供单进程或多进程迭代器。它可以对我们上面所说的数据集`Dataset`作进一步的设置。

### 训练

步骤：

- 通过调用 `net(X)` 生成预测并计算损失 `l`（正向传播）。
- 通过进行反向传播来计算梯度。
- 通过调用优化器来更新模型参数。

## 3.4 softmax回归

交叉熵损失：用下式定义损失l，它是所有标签分布的预期损失值。
$$
l(y,\hat x)=-\sum^q_{j=1}y_ilog\hat y_j
$$

### 3.4.7 信息论基础

惊异：当我们赋予一个事件较低的概率时，我们的惊异会更大。用下式量化惊异：
$$
log\frac {1}{P(j)}=-logP(j)
$$
熵：知道真实概率的人所经历的惊异程度。
$$
H[P]=\sum_j-P(j)logP(j)
$$
交叉熵：交叉熵从P到Q，记为H(P,Q)，是主观概率为Q的观察者在看到根据概率P实际生成的数据时的预期惊异。当P=Q时，交叉熵达到最低。在这种情况下，从P到Q的交叉熵是H(P,P)=H(P)。

## 3.5 图像分类数据集

```
import torch
import torchvision #pytorch对于计算机视觉模型实现的一些库
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l

d2l.use_svg_display() #使用svg来显示图片，清晰度会高一点

#读取数据集
# 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式
# 并除以255使得所有像素的数值均在0到1之间
#下载到上一级目录的data下面
#train=True，表示下载的是训练数据集
#transform=trans，表示下载的要转为pytorch的tensor，而不是一堆图片
#download=True 意思是默认从网上下载
trans = transforms.ToTensor()
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
#测试数据集，验证模型的好坏
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)
```

```
len(mnist_train), len(mnist_test)
 
#下载成功后输出发现，训练数据集有60000张图片，测试数据集有10000张图片
 
#输出结果
 
(60000, 10000)
```

```
# fashionmnist数据集同时包含图形和标签，第二个零表示取图片，如果是【1】则是标签
 
mnist_train[0][0].shape
#第一个零：就是第零个样例
#第二个零：表示取图片，如果是1就表示标签
 
#输出1表示是黑白图片
#28，28分表表示长和宽<br><br>#输出结果
```

```
#以下函数用于在数字标签索引及其文本名称之间进行转换
def get_fashion_mnist_labels(labels): 
    """返回Fashion-MNIST数据集的文本标签。"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]
    
#创建一个函数来可视化这些样本
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
    """Plot a list of images."""
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten() #将axes由n*m的Axes组展平成1*nm的Axes组
    #在for循环里enumerate()函数是一个枚举函数，用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标。
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            # 图片张量
            ax.imshow(img.numpy())
        else:
            # PIL图片
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes
    
#构造了pytorch数据集之后放在DateLoader中，指定一个batch_size
#next()拿到第一个小批量。batch_size：批量大小
X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
#2行9列
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));#y是一个数值的标号

#读取小批量
batch_size = 256

def get_dataloader_workers():  #@save
    """使用4个进程来读取数据。"""
    return 4
#shuffle=True表明需要随机，打乱顺序
#训练集需要打乱顺序，测试集不用打乱顺序
#num_workers表明要给多少进程
train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                             num_workers=get_dataloader_workers())
#读取训练数据所需的时间                             
timer = d2l.Timer()
for X, y in train_iter:
    continue
f'{timer.stop():.2f} sec'
```

```
#整合所有组件
def load_data_fashion_mnist(batch_size, resize=None):  #@save
    """下载Fashion-MNIST数据集，然后将其加载到内存中。"""
    trans = [transforms.ToTensor()]
    if resize: #resize调整图像大小
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    #下载数据集
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
    #返回小批量
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))
```

```
#通过指定resize参数来测试load_data_fashion_mnist函数的图像大小调整功能
#32表示为batch_size的大小，resize表示重新调整的图片大小
train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
for X, y in train_iter:
    print(X.shape, X.dtype, y.shape, y.dtype)
    break
 
#输出结果
 
torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64
```

## 3.6 softmax回归的从零开始实现

```
import torch
from IPython import display
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### 3.6.1 初始化模型参数

```
num_inputs = 784 # 样本的长和宽都是28，将其展平为空间向量，长度变为784
num_outputs = 10 # 数据集类别，也是输出数

#size=(num_inputs, num_outputs)；行数等于输入的个数，列数等于输出的个数
#requires_grad=True表明要计算梯度
#权重
#在这里W被定义为一个784*10的矩阵
W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)

#偏离
b = torch.zeros(num_outputs, requires_grad=True)
```

### 3.6.2 定义softmax操作

softmax由三个步骤组成：

（1）对每个项求幂（使用`exp`）；

（2）对每一行求和（小批量中每个样本是一行），得到每个样本的归一化常数；

（3）将每一行除以其归一化常数，确保结果的和为1。 
$$
softmax(X)_{ij}=\frac {exp(X_{ij})}{\sum_kexp(X_{ik})}
$$

```
# 定义softmax()函数
 
def softmax(X):
    #torch.exp()对每个元素做指数计算
    X_exp = torch.exp(X)
     
    #对矩阵的每一行求和，重新生成新的矩阵
    partition = X_exp.sum(1, keepdim=True)
     
     
    # X_exp / partition：每一行代表一个样本，行中的每个数据代表在该类别的概率
    return X_exp / partition  # 这里应用了广播机制
```

### 3.6.3 定义模型

```
#在将数据传递到我们的模型之前，我们使用reshape函数将每张原始图像展平为向量。
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
```

### 3.6.4 定义损失函数

下面，我们创建一个数据`y_hat`，其中包含2个样本在3个类别的预测概率，它们对应的标签`y`。 有了`y`，我们知道在第一个样本中，第一类是正确的预测，而在第二个样本中，第三类是正确的预测。 然后使用`y`作为`y_hat`中概率的索引，我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。

```
y = torch.tensor([0, 2])
 
# y_hat为预测值，此次是对两个样本做预测值
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
 
# 对第0个样本，拿出y0的数据=0.1；对第1个样本，y的值为2，对应y_hat的第三类0.5
# 第一个参数[0,1]表示样本号，第二个参数y表示在第一个参数确定的样本中取数的序号
y_hat[[0, 1], y]
 
#输出结果
 
tensor([0.1000, 0.5000])
```

```
#实现交叉熵损失函数
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y]) 
    #对于每一行range(len(y_hat))：生成一个从零开始一直到len(y_hat)的向量，参考上面的y_hat[[0,1],y]

cross_entropy(y_hat, y)
```

### 3.6.5 分类准确率

```
def accuracy(y_hat, y): 
    """计算预测正确的数量。"""
     # len是查看矩阵的行数
    # y_hat.shape[1]就是去列数，y_hat
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)  #用argmax获得每行中最大元素的索引来获得预测类别
    #将y_hat转换为y的数据类型然后作比较
    #使用cmp函数存储bool类型
    cmp = y_hat.type(y.dtype) == y
    #将cmp转化为y的数据类型再求和——得到找出来预测正确的类别数
    return float(cmp.type(y.dtype).sum())
```

```
accuracy(y_hat, y) / len(y)

#输出结果
 
0.5
```

```
# 评估任意模型的准确率
 
def evaluate_accuracy(net, data_iter):  #@save
    """计算在指定数据集上模型的精度。"""
     
     
    # isinstance()：判断一个对象是否是一个已知的类型
    # 判断输入的net模型是否是torch.nn.Module类型
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式（不用计算梯度）
         
    metric = Accumulator(2)  # 存储正确预测数、预测总数
     
    # 每次从迭代器中拿出一个x和y
    for X, y in data_iter:
         
        # 1、net(X)：X放在net模型中进行softmax操作
        # 2、accuracy(net(X), y)：再计算所有预算正确的样本数
        # numel()函数：返回数组中元素的个数，在此可以求得样本数
        metric.add(accuracy(net(X), y), y.numel())
         
    #metric[0]:分类正确的样本数，metric[1]:总的样本数
    return metric[0] / metric[1]
 
evaluate_accuracy(net, test_iter)
 
#输出结果
0.1001
```

<!--`Accumulator`是一个实用程序类，用于对多个变量进行累加。 在上面的`evaluate_accuracy`函数中，我们在`Accumulator`实例中创建了2个变量，用于分别存储正确预测的数量和预测的总数量。当我们遍历数据集时，两者都将随着时间的推移而累加。-->

```
class Accumulator:  #@save
    """在`n`个变量上累加。"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

### 3.6.6 训练

```
def train_epoch_ch3(net, train_iter, loss, updater):  
    """训练模型一个迭代周期（定义见第3章）。"""<br>
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()#告诉pytorch我要计算梯度
         
    # 训练损失总和、训练准确度总和、样本数
    # Accumulator 是一个实用程序类，用于对多个变量进行累加
    #在此创建了一个长度为三的迭代器，用于累加信息
    metric = Accumulator(3)
     
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()#先把梯度设置为零
            l.backward() #计算梯度
            updater.step()#自更新
             
            metric.add(
                float(l) * len(y), accuracy(y_hat, y),
                y.size().numel())
        else:
            # 使用定制的优化器和损失函数
            # 如果是自我实现的话，l出来就是向量，我们先做求和，再求梯度
            l.sum().backward()
            updater(X.shape[0])
            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练准确率
    # metric[0]就是损失样本数目；metric[1]是训练正确的样本数；metric[2]是总的样本数
    return metric[0] / metric[2], metric[1] / metric[2]
```

定义一个在动画中绘制数据的实用程序类。（不懂）

```
class Animator:  #@save
    """在动画中绘制数据。"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
```

```
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """训练模型（定义见第3章）。"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
     
     
    # num_epochs：训练次数
    for epoch in range(num_epochs):
        #train_epoch_ch3：训练模型，返回准确度和错误度
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        #在测试数据集上评估精度
        test_acc = evaluate_accuracy(net, test_iter)
         
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```

```
#使用小批量随机梯度下降来优化模型的损失函数
lr = 0.1 #学习率

def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)
```

```
#迭代周期设为10
num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
```

### 3.6.7 预测

```
def predict_ch3(net, test_iter, n=6):  #@save
    """预测标签（定义见第3章）。"""
    for X, y in test_iter:
        break
    #真实标号
    trues = d2l.get_fashion_mnist_labels(y)
    #预测标号
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true + '\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])
 
predict_ch3(net, test_iter)
```

## 3.7 softmax回归的简洁实现

```
import torch
from torch import nn
from d2l import torch as d2l


batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### 3.7.1 初始化模型参数

```
# PyTorch不会隐式地调整输入的形状。因此，我们在线性层前定义了展平层（flatten），来调整网络输入的形状
# Flatten()将任何维度的tensor转化为2D的tensor
# Linear(784, 10)定义线性层，输入是784，输出是10
# nn.Sequential:一个时序容器。Modules 会以他们传入的顺序被添加到容器中
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))
 
def init_weights(m):
    if type(m) == nn.Linear:
        # normal_正态分布
        nn.init.normal_(m.weight, std=0.01)
 
# apply()用途：当一个函数的参数存在于一个元组或者一个字典中时，用来间接的调用这个函数，并将元组或者字典中的参数按照顺序传递给参数
# 意思也就是在每层跑一下该函数
net.apply(init_weights);
```

```
#在交叉熵损失函数中传递未归一化的预测，并同时计算softmax及其对数
loss = nn.CrossEntropyLoss()
```

### 3.7.3 优化算法

```
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
```

### 3.7.4 训练

```
#调用3.6定义的训练函数来训练模型
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

# 第4章 多层感知机

## 4.1 多层感知机

多层感知机(MLP)：也称为人工神经网络(ANN)，输入层与输出层之间含有隐藏层。

从线性到非线性：在仿射变换之后，对每个隐藏层单元应用非线性的激活函数σ。
$$
H=\sigma(XW^{(1)}+b^{(1)})
$$

$$
O=HW^{(2)}+b^{(2)}
$$

**ReLU函数：**
$$
ReLU(X)=max(x,0)
$$

```
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

```
#求导
#当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1
y.backward(torch.ones_like(x), retain_graph=True) #torch.ones_like(x),生成x形状的元素都为1的向量
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))
```

**sigmoid函数：**
$$
sigmoid(x)=\frac 1{1+exp(-x)}
$$

```
y = torch.sigmoid(x)
```

sigmoid导数计算公式：
$$
\frac d{dx}sigmoid(x)=sigmoid(x)(1-sigmoid(x))
$$

```
#计算导数时，先清空之前的梯度
x.grad.data.zero_()
```

tanh函数：
$$
tanh(x)=\frac {1-exp(-2x)}{1+exp(-2x)}
$$
tanh导数计算公式：
$$
\frac d{dx}tanh(x)=1-tanh^2(x)
$$

## 4.2 多层感知机的从零开始实现

```
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### 4.2.1 初始化模型参数

通常，我们选择2的若干次幂作为层的宽度。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。

```
# num_inputs：输入；num_outputs：输出；输入与输出是由数据决定的
#num_hiddens：人为决定，隐藏层的大小
num_inputs, num_outputs, num_hiddens = 784, 10, 256
 
 
# nn.Parameter()函数的目的就是让该变量在学习的过程中不断的修改其值以达到最优化。
# nn.Parameter()参考：https://www.jianshu.com/p/d8b77cc02410
# torch.randn()：返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数
 
W1 = nn.Parameter(
    torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)#*0.01意思是方差变为0.01
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(
    torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
```

### 4.2.2 激活函数

```
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
```

### 4.2.3 模型

```
def net(X):
    X = X.reshape((-1, num_inputs)) #将二维转化为num_inputs
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    return (H@W2 + b2)
```

### 4.2.4 损失函数

```
loss = nn.CrossEntropyLoss()
```

### 4.2.5 训练

多层感知机的训练函数与softmax训练函数一致

```
# torch.optim.SGD：实现随机梯度下降，params: 待优化参数的iterable或者是定义了参数组的dict
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
# 对学习的模型进行评估
d2l.predict_ch3(net, test_iter)
```

## 4.3 多层感知机的简洁实现

```
import torch
from torch import nn
from d2l import torch as d2l

#模型

# 因为图片是一个3D的东西，然后使用nn.Flatten()为二维
# nn.Linear(784, 256)线性层，输入为784，输出为256
# nn.Linear(256, 10)线性层，输入为256，输出为10
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),
                    nn.Linear(256, 10))
 
def init_weights(m):
    if type(m) == nn.Linear:
        #从给定均值和标准差的正态分布(mean, std)中生成值，填充输入的张量或变量
        nn.init.normal_(m.weight, std=0.01)
 
# net.apply：会先遍历子线性层，再遍历父线性层
net.apply(init_weights);

#训练过程

# num_epochs：表示跑多少轮
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss()# 损失函数
# 更新数据
trainer = torch.optim.SGD(net.parameters(), lr=lr)
# 下载测试数据集和训练数据集
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)nn.init.normal_
# 直接调用d2l包的train_ch3函数
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## 4.4 模型选择、欠拟合和过拟合

### 4.4.1 训练误差和泛化误差

训练误差：模型在训练集的误差；

泛化误差：将模型应用到无限多的数据样本时的误差。现实中只能用测试集误差来估计泛化误差。

### 4.4.2 模型选择

模型选择：评估几个候选模型后选择出最终的模型，为了确定候选模型中的最佳模型，我们通常会使用验证集。

当训练数据较少时，采用**K折交叉验证**，即将原始训练数据分为K个不重叠的子集，每次在K-1个子集进行训练，在剩余的一个子集进行验证，执行K次模型训练和验证。最后，通过对KK次实验的结果取平均来估计训练和验证误差。

### 4.4.3 欠拟合还是过拟合？

欠拟合：训练误差和验证误差都很大，而且训练误差与测试误差很接近。

过拟合：训练误差明显低于验证误差。

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20210928211718626.png" alt="模型复杂度对欠拟合和过拟合的影响" style="zoom:67%;" />

<center>模型复杂度对欠拟合和过拟合的影响</center>

### 4.4.4 多项式回归

```
import math
import numpy as np
import torch
from torch import nn
from d2l import torch as d2l
```

给定三阶多项式来生成训练和测试数据的标签：
$$
y=5+1.2x-3.4\frac {x^2}{2!}+5.6\frac {x^3}{3!}+\in,where\in \sim N(0,0.01^2)
$$


```
# 生成数据集
max_degree = 20  # 多项式的最大阶数
n_train, n_test = 100, 100  # 训练和测试数据集大小
true_w = np.zeros(max_degree)  # 分配大量的空间
true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])

# features初始化为x
features = np.random.normal(size=(n_train + n_test, 1))#随机生成正态分布，200行1列
np.random.shuffle(features)
poly_features = np.power(features, np.arange(max_degree).reshape(1, -1)) #reshape(1,-1)为1行
for i in range(max_degree):
    poly_features[:, i] /= math.gamma(i + 1)  # `gamma(n)` = (n-1)!
# `labels`的维度: (`n_train` + `n_test`,)
labels = np.dot(poly_features, true_w) #向量
labels += np.random.normal(scale=0.1, size=labels.shape)#高斯噪声
```

```
# NumPy ndarray转换为tensor
true_w, features, poly_features, labels = [torch.tensor(x, dtype=
    d2l.float32) for x in [true_w, features, poly_features, labels]]

features[:2], poly_features[:2, :], labels[:2]
```

```
#定义损失函数
def evaluate_loss(net, data_iter, loss):  #@save
    """评估给定数据集上模型的损失。"""
    metric = d2l.Accumulator(2)  # 损失的总和, 样本数量
    for X, y in data_iter:
        out = net(X)
        y = y.reshape(out.shape)
        l = loss(out, y)
        metric.add(l.sum(), l.numel())
    return metric[0] / metric[1]
```

```
#定义训练损失函数
def train(train_features, test_features, train_labels, test_labels,
          num_epochs=400):
    loss = nn.MSELoss()#均方损失函数，(x-y)^2
    input_shape = train_features.shape[-1]#返回的是train_features的shape
    # 不设置偏置，因为我们已经在多项式特征中实现了它
    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))
    batch_size = min(10, train_labels.shape[0])
    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),
                                batch_size)
    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),
                               batch_size, is_train=False)
    trainer = torch.optim.SGD(net.parameters(), lr=0.01)
    #在动画中绘制数据的实用程序类
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        #20轮绘制一次
        if epoch == 0 or (epoch + 1) % 20 == 0:
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
                                     evaluate_loss(net, test_iter, loss)))
    print('weight:', net[0].weight.data.numpy())
```

```
#三阶多项式函数拟合

# 从多项式特征中选择前4个维度，即 1, x, x^2/2!, x^3/3!
train(poly_features[:n_train, :4], poly_features[n_train:, :4],
      labels[:n_train], labels[n_train:])
```

```
#线性函数拟合（欠拟合）

# 从多项式特征中选择前2个维度，即 1, x
train(poly_features[:n_train, :2], poly_features[n_train:, :2],
      labels[:n_train], labels[n_train:])
```

```
#高阶多项式拟合（过拟合）

# 从多项式特征中选取所有维度
train(poly_features[:n_train, :], poly_features[n_train:, :],
      labels[:n_train], labels[n_train:], num_epochs=1500)
```

## 4.5 权重衰减

### 4.5.1 范数与权重衰减

权重衰减也称为L2正则化，目的是为了让权重衰减到更小的值，在一定程度上减少模型过拟合的问题。

思考：L2正则化项有让w变小的效果，但是为什么w变小可以防止过拟合呢？

原理：（1）从模型的复杂度上解释：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合更好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。（2）从数学方面的解释：过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。

![img](https://img-blog.csdn.net/20180630161619869?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Byb2dyYW1fZGV2ZWxvcGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

损失函数：
$$
L(w,b)=\frac 1n\sum_{i=1}^n\frac 12(w^⊤x^{(i)}+b−y^{(i)})^2+\frac {\lambda}{2}||w||^2
$$

$$
w=(1-\eta \lambda) w-\frac {\eta}{|B|}\sum_{i\in B}x^{(i)}(w^⊤x^{(i)}+b-y^{(i)})
$$

### 4.5.2 高维线性回归

$$
y=0.05+\sum ^d_{i=1}0.01x_i+\in,where \in \sim N(0,0.01^2)
$$

维数d=200，训练集个数为20

```
# 生成数据
import torch
from torch import nn
from d2l import torch as d2l

n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5
true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05 # true_w：是一个200*1的矩阵，矩阵内容全为0.01
# synthetic_data：合成数据集
# train_data：是一个包含20个样本的训练数据集
train_data = d2l.synthetic_data(true_w, true_b, n_train)
train_iter = d2l.load_array(train_data, batch_size)
test_data = d2l.synthetic_data(true_w, true_b, n_test)
test_iter = d2l.load_array(test_data, batch_size, is_train=False)
```

### 4.5.3 从零开始实现

```
# 初始化模型参数
def init_params():
    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return [w, b]
```

```
# 定义L2范数惩罚
def l2_penalty(w):
    return torch.sum(w.pow(2)) / 2
```

```python
# 训练代码实现
def train(lambd):
    w, b = init_params()
    # linreg：线性回归
    # squared_loss：平方损失函数
    # lambda函数也称为匿名函数（在这里相当于定义模型） https://www.cnblogs.com/curo0119/p/8952536.html
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    # num_epochs：表示迭代次数；lr=0.003：表示学习率
    num_epochs, lr = 100, 0.003
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            with torch.enable_grad():#类似requires_grad,https://www.jianshu.com/p/1cea017f5d11
                # 增加了L2范数惩罚项，广播机制使l2_penalty(w)成为一个长度为`batch_size`的向量。
                l = loss(net(X), y) + lambd * l2_penalty(w)
            l.sum().backward()
            d2l.sgd([w, b], lr, batch_size)
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数是：', torch.norm(w).item()) #torch.norm()求范数,.item()是输出tensor的元素
```

```
# 忽略正则化训练
train(lambd=0)

#输出
w的L2范数是： 14.599337577819824
```

```python
# 使用正则化
train(lambd=3)

#输出
w的L2范数是： 0.3641825020313263
```

### 4.5.4 简洁实现

```python
'''
在实例化优化器时直接通过weight_decay指定weight decay超参数。默认情况下，PyTorch同时衰减权重和偏移。这里我们只为权重设置了weight_decay，所以bias参数 b 不会衰减。
'''
def train_concise(wd):
    net = nn.Sequential(nn.Linear(num_inputs, 1))
    for param in net.parameters():
        param.data.normal_() # L2范数
    loss = nn.MSELoss()
    num_epochs, lr = 100, 0.003
    # 偏置参数没有衰减。
    trainer = torch.optim.SGD([
        {"params":net[0].weight,'weight_decay': wd},
        {"params":net[0].bias}], lr=lr)
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            with torch.enable_grad():
                trainer.zero_grad()
                l = loss(net(X), y)
            l.backward()
            trainer.step()
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数：', net[0].weight.norm().item())
```

## 4.6 Dropout

Dropout：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。可以解决过拟合问题。https://blog.csdn.net/program_developer/article/details/80737724

### 4.6.4 从零开始实现

要实现单层的dropout函数，我们必须从伯努利（二元）随机变量中提取与我们的层的维度一样多的样本，其中随机变量以概率1−p取值1（保持），以概率p取值0（丢弃）。实现这一点的一种简单方式是首先从均匀分布U[0,1]中抽取样本。那么我们可以保留那些对应样本大于p的节点，把剩下的丢弃。

```python
import torch
from torch import nn
from d2l import torch as d2l

#实现单层的dropout函数
def dropout_layer(X, dropout):
    # assert：断言。表示程序只有在符合以下条件下才能正常运行
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃。
    if dropout == 1:
        return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留。
    
    # torch.randn()随机生成了一个和X.shape相同的mask，均值为“0”，方差为“1”
    # 且大于dropout的地方设置为1，其他地方设置为0
    if dropout == 0:
        return X
    # mask * X / (1.0 - dropout)没有丢弃的输入部分的值会因为表达式的分母存在而改变，而训练数据的标签还是原来的值
    # mask * X 不是矩阵的乘法，而是哈达马积，若A=(aij)和B=(bij)是两个同阶矩阵，若cij=aij×bij,则称矩阵C=(cij)为A和B的哈达玛积
    mask = (torch.Tensor(X.shape).uniform_(0, 1) > dropout).float()
    return mask * X / (1.0 - dropout)
```

```python
# 定义模型参数
# 定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256
```

```python
# 我们可以分别为每一层设置丢弃概率。 一种常见的技巧是在靠近输入层的地方设置较低的丢弃概率。

dropout1, dropout2 = 0.2, 0.5

class Net(nn.Module):
    # self是实例化的对象，
    #  super(Net, self).__init__()：子类把父类的__init__()放到自己的__init__()当中，这样子类就有了父类的__init__()的     #  那些东西
    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,
                 
                 is_training = True):
        super(Net, self).__init__()
        self.num_inputs = num_inputs
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs, num_hiddens1)
        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)
        self.lin3 = nn.Linear(num_hiddens2, num_outputs)
        self.relu = nn.ReLU()

    def forward(self, X):
        # H1：第一个隐藏层的输出
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        # 只有在训练模型时才使用dropout
        if self.training == True:
            # 在第一个全连接层之后添加一个dropout层
            H1 = dropout_layer(H1, dropout1)
        # 把H1作为输入进第二个隐藏层
        H2 = self.relu(self.lin2(H1))
        if self.training == True:
            # 在第二个全连接层之后添加一个dropout层
            H2 = dropout_layer(H2, dropout2)
        # 把 H2 作为输入传递给输出层   
        out = self.lin3(H2)
        return out


net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)
```

```python
# 训练和测试

num_epochs, lr, batch_size = 10, 0.5, 256
loss = nn.CrossEntropyLoss()
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## 4.7 正向传播、反向传播和计算图

正向传播：按顺序计算和储存神经网络中每层的结果。

反向传播：计算梯度的方法。按相反的顺序从输出层到输入层遍历网络。

## 4.8 数值稳定性和模型初始化

### 4.8.1 梯度消失和梯度爆炸

梯度消失：参数更新过小，在每次更新时几乎不会移动，导致无法学习。如sigmoid激活函数，当输入过大或过小时，梯度接近于0。采用ReLU函数缓解梯度消失问题，加速收敛。

梯度爆炸：参数更新过大，破坏了模型的稳定收敛。

### 4.8.2 参数初始化

Xavier初始化：
$$
\frac 12(n_{in}+n_{out})\sigma^2=1
$$

$$
n_{in}输入层的数量，n_{out}输出层的数量
$$

## 4.9 环境和分布偏移

### 4.9.1 分布偏移的类型

当数据分布发生变化时，要求算法实时更新，动态调整。

协变量偏移：输入的分布改变，但标签函数不变。如训练集是真实的猫狗图像，测试集却是卡通图像。

标签偏移：标签边缘概率P(y)改变，但类别条件分布P(y|x)在不同的领域之间保持不变。

概念偏移：类别条件分布P(y|x)发生变化，比如机器翻译系统在不同地区翻译的语言不同。

### 4.9.3 分布偏移纠正

协变量偏移：logistic回归。

概念偏移：用新数据执行更新步骤。

## 4.10 实战Kaggle比赛：预测房价

Python **split()** 通过指定分隔符对字符串进行切片，如果参数 num 有指定值，则分隔 num+1 个子字符串。

```python
str.split(str="", num=string.count(str))
```

```python
f.read(1048576) # 每次读取1048576字节，即1MB
```

```python
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index #提取出所有数字的列
```

```python
all_features = pd.get_dummies(all_features, dummy_na=True)  #pandas实现one-hot编码
```

```python
torch.clamp(input, min, max, out=None)
# 将输入input张量每个元素的夹紧到区间 [min,max][min,max]，并返回结果到一个新张量。
```

# 5. 深度学习计算

## 5.1 层和块

```python
super().__init__()  # 调用父类的__init__函数
```

```python
# 混合搭配组合块
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

## 5.2 参数管理

```python
#  内置初始化
#  将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0。
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
```

```python
#  将所有参数初始化为给定的常数（比如1）
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)
net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
```

```python
#  自定义初始化
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5  # 将绝对值>=5的数保留，其余置为0

net.apply(my_init)
net[0].weight[:2]
```

## 5.4 自定义层

### 5.4.1 不带参数的层

```python
class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
```

### 5.4.2 带参数的层

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))  #初始化
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```

## 5.5 读写文件

```python
#  保存张量
x = torch.arange(4)
torch.save(x, 'x-file')
x2 = torch.load('x-file')  #  加载张量
#  保存张量列表
y = torch.zeros(4)
torch.save([x, y],'x-files')
#  保存字典
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
```

```
#  加载和保存模型参数
#  创建多层感知机
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
#  保存模型参数
torch.save(net.state_dict(), 'mlp.params')
#  读取模型参数
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()
```

## 5.6 GPU

```python
!nvidia-smi #  查看GPU
```

```python
torch.cuda.device('cuda:{i}')  #  表示第i块GPU，(i从0开始)
```

```python
torch.cuda.device_count() #  查看GPU数量
```

张量默认在CPU上创建，在GPU上存储张量

```python
X = torch.ones(2, 3, device=try_gpu())
Y = torch.rand(2, 3, device=try_gpu(1))
```

```python
#  复制
Z = X.cuda(1)  #  将X复制到cuda1
print(X)
print(Z)
```

```
#  将神经网络模型放在GPU上
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())
```

# 6. 卷积神经网络

## 6.1 从全连接层到卷积

平移不变性：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。

局部性：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，在后续神经网络，整个图像级别上可以集成这些局部特征用于预测。

卷积：
$$
(f*g)(X)=\int f(Z)g(X-Z)dZ
$$

## 6.2 图像卷积

### 6.2.1 互相关运算

互相关运算：也就是卷积层

卷积层输出大小：
$$
(n_h-k_h+1)×(n_w-k_w+1)
$$

```python
import torch
from torch import nn
from d2l import torch as d2l


def corr2d(X, K):  #@save
    """计算二维互相关运算。"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
```

### 6.2.2 卷积层

```python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

### 6.2.4 学习卷积核

我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较 `Y` 与卷积层输出的平方误差，然后计算梯度来更新卷积核。

```python
# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），
# 其中批量大小和通道数都为1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # 迭代卷积核
    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad  ## 3e-2是学习率
    if (i + 1) % 2 == 0:
        print(f'batch {i+1}, loss {l.sum():.3f}')
```

### 6.2.6 特征映射和感受野

特征映射：输出的卷积层。

感受野：卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点对应输入图上的区域。

## 6.3 填充和步幅

### 6.3.1 填充

在应用多层卷积时，我们常常丢失边缘像素。解决这个问题的简单方法即为*填充*（padding），在输入图像的边界填充元素（通常填充元素是 0）。

添加p<sub>h</sub>行填充（一半在顶部，一半在底部），p<sub>w</sub>列填充（一半在左侧，一半在右侧），则输出形状为
$$
(n_h-k_h+p_h+1)×(n_w-k_w+p_w+1)
$$
卷积核的高度和宽度通常为奇数，例如1，3，5，7。选择奇数的好处是，保持空间维度的同时，可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。

当卷积内核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。在如下示例中，我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。

```python
conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape
```

### 6.3.2 步幅

步幅：每次滑动元素的数量。

当垂直步幅为s<sub>h</sub>、水平步幅为s<sub>w</sub>时，输出的形状为：
$$
[(n_h-k_h+p_h+s_h)/s_h]×[(n_w-k_w+p_w+s_w)/s_w]
$$

## 6.4 多输入多输出通道

### 6.4.1 多输入通道

假设输入通道数为C<sub>i</sub>，则卷积核的通道数也为C<sub>i</sub>，对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和，得到输出的二维张量。

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211101113827183.png" alt="image-20211101113827183" style="zoom: 80%;" />

### 6.4.2 多输出通道

假设输入通道数为C<sub>i</sub>，输出通道数为C<sub>0</sub>，则卷积核的形状为C<sub>0</sub>×C<sub>i</sub>×k<sub>h</sub>×k<sub>w</sub>。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。

### 6.4.3 1×1卷积

用来升/降维，可以看成全连接层。

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211101145444402.png" alt="image-20211101145444402" style="zoom:80%;" />

## 6.5 汇聚层（池化层）

最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。

池化层：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。

### 6.5.1 最大汇聚成和平均汇聚层

与卷积层类似，固定窗口滑动，通常计算池化窗口中所有元素的最大值或平均值，称为最大汇聚层(maximun pooling)和平均汇聚层(average pooling)。

![image-20211101153356951](C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211101153356951.png)

### 6.5.3 多个通道

汇聚层在每个输入通道单独运算，而不是像卷积层一样在通道上对输入进行汇总。

因此，汇聚层的输出通道数和输入通道数相同。

## 6.6 卷积神经网络(LeNet)

### 6.6.1 LeNet

LeNet：由两个卷积块，三个全连接层组成。每个卷积块包含一个卷积层、一个sigmoid激活函数和平均池化层。

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211104171848174.png" alt="image-20211104171848174" style="zoom:80%;" />

```python
import torch
from torch import nn
from d2l import torch as d2l


class Reshape(torch.nn.Module):
    def forward(self, x):
        return x.view(-1, 1, 28, 28)  # view()与reshape作用类似

net = torch.nn.Sequential(
    Reshape(),
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),  #输入1通道，输出6通道
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),          
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```

```python
# 打印出网络结构
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
```

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211104174121858.png" alt="image-20211104174121858" style="zoom: 67%;" />

### 6.6.2 模型训练

**使用GPU训练**

```python
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)

def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
    """使用GPU计算模型在数据集上的精度。"""
    # isinstance()：判断一个对象是否是一个已知的类型
	# 判断输入的net模型是否是torch.nn.Module类型
    if isinstance(net, torch.nn.Module):
        net.eval()  # 设置为评估模式（不用计算梯度）
        if not device:
            device = next(iter(net.parameters())).device
    # 正确预测的数量，总预测的数量
    metric = d2l.Accumulator(2) # 存储正确预测数、预测总数
    for X, y in data_iter:
        if isinstance(X, list):
            # BERT微调所需的（之后将介绍）
            X = [x.to(device) for x in X]
        else:
            X = X.to(device)
        y = y.to(device)
        # accuracy(net(X), y)：再计算所有预算正确的样本数
        # numel()函数：返回数组中元素的个数，在此可以求得样本数
        metric.add(d2l.accuracy(net(X), y), y.numel())
    #metric[0]:分类正确的样本数，metric[1]:总的样本数
    return metric[0] / metric[1]
```

```python
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """用GPU训练模型(在第六章定义)。"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # 训练损失之和，训练准确率之和，范例数
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():  #  强制之后的内容不进行计算图构建
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
```

```python
lr,num_epochs = 1.5,15
train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())
```

# 7. 现代卷积神经网络

## 7.1 深度卷积神经网络（AlexNet）

AlexNet：由八层组成，五个卷积层，两个全连接隐藏层，一个全连接输出层。使用ReLU作为激活函数。

![image-20211104220407851](C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211104220407851.png)

AlexNet通过dropout控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 为了进一步扩充数据，AlexNet在训练时增加了大量的图像**增强数据**，如翻转、裁切和变色。 这使得模型更健壮，更大的样本量有效地**减少了过拟合**。 

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    # 这里，我们使用一个11*11的更大窗口来捕捉对象。
    # 同时，步幅为4，以减少输出的高度和宽度。
    # 另外，输出通道的数目远大于LeNet
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 使用三个连续的卷积层和较小的卷积窗口。
    # 除了最后的卷积层，输出通道的数量进一步增加。
    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),
    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过度拟合
    nn.Linear(6400, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
    nn.Linear(4096, 10))
```

```python
#  构造一个高度和宽度都为224的单通道数据，来观察每一层输出的形状
X = torch.randn(1, 1, 224, 224)
for layer in net:
    X=layer(X)
    print(layer.__class__.__name__,'Output shape:\t',X.shape)
```

```python
#  读取数据集
batch_size = 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
#  训练
lr, num_epochs = 0.01, 10
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

## 7.2 使用块的网络

### 7.2.1 VGG块

经典CNN基本组成部分：带填充以保持分辨率的卷积层、非线性激活函数(ReLU等)、池化层。

VGG块：由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。

```python
import torch
from torch import nn
from d2l import torch as d2l

# 带有  3×3  卷积核、填充为 1（保持高度和宽度）的卷积层
# 带有  2×2  池化窗口、步幅为 2（每个块后的分辨率减半）的最大汇聚层
def vgg_block(num_convs, in_channels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)
```

### 7.2..2 VGG网络

![image-20211105094205089](C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211105094205089.png)

超参数变量conv_arch：指定每个VGG块里卷积层个数和输出通道数。

原始 VGG 网络有 5 个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有 64 个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到 512。由于该网络使用 8 个卷积层和 3 个全连接层，因此它通常被称为 VGG-11。

```python
#  定义超参数
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))

def vgg(conv_arch):
    conv_blks = []
    in_channels = 1
    # 卷积层部分
    for (num_convs, out_channels) in conv_arch:
        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
        in_channels = out_channels

    return nn.Sequential(
        *conv_blks, nn.Flatten(),
        # 全连接层部分
        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 10))

net = vgg(conv_arch)
```

```python
X = torch.randn(size=(1, 1, 224, 224))
for blk in net:
    X = blk(X)
    print(blk.__class__.__name__,'output shape:\t',X.shape)
```

```python
#  输出
Sequential output shape:     torch.Size([1, 64, 112, 112])
Sequential output shape:     torch.Size([1, 128, 56, 56])
Sequential output shape:     torch.Size([1, 256, 28, 28])
Sequential output shape:     torch.Size([1, 512, 14, 14])
Sequential output shape:     torch.Size([1, 512, 7, 7])
Flatten output shape:        torch.Size([1, 25088])
Linear output shape:         torch.Size([1, 4096])
ReLU output shape:   torch.Size([1, 4096])
Dropout output shape:        torch.Size([1, 4096])
Linear output shape:         torch.Size([1, 4096])
ReLU output shape:   torch.Size([1, 4096])
Dropout output shape:        torch.Size([1, 4096])
Linear output shape:         torch.Size([1, 10])
```

在每个块的高度和宽度减半，最终高度和宽度都为7。最后再展平表示，送入全连接层处理。

### 7.2.3 训练模型

```python
#  减少通道数
ratio = 4
small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch] #  //表示取整除
net = vgg(small_conv_arch)

lr, num_epochs, batch_size = 0.05, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

## 7.3 网络中的网络（NiN）

想要早期使用全连接层，必须使用稠密层(Flatten)展开，但会完全放弃表征的空间结构。*网络中的网络* (*NiN*) 提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机。

### 7.3.1 NiN块

NiN 的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。 如果我们将权重连接到每个空间位置，我们可以将其视为 1×1卷积层，或作为在每个像素位置上独立作用的全连接层。 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。

NiN块：以一个普通卷积层开始，后面是两个1×1 的卷积层。这两个1×1 卷积层充当带有 ReLU 激活函数的逐像素全连接层。 第一层的卷积窗口形状通常由用户设置。 随后的卷积窗口形状固定为 1×1。

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211105104323318.png" alt="image-20211105104323318" style="zoom:80%;" />

```python
import torch
from torch import nn
from d2l import torch as d2l


def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())
```

### 7.3.2 NiN模型

NiN 和 AlexNet 之间的一个显著区别是 NiN 完全取消了全连接层。 相反，NiN 使用一个 NiN块，其输出通道数等于标签类别的数量。最后放一个 **全局平均汇聚层**（global average pooling layer），生成一个多元逻辑向量。

```python
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0),
    nn.MaxPool2d(3, stride=2),
    nin_block(96, 256, kernel_size=5, strides=1, padding=2),
    nn.MaxPool2d(3, stride=2),
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),
    nn.MaxPool2d(3, stride=2),
    nn.Dropout(0.5),
    # 标签类别数是10
    nin_block(384, 10, kernel_size=3, strides=1, padding=1),
    nn.AdaptiveAvgPool2d((1, 1)),  #  设置输出形状，自适应设置核的大小和步长
    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)
    nn.Flatten())
```

## 7.4 含并行连结的网络（GoogLeNet）

### 7.4.1 Inception块

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211105112945418.png" alt="image-20211105112945418" style="zoom:80%;" />

这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道的数量。

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l


class Inception(nn.Module):
    # `c1`--`c4` 是每条路径的输出通道数
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # 线路1，单1 x 1卷积层
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        # 线路2，1 x 1卷积层后接3 x 3卷积层
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # 线路3，1 x 1卷积层后接5 x 5卷积层
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # 线路4，3 x 3最大汇聚层后接1 x 1卷积层
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        # dim=1 在通道维度上连结输出
        return torch.cat((p1, p2, p3, p4), dim=1)
```

### 7.4.2 GoogLeNet模型

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211105151220358.png" alt="image-20211105151220358" style="zoom:80%;" />

```python
#  第一个模块使用 64 个通道、  7×7  卷积层
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
#  第二个模块使用两个卷积层：第一个卷积层是 64个通道、  1×1  卷积层；第二个卷积层使用将通道数量增加三倍的  3×3  卷积层。
b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),
                   nn.ReLU(),
                   nn.Conv2d(64, 192, kernel_size=3, padding=1),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
#  第三个模块串联两个完整的Inception块
b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                   Inception(256, 128, (128, 192), (32, 96), 64),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
#  第四个模块串联五个Inception块
b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
                   Inception(512, 160, (112, 224), (24, 64), 64),
                   Inception(512, 128, (128, 256), (24, 64), 64),
                   Inception(512, 112, (144, 288), (32, 64), 64),
                   Inception(528, 256, (160, 320), (32, 128), 128),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
#  第五个模块包含两个Inception块，全局平均汇聚层
b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                   Inception(832, 384, (192, 384), (48, 128), 128),
                   nn.AdaptiveAvgPool2d((1,1)),
                   nn.Flatten())

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))
```

## 7.5 批量归一化

批量归一化：加速深层网络的收敛速度。

### 7.5.1 训练神经网络

为什么要批量归一化？

答：将参数的量级进行统一；越深层的网络越容易过拟合，因此正则化很重要。

### 7.5.2 批量归一化层

对于全连接层：将批量归一化层置于全连接层中的仿射变换和激活函数之间。

对于卷积层：在卷积层之后和非线性激活函数之前应用批量归一化。

### 7.5.3 从零实现

```python
import torch
from torch import nn
from d2l import torch as d2l

# 拉伸gamma，偏移beta
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # 通过 `is_grad_enabled` 来判断当前模式是训练模式还是预测模式
    if not torch.is_grad_enabled():
        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 使用全连接层的情况，计算特征维上的均值和方差
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。
            # 这里我们需要保持X的形状以便后面可以做广播运算
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # 训练模式下，用当前的均值和方差做标准化
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 更新移动平均的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # 缩放和移位
    return Y, moving_mean.data, moving_var.data
```

```python
class BatchNorm(nn.Module):
    # `num_features`：完全连接层的输出数量或卷积层的输出通道数。
    # `num_dims`：2表示完全连接层，4表示卷积层
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # 非模型参数的变量初始化为0和1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # 如果 `X` 不在内存上，将 `moving_mean` 和 `moving_var`
        # 复制到 `X` 所在显存上
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 保存更新过的 `moving_mean` 和 `moving_var`
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
```

```python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(),
    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),
    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),
    nn.Linear(84, 10))

lr, num_epochs, batch_size = 1.0, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
#  查看从第一个批量归一化层中学到的拉伸参数 gamma 和偏移参数 beta
net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))
```

## 7.6 残差网络（ResNet）

 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。

### 7.6.2 残差块

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211108103530886.png" alt="image-20211108103530886" style="zoom:80%;" />

拟合出残差映射f(x)-x，残差映射更容易优化。

ResNet 沿用了 VGG 完整的 3×3卷积层设计。 残差块里首先有 2 个有相同输出通道数的 3×3卷积层。 每个卷积层后接一个批量归一化层和 ReLU 激活函数。 然后我们通过跨层数据通路，跳过这 2 个卷积运算，将输入直接加在最后的 ReLU 激活函数前。 这样的设计要求 2 个卷积层的输出与输入形状一样，从而可以相加。 如果想改变通道数，就需要引入一个额外的 1×1 卷积层来将输入变换成需要的形状后再做相加运算。

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211108111501419.png" alt="image-20211108111501419" style="zoom:50%;" />

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l


class Residual(nn.Module):  #@save
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)
        self.relu = nn.ReLU(inplace=True)  
        # inplace=True:从上层网络Conv2d中传递下来的tensor直接进行修改，这样能够节省运算内存，不用多存储其他变量

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
```

### 7.6.3 ResNet模型

![image-20211108143838460](C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211108143838460.png)

ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样： 在输出通道数为 64、步幅为 2 的 7×77×7 卷积层后，接步幅为 2 的 3×33×3 的最大汇聚层。 不同之处在于 ResNet 每个卷积层后增加了批量归一化层。

GoogLeNet 在后面接了 4 个由Inception块组成的模块。 ResNet 则使用 4 个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为 2 的最大汇聚层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。

每个模块有 4 个卷积层（不包括恒等映射的 1×11×1 卷积层）。 加上第一个 7×77×7 卷积层和最后一个全连接层，共有 18 层。 因此，这种模型通常被称为 ResNet-18。

```python
def resnet_block(input_channels, num_channels, num_residuals,
                 first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(input_channels, num_channels,
                                use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
    return blk
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.BatchNorm2d(64), nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))
b3 = nn.Sequential(*resnet_block(64, 128, 2))
b4 = nn.Sequential(*resnet_block(128, 256, 2))
b5 = nn.Sequential(*resnet_block(256, 512, 2))

net = nn.Sequential(b1, b2, b3, b4, b5,
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten(), nn.Linear(512, 10))
```

## 7.7 稠密连接网络（DenseNet）

稠密连接网络在某种程度上是 ResNet 的逻辑扩展，ResNet 和 DenseNet 的关键区别在于，DenseNet 输出是连接，而不是如 ResNet 的简单相加。

稠密网络主要由 2 部分构成： *稠密块*（dense block）和 *过渡层* （transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211108144615994.png" alt="image-20211108144615994" style="zoom:80%;" />

### 7.7.2 稠密块体

```python
import torch
from torch import nn
from d2l import torch as d2l


def conv_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels), nn.ReLU(),
        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))

class DenseBlock(nn.Module):
    def __init__(self, num_convs, input_channels, num_channels):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(num_convs):
            layer.append(conv_block(
                num_channels * i + input_channels, num_channels))
        self.net = nn.Sequential(*layer)

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            # 连接通道维度上每个块的输入和输出
            X = torch.cat((X, Y), dim=1)
        return X
```

### 7.7.3 过渡层

由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过 1×11×1 卷积层来减小通道数，并使用步幅为 2 的平均汇聚层减半高和宽，从而进一步降低模型复杂度。

```python
def transition_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels), nn.ReLU(),
        nn.Conv2d(input_channels, num_channels, kernel_size=1),
        nn.AvgPool2d(kernel_size=2, stride=2))
```

### 7.7.4 DenseNet模型

```python
b1 = nn.Sequential(
    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
    nn.BatchNorm2d(64), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

# `num_channels`为当前的通道数
num_channels, growth_rate = 64, 32
num_convs_in_dense_blocks = [4, 4, 4, 4]
blks = []
for i, num_convs in enumerate(num_convs_in_dense_blocks):
    blks.append(DenseBlock(num_convs, num_channels, growth_rate))
    # 上一个稠密块的输出通道数
    num_channels += num_convs * growth_rate
    # 在稠密块之间添加一个转换层，使通道数量减半
    if i != len(num_convs_in_dense_blocks) - 1:
        blks.append(transition_block(num_channels, num_channels // 2))
        num_channels = num_channels // 2
        
net = nn.Sequential(
    b1, *blks,
    nn.BatchNorm2d(num_channels), nn.ReLU(),
    nn.AdaptiveMaxPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(num_channels, 10))
```

# 8. 循环神经网络（RNN）

卷积神经网络可以有效处理空间信息，循环神经网络适合处理序列信息，如预测股市波动等。循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。

## 8.1 序列模型

### 8.1.1 统计工具

#### 8.1.1.1 自回归模型

自回归模型：用观测序列x<sub>t-1</sub>，...，x<sub>t-τ</sub>来预测。

隐变量自回归模型：保留一些对过去观测的总结。

## 8.2 文本预处理

步骤：

（1）将文本作为字符串加载到内存中；

（2）将字符串拆分为词元（如单词和字符）；

（3）建立一个词汇表，将拆分的词元映射到数字索引；

（4）将文本转换为数字索引序列，方便模型操作。

### 8.2.1 读取数据集

```python
import collections  #  collections模块包含了除list、dict、和tuple之外的容器数据类型
import re
from d2l import torch as d2l

d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL+'timemachine.txt','090b5e7e70c295757f55df93cb0a180b9691891a')

def read_time_machine():
    with open(d2l.download('time_machine'),'r')as f:
        lines = f.readlines()
    #re.sub(r'[A-Za-z]', '*', s) 这句话则表示只匹配单一字母，并将每一个字母替换为一个星号 
    #加上^  取反
    #.strip()去除字符串首尾的空格
    #.lower()将字符串中的所有大写字母转换为小写字母
    return [re.sub('[^A-Za-z+]',' ',line).strip().lower()for line in lines]

lines = read_time_machine()
print(f'# text lines:{len(lines)}')
print(lines[0])
print(lines[10])
```

### 8.2.2 词元化

把每条文本行拆分为词元。

```python
def tokenize(lines, token='word'):  #@save
    """将文本行拆分为单词或字符词元。"""
    if token == 'word':
        return [line.split() for line in lines]# 通过指定分隔符对字符串进行切片
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])
```

### 8.2.3 词汇表

构建一个字典，即词汇表，将字符串类型的词元映射到从0开始的数字序列中。

```python
class Vocab:  #@save
    """文本词汇表"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                  reverse=True)
        # 未知词元的索引为0
        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens
        uniq_tokens += [token for token, freq in self.token_freqs
                        if freq >= min_freq and token not in uniq_tokens]
        self.idx_to_token, self.token_to_idx = [], dict()
        for token in uniq_tokens:
            self.idx_to_token.append(token)
            self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

def count_corpus(tokens):  
    """统计词元的频率。"""
    # 这里的 `tokens` 是 1D 列表或 2D 列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成使用词元填充的一个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)  #collections.Counter统计词元出现的次数
```

```python
vocab = Vocab(tokens)
print(list(vocab.token_to_idx.items())[:10])
```

### 8.2.4 整合所有功能

```python
def load_corpus_time_machine(max_tokens=-1):  #@save
    """返回时光机器数据集的词元索引列表和词汇表。"""
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokens)
    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，
    # 所以将所有文本行展平到一个列表中
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
```

## 8.3 语言模型和数据集

语言模型：目标是估计序列的联合概率。一个理想的语言模型能够基于模型本身生成自然文本。

为了计算语言模型，我们需要计算单词的概率和给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。
$$
P(deep,learning,is,fun) = P(deep)P(learning|deep)P(is|deep,learning)P(fun|deep,learning,is)
$$
通常，涉及一个、两个和三个变量的概率公式分别被称为“一元语法”（unigram）、“二元语法”（bigram）和“三元语法”（trigram）模型。

### 8.3.3 自然语言统计

```python
import random
import torch
from d2l import torch as d2l


tokens = d2l.tokenize(d2l.read_time_machine())
# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
vocab.token_freqs[:10]  #打印前10个最常用的（频率最高的）单词。
```

```python
#  绘制词频图
freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')
```

```python
#  查看二元语法和三元语法
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
bigram_vocab = d2l.Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]

trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]

bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```

![../_images/output_language-models-and-dataset_789d14_54_0.svg](https://zh-v2.d2l.ai/_images/output_language-models-and-dataset_789d14_54_0.svg)



### 8.3.4 读取长序列数据

**随机采样：**每个样本都是在原始的长序列上任意捕获的子序列，在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。

```python
def seq_data_iter_random(corpus, batch_size, num_steps):  #@save
    """使用随机抽样生成一个小批量子序列。"""
    # 从随机偏移量开始对序列进行分区，随机范围包括`num_steps - 1`
    corpus = corpus[random.randint(0, num_steps - 1):]
    # 减去1，是因为我们需要考虑标签
    num_subseqs = (len(corpus) - 1) // num_steps
    # 长度为`num_steps`的子序列的起始索引
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # 在随机抽样的迭代过程中，
    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
    random.shuffle(initial_indices)

    def data(pos):
        # 返回从`pos`位置开始的长度为`num_steps`的序列
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # 在这里，`initial_indices`包含子序列的随机起始索引
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)
```

```python
my_seq = list(range(35))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

```python
#  输出
X:  tensor([[23, 24, 25, 26, 27],
        [18, 19, 20, 21, 22]])
Y: tensor([[24, 25, 26, 27, 28],
        [19, 20, 21, 22, 23]])
X:  tensor([[28, 29, 30, 31, 32],
        [ 8,  9, 10, 11, 12]])
Y: tensor([[29, 30, 31, 32, 33],
        [ 9, 10, 11, 12, 13]])
X:  tensor([[ 3,  4,  5,  6,  7],
        [13, 14, 15, 16, 17]])
Y: tensor([[ 4,  5,  6,  7,  8],
        [14, 15, 16, 17, 18]])
```

**顺序分区：**在迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。

```python
def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save
    """使用顺序分区生成一个小批量子序列。"""
    # 从随机偏移量开始划分序列
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
```

```python
for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

```python
#  输出
X:  tensor([[ 4,  5,  6,  7,  8],
        [19, 20, 21, 22, 23]])
Y: tensor([[ 5,  6,  7,  8,  9],
        [20, 21, 22, 23, 24]])
X:  tensor([[ 9, 10, 11, 12, 13],
        [24, 25, 26, 27, 28]])
Y: tensor([[10, 11, 12, 13, 14],
        [25, 26, 27, 28, 29]])
X:  tensor([[14, 15, 16, 17, 18],
        [29, 30, 31, 32, 33]])
Y: tensor([[15, 16, 17, 18, 19],
        [30, 31, 32, 33, 34]])
```

将上面的两个采样函数包装到一个类中，以便稍后可以将其用作数据迭代器

```python
class SeqDataLoader:  #@save
    """加载序列数据的迭代器。"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
```

```python
#  定义函数 load_data_time_machine ，它同时返回数据迭代器和词汇表
def load_data_time_machine(batch_size, num_steps,  #@save
                           use_random_iter=False, max_tokens=10000):
    """返回时光机器数据集的迭代器和词汇表。"""
    data_iter = SeqDataLoader(
        batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
```

## 8.4 循环神经网络

隐变量模型：
$$
P(x_t|x_{t-1},...,x_1)\approx P(x_t|h_{t-1})
$$
其中h<sub>t-1</sub>是隐藏状态，也称为隐藏变量。可以基于当前输入x<sub>t</sub>和先前隐藏状态h<sub>t-1</sub>来计算时间步t处的任何时间的隐藏状态：
$$
h_t=f(x_t,h_{t-1})
$$
循环神经网络：具有隐藏状态的神经网络。

与无隐藏状态的神经网络不同的是，当前时间步隐藏变量的计算由当前时间步的输入与前一个时间步的隐藏变量一起确定：
$$
H_t=\phi(X_tW_xh+H_{t-1}W_{hh}+b_h)
$$
这些变量捕获并保留了序列直到其当前时间步的历史信息，就如当前时间步下神经网络的状态或记忆，因此这样的隐藏变量被称为**隐藏状态**。基于循环计算的隐状态神经网络被命名为**循环神经网络**。在循环神经网络中执行计算的层称为**循环层**。

<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211116192505796.png" alt="image-20211116192505796" style="zoom: 80%;" />

![../_images/rnn-train.svg](https://zh-v2.d2l.ai/_images/rnn-train.svg)

困惑度(Perplexity)：度量语言模型的质量，一个序列中所有的n个词元的交叉熵损失平均数的指数。
$$
exp(-\frac 1n\sum_{t=1}^nlogP(x_t|x_{t-1},...,x_1))
$$

- 在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。
- 在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。
- 在基线上，该模型的预测是词汇表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词汇表中唯一词元的数量。

## 8.5 循环神经网络的从零开始实现

读取数据集

```python
%matplotlib inline
import math
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

#  读取数据集
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

# one-hot编码
# F.one_hot(torch.tensor([0, 2]), len(vocab))
```

初始化模型参数，隐藏单元数`num_hiddens`是一个可调的超参数

```python
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01

    # 隐藏层参数
    W_xh = normal((num_inputs, num_hiddens))
    W_hh = normal((num_hiddens, num_hiddens))
    b_h = torch.zeros(num_hiddens, device=device)
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True) #表明要计算梯度
    return params
```

### 8.5.3 循环神经网络模型

```python
#  初始化隐藏状态
def init_rnn_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )

def rnn(inputs, state, params):
    # `inputs`的形状：(`时间步数量`，`批量大小`，`词表大小`)
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    # `X`的形状：(`批量大小`，`词表大小`)
    for X in inputs:
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
        Y = torch.mm(H, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
```

```python
class RNNModelScratch: #@save
    """从零开始实现的循环神经网络模型"""
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn

    def __call__(self, X, state):
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params)

    def begin_state(self, batch_size, device):
        return self.init_state(batch_size, self.num_hiddens, device)
```

输出形状是（时间步数×批量大小，词汇表大小），而隐藏状态形状保持不变，即（批量大小, 隐藏单元数）。

### 8.5.4 预测

首先循环遍历prefix中的字符，不断地将隐藏状态传递到下一个时间步，但是不生成任何输出。这被称为“预热”（warm-up）期，因为在此期间模型会自我更新（例如，更新隐藏状态），但不会进行预测。预热期结束后，隐藏状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。

```python
def predict_ch8(prefix, num_preds, net, vocab, device):  #@save
    """在`prefix`后面生成新字符。"""
    state = net.begin_state(batch_size=1, device=device)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))
    for y in prefix[1:]:  # 预热期
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds):  # 预测`num_preds`步
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
```

### 8.5.5 梯度裁剪

为了防止梯度爆炸，将梯度g投影回给定半径的球来裁剪梯度g。
$$
g\leftarrow min(1,\frac {\theta}{||g||})g
$$

```python
def grad_clipping(net, theta):  #@save
    """裁剪梯度。"""
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm
```

### 8.5.6 训练

```python
#@save
def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
    """训练模型一个迭代周期（定义见第8章）。"""
    state, timer = None, d2l.Timer()
    metric = d2l.Accumulator(2)  # 训练损失之和, 词元数量
    for X, Y in train_iter:
        if state is None or use_random_iter:
            # 在第一次迭代或使用随机抽样时初始化`state`
            state = net.begin_state(batch_size=X.shape[0], device=device)
        else:
            if isinstance(net, nn.Module) and not isinstance(state, tuple):
                # `state`对于`nn.GRU`是个张量
                state.detach_()
            else:
                # `state`对于`nn.LSTM`或对于我们从零开始实现的模型是个张量
                for s in state:
                    s.detach_()
        y = Y.T.reshape(-1)
        X, y = X.to(device), y.to(device)
        y_hat, state = net(X, state)
        l = loss(y_hat, y.long()).mean()
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.backward()
            grad_clipping(net, 1)
            updater.step()
        else:
            l.backward()
            grad_clipping(net, 1)
            # 因为已经调用了`mean`函数
            updater(batch_size=1)
        metric.add(l * y.numel(), y.numel())
    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()
```

```python
#@save
def train_ch8(net, train_iter, vocab, lr, num_epochs, device,
              use_random_iter=False):
    """训练模型（定义见第8章）。"""
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
                            legend=['train'], xlim=[10, num_epochs])
    # 初始化
    if isinstance(net, nn.Module):
        updater = torch.optim.SGD(net.parameters(), lr)
    else:
        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)
    # 训练和预测
    for epoch in range(num_epochs):
        ppl, speed = train_epoch_ch8(
            net, train_iter, loss, updater, device, use_random_iter)
        if (epoch + 1) % 10 == 0:
            print(predict('time traveller'))
            animator.add(epoch + 1, [ppl])
    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')
    print(predict('time traveller'))
    print(predict('traveller'))
```

```python
num_epochs, lr = 500, 1
train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())
```

## 8.6 循环神经网络的简洁实现

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

# 构造一个具有256个隐藏单元的单隐藏层的循环神经网络层 rnn_layer
num_hiddens = 256
rnn_layer = nn.RNN(len(vocab), num_hiddens)

#@save
class RNNModel(nn.Module):
    """循环神经网络模型。"""
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # 如果RNN是双向的（之后将介绍），`num_directions`应该是2，否则应该是1。
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # 全连接层首先将`Y`的形状改为(`时间步数`*`批量大小`, `隐藏单元数`)。
        # 它的输出形状是 (`时间步数`*`批量大小`, `词表大小`)。
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # `nn.GRU` 以张量作为隐藏状态
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens),
                                device=device)
        else:
            # `nn.LSTM` 以张量作为隐藏状态
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))
```

```python
device = d2l.try_gpu()
net = RNNModel(rnn_layer, vocab_size=len(vocab))
net = net.to(device)
d2l.predict_ch8('time traveller', 10, net, vocab, device)

num_epochs, lr = 500, 1
d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)
```

# 9. 现代循环神经网络

## 9.1 门控循环单元(GRU)

### 9.1.1 门控隐藏状态

门控循环单元：有专门的机制来确定应该何时**更新**隐藏状态，以及应该何时**重置**隐藏状态。

重置门、更新门：(0,1)区间的向量，重置门允许我们控制可能还想记住的过去状态的数量，更新门将允许我们控制新状态中有多少个是旧状态的副本。

**候选隐藏状态：**
$$
\overline H_t=tanh(X_tW_{xh}+(R_t\oplus H_{t-1})W_{hh}+b_h)
$$
每当重置门 Rt中的项接近 1时,为普通的循环神经网络，对于重置门 Rt中所有接近 0 的项，候选隐藏状态是以 Xt作为输入的多层感知机的结果。。因此，任何预先存在的隐藏状态都会被 **重置** 为默认值。

**隐藏状态：**
$$
H_t=Z_t\oplus H_{t-1}+(1-Z_t)\oplus \overline H_t
$$
每当更新门 Zt接近 1 时，我们就只保留旧状态。此时，来自 Xt的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步 t。相反，当 Zt接近 0 时，新的隐藏状态 Ht就会接近候选的隐藏状态。

![../_images/gru-3.svg](https://zh-v2.d2l.ai/_images/gru-3.svg)

总之，门控循环单元具有以下两个显著特征：

- 重置门有助于捕获序列中的短期依赖关系。
- 更新门有助于捕获序列中的长期依赖关系。

### 9.1.2 从零开始实现

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

#  初始化模型参数
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def three():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    W_xz, W_hz, b_z = three()  # 更新门参数
    W_xr, W_hr, b_r = three()  # 重置门参数
    W_xh, W_hh, b_h = three()  # 候选隐藏状态参数
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params

#  定义模型
def init_gru_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )

def gru(inputs, state, params):
    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)
        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)
        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)
        H = Z * H + (1 - Z) * H_tilda
        Y = H @ W_hq + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)

#  训练与预测
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,
                            init_gru_state, gru)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

### 9.1.3 简洁实现

```python
num_inputs = vocab_size
gru_layer = nn.GRU(num_inputs, num_hiddens)
model = d2l.RNNModel(gru_layer, len(vocab))
model = model.to(device)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

## 9.2 长短期记忆网络(LSTM)

### 9.2.1 门控记忆单元

输入门It：用来决定何时将数据读入单元，控制采用多少来自候选记忆单元的新数据；

输出门Ot：用来从单元中读出条目，只要输出门接近 11，我们就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近 00，我们只保留存储单元内的所有信息，并且没有进一步的过程需要执行；

遗忘门Ft：重置单元的内容，控制保留了多少就记忆单元的内容。

候选记忆单元：
$$
\overline C_t=tanh(X_tW_{xc}+H_{t-1}W{hc}+b_c)
$$
记忆单元：
$$
C_t=F_t\oplus C_{t-1}+I_t\oplus \overline C_t
$$
如果遗忘门始终为 1 且输入门始终为 0，则过去的记忆单元 Ct−1将随时间被保存并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。

![../_images/lstm-2.svg](https://zh-v2.d2l.ai/_images/lstm-2.svg)

 隐藏状态：
$$
H_t=O_t\oplus tanh(C_t)
$$
<img src="C:\Users\WJL\AppData\Roaming\Typora\typora-user-images\image-20211117193635027.png" alt="image-20211117193635027" style="zoom:67%;" />

### 9.2.2 从零开始实现

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

#  初始化模型参数
def get_lstm_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def three():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    W_xi, W_hi, b_i = three()  # 输入门参数
    W_xf, W_hf, b_f = three()  # 遗忘门参数
    W_xo, W_ho, b_o = three()  # 输出门参数
    W_xc, W_hc, b_c = three()  # 候选记忆单元参数
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,
              b_c, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params

#  定义模型
def init_lstm_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device),
            torch.zeros((batch_size, num_hiddens), device=device))
def lstm(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
     W_hq, b_q] = params
    (H, C) = state
    outputs = []
    for X in inputs:
        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)
        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)
        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)
        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)
        C = F * C + I * C_tilda
        H = O * torch.tanh(C)
        Y = (H @ W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H, C)
```

```python
#  训练和预测
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,
                            init_lstm_state, lstm)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

### 9.2.3 简洁实现

```python
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

## 9.3 深度循环神经网络

深度循环神经网络：将多层循环神经网络堆叠在一起，通过对几个简单层的组合，来产生灵活的机制。

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
device = d2l.try_gpu()
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)

num_epochs, lr = 500, 2
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

## 9.4 双向循环神经网络

在很多情况下，每个短语的下文传达了重要的信息，单纯用序列模型表现不佳。

双向循环神经网络：添加了反向传递信息的隐藏层。使用来自过去和未来的观测信息来预测当前的观测。双向循环网络不能预测未来，并且其计算量大，速度慢。

![../_images/birnn.svg](https://zh-v2.d2l.ai/_images/birnn.svg)

前向和反向隐状态的更新如下：
$$
\begin{split}\begin{aligned}
\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)}  + \mathbf{b}_h^{(f)}),\\
\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)}),
\end{aligned}\end{split}
$$
将前向隐状态和反向隐状态连接起来，获得需要送入输出层的隐状态Ht。在具有多个隐藏层的深度双向循环神经网络中， 该信息作为输入传递到下一个双向层。 最后，输出层计算得到的输出为 （q是输出单元的数目）：
$$
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.
$$

```python
# 双向循环神经网络的错误应用

import torch
from torch import nn
from d2l import torch as d2l

# 加载数据
batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
# 通过设置“bidirective=True”来定义双向LSTM模型
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
# 训练模型
num_epochs, lr = 500, 1
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

## 9.6 编码器-解码器结构

编码器：接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。

解码器：将固定形状的编码状态映射到长度可变的序列。

![../_images/encoder-decoder.svg](https://zh-v2.d2l.ai/_images/encoder-decoder.svg)

```python
from torch import nn

#@save
class Encoder(nn.Module):
    """编码器-解码器架构的基本编码器接口"""
    def __init__(self, **kwargs):
        super(Encoder, self).__init__(**kwargs)

    def forward(self, X, *args):
        raise NotImplementedError  #raise抛出异常
```

```python
#@save
class Decoder(nn.Module):
    """编码器-解码器架构的基本解码器接口"""
    def __init__(self, **kwargs):
        super(Decoder, self).__init__(**kwargs)

    def init_state(self, enc_outputs, *args):
        raise NotImplementedError

    def forward(self, X, state):
        raise NotImplementedError
```

```python
# 合并编码器和解码器
class EncoderDecoder(nn.Module):
    """编码器-解码器架构的基类"""
    def __init__(self, encoder, decoder, **kwargs):
        super(EncoderDecoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, enc_X, dec_X, *args):
        enc_outputs = self.encoder(enc_X, *args)
        dec_state = self.decoder.init_state(enc_outputs, *args)
        return self.decoder(dec_X, dec_state)
```

# 10. 注意力机制

## 10.1 注意力提示

非自主性提示：基于环境中物体的突出行和易见性，如注意力集中在红色而不是灰色。

自主性提示：依赖于任务的意志提示（想读一本书），注意力被自主引导到书上。

在注意力机制的背景下，我们将自主性提示称为**查询**，给定任何查询，注意力机制通过注意力汇聚， 将选择引导至**感官输入**。在注意力机制中，这些感官输入被称为**值**。

![../_images/qkv.svg](https://zh-v2.d2l.ai/_images/qkv.svg)

### 10.1.3 注意力的可视化

平均汇聚层可以被视为输入的加权平均值， 其中各输入的权重是一样的。 实际上，注意力汇聚得到的是加权平均的总和值， 其中权重是在给定的查询和不同的键之间计算得出的。

```python
import torch
from d2l import torch as d2l
#  输入matrices的形状是 （要显示的行数，要显示的列数，查询的数目，键的数目）
def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
                  cmap='Reds'):
    """显示矩阵热图"""
    d2l.use_svg_display()
    num_rows, num_cols = matrices.shape[0], matrices.shape[1]
    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
                                 sharex=True, sharey=True, squeeze=False)
    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)  #绘制热图
            if i == num_rows - 1:
                ax.set_xlabel(xlabel)
            if j == 0:
                ax.set_ylabel(ylabel)
            if titles:
                ax.set_title(titles[j])
    fig.colorbar(pcm, ax=axes, shrink=0.6);   #  给子图添加colorbar（颜色条或渐变色条）
```

![../_images/output_attention-cues_054b1a_30_0.svg](https://zh-v2.d2l.ai/_images/output_attention-cues_054b1a_30_0.svg)

## 10.2 注意力汇聚：Nadaraya-Watson 核回归

生成人工数据集：
$$
y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon
$$

```python
#  生成数据集
n_train = 50  # 训练样本数
x_train, _ = torch.sort(torch.rand(n_train) * 5)   # 排序后的训练样本

def f(x):
    return 2 * torch.sin(x) + x**0.8

y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # 训练样本的输出
x_test = torch.arange(0, 5, 0.1)  # 测试样本
y_truth = f(x_test)  # 测试样本的真实输出
n_test = len(x_test)  # 测试样本数
n_test
```

Nadaraya-Watson核回归：根据输入的位置对输出yi进行加权：
$$
\begin{split}\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}\end{split}
$$
如果一个键xixi越是接近给定的查询x， 那么分配给这个键对应值yi的注意力权重就会越大， 也就“获得了更多的注意力”。

带参数的Nadaraya-Watson核回归：
$$
\begin{split}\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.\end{aligned}\end{split}
$$

```python
# 批量矩阵乘法
X = torch.ones((2, 1, 4))
Y = torch.ones((2, 4, 6))
torch.bmm(X, Y).shape
```

```python
#  输出
torch.Size([2, 1, 6])
```

```python
#  定义模型

class NWKernelRegression(nn.Module):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.w = nn.Parameter(torch.rand((1,), requires_grad=True))

    def forward(self, queries, keys, values):
        # `queries` 和 `attention_weights` 的形状为 (查询个数，“键－值”对个数)
        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))
        self.attention_weights = nn.functional.softmax(
            -((queries - keys) * self.w)**2 / 2, dim=1)
        # `values` 的形状为 (查询个数，“键－值”对个数)
        return torch.bmm(self.attention_weights.unsqueeze(1),
                         values.unsqueeze(-1)).reshape(-1)
    
# `X_tile` 的形状: (`n_train`，`n_train`)，每一行都包含着相同的训练输入
X_tile = x_train.repeat((n_train, 1))
# `Y_tile` 的形状: (`n_train`，`n_train`)，每一行都包含着相同的训练输出
Y_tile = y_train.repeat((n_train, 1))
# `keys` 的形状: ('n_train'，'n_train' - 1)
keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))
# `values` 的形状: ('n_train'，'n_train' - 1)
values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))

net = NWKernelRegression()
loss = nn.MSELoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=0.5)
animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])

for epoch in range(5):
    trainer.zero_grad()
    # L2 Loss = 1/2 * MSE Loss
    l = loss(net(x_train, keys, values), y_train) / 2
    l.sum().backward()
    trainer.step()
    print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')
    animator.add(epoch + 1, float(l.sum()))
```

## 10.3 注意力评分函数

![../_images/attention-output.svg](https://zh-v2.d2l.ai/_images/attention-output.svg)

**掩蔽softmax操作：**softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。为了仅将有意义的词元作为值来获取注意力汇聚， 我们可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。

```python
#@save
def masked_softmax(X, valid_lens):
    """通过在最后一个轴上掩蔽元素来执行 softmax 操作"""
    # `X`: 3D张量，`valid_lens`: 1D或2D 张量
    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)
        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0
        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,
                              value=-1e6)
        return nn.functional.softmax(X.reshape(shape), dim=-1)
```

**加性注意力：**当查询和键是不同长度的矢量时， 我们可以使用加性注意力作为评分函数。
$$
a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R}
$$

```python
#@save
class AdditiveAttention(nn.Module):
    """加性注意力"""
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # 在维度扩展后，
        # `queries` 的形状：(`batch_size`，查询的个数，1，`num_hidden`)
        # `key` 的形状：(`batch_size`，1，“键－值”对的个数，`num_hiddens`)
        # 使用广播方式进行求和
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # `self.w_v` 仅有一个输出，因此从形状中移除最后那个维度。
        # `scores` 的形状：(`batch_size`，查询的个数，“键-值”对的个数)
        scores = self.w_v(features).squeeze(-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # `values` 的形状：(`batch_size`，“键－值”对的个数，值的维度)
        return torch.bmm(self.dropout(self.attention_weights), values)
```

```python
queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))
# `values` 的小批量，两个值矩阵是相同的
values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(
    2, 1, 1)
valid_lens = torch.tensor([2, 6])

attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,
                              dropout=0.1)
attention.eval()
attention(queries, keys, values, valid_lens)

d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
```

**缩放点积注意力：**使用点积可以得到计算效率更高的评分函数， 但是点积操作要求查询和键具有相同的长度d。

评分函数：
$$
a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}
$$
缩放点积的注意力：
$$
\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}
$$

```python
#@save
class DotProductAttention(nn.Module):
    """缩放点积注意力"""
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # `queries` 的形状：(`batch_size`，查询的个数，`d`)
    # `keys` 的形状：(`batch_size`，“键－值”对的个数，`d`)
    # `values` 的形状：(`batch_size`，“键－值”对的个数，值的维度)
    # `valid_lens` 的形状: (`batch_size`，) 或者 (`batch_size`，查询的个数)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # 设置 `transpose_b=True` 为了交换 `keys` 的最后两个维度
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)
```

```python
queries = torch.normal(0, 1, (2, 1, 2))
attention = DotProductAttention(dropout=0.5)
attention.eval()
attention(queries, keys, values, valid_lens)

d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
```

## 10.4 Bahdanau注意力

Bahdanau注意力模型：
$$
\mathbf{c}_{t'} = \sum_{t=1}^T \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_t) \mathbf{h}_t
$$
![../_images/seq2seq-attention-details.svg](https://zh-v2.d2l.ai/_images/seq2seq-attention-details.svg)

## 10.5 多头注意力

**多头注意力：**用独立学习得到的h组不同的线性投影来变换查询、键和值。 然后，这h组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这hh个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。

![../_images/multi-head-attention.svg](https://zh-v2.d2l.ai/_images/multi-head-attention.svg)

## 10.6 自注意力和位置编码

将词元序列输入注意力池化中， 以便同一组词元同时充当查询、键和值。 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为**自注意力**。

卷积神经网络和自注意力都拥有并行计算的优势， 而且自注意力的最大路径长度最短。 但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。

在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，我们通过在输入表示中添加**位置编码**来注入绝对的或相对的位置信息。

























